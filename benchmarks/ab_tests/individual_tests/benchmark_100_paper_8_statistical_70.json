{
  "test_id": "benchmark_100_paper_8_statistical_70",
  "technique": "statistical_70%",
  "original_prompt": "Z q t ( jD ) p ( y t +1 j f; x t +1 ). If the posterior at t + 1 is approximated directly one gets q t +1 ( jD ; x t +1 ; y t +1 ). BALD calculates the entropy dierence between q t and ^ p t +1 , without having to compute q t +1 for each candidate x . In contrast, the IVM calculates the entropy change between q t and q t +1 . The IVM’s ap- proach cannot calculate the entropy of the full innite dimensional posterior, and requires O ( N x N y ) posterior updates. To do these updates eciently, ap- proximate inference is performed using Assumed Density Filtering (ADF). Using ADF means that q t +1 is a direct approximation to ^ p t +1 , indicating that the IVM makes a further approximation to BALD. Since BALD only requires O (1) posterior updates it can aord to use more accurate, iterative procedures, such as EP. Information Theoretic approaches: Maximum Entropy Sampling (MES) [ Sebastiani and Wynn, 2000 ] explicitly works in dataspace (Eqn. (2) ). MES was proposed for regression models with input-independent observation noise. Al- though Eqn. (2) is used, the second term is constant because of input independent noise and is ignored. One cannot, however, use MES for heteroscedastic re- gression or classication; it fails to dierentiate between model uncertainty and observation uncertainty (about which our model may be condent). Some toy demonstrations show this ‘information based’ active learning criterion performing pathologically in classication by repeatedly querying points close the decision boundary or in regions of high observation uncertainty e.g. [ Huang et al., 2010 ]. This is because MES is inappropriate in this domain; BALD distinguishes be- tween observation and model uncertainty and eliminates these problems as we will show. Mutual-information based objective functions are presented in [ Ertin et al., , Fuhrmann, 2003 ]. They maximise the mutual information between the variable being measured and the variable of interest. Fuhrmann [ Fuhrmann, 2003 ] applies this to linear Gaussian models and acoustic arrays, Ertin et al. [ Ertin et al., ] to a communications channel. Although related, these objectives do not work with the model parameters and are not applied to classication. [ Guestrin et al., 2005 , Krause et al., 2006 ] also use mutual information. They specify interest points in advance and maximise the expected mutual information between the predictive distributions at these points and at the observed locations. Although this is a objective is promising for regression, it is not tractable for models with input-dependent observation noise, such as classication or preference learning. Decision theoretic: We briey mention decision theoretic approaches to ac- tive learning. Two closely related algorithms, [ Kapoor et al., 2007 , Zhu et al., 2003 ], seek to minimize the expected cost i.e. loss weighted misclassication probability on all seen and future data. These methods observe the locations of the test points and their objective functions become monotonic in the predictive entropies at the test points. [ Kapoor et al., 2007 ] also includes an empirical error term 8",
  "compressed_prompt": "Z ( jD ) p ( y j f; x ). If posterior at + 1 approximated directly one gets ( jD ; x ; y ). BALD calculates entropy dierence between ^ p , without having compute each candidate x . In contrast, IVM calculates entropy change between . The IVM’s ap- proach cannot calculate entropy full innite dimensional posterior, requires O ( N x N y ) posterior updates. To do these updates eciently, ap- proximate inference performed using Assumed Density Filtering (ADF). Using ADF means that direct approximation ^ p indicating that IVM makes further approximation BALD. Since BALD only requires O (1) posterior updates it can aord use more accurate, iterative procedures, such as EP. Information Theoretic approaches: Maximum Entropy Sampling (MES) Sebastiani Wynn, 2000 explicitly works dataspace (Eqn. (2) ). MES was proposed regression models with input-independent noise. Al- though Eqn. (2) used, second term constant because input independent noise ignored. One cannot, however, use MES heteroscedastic re- gression or classication; it fails dierentiate uncertainty uncertainty (about which our model may be condent). Some toy demonstrations show this ‘information based’ active learning criterion performing pathologically classication by repeatedly querying points close decision boundary or regions high uncertainty e.g. Huang 2010 ]. This because MES inappropriate this domain; BALD distinguishes be- tween uncertainty eliminates problems as we will show. Mutual-information based objective functions are presented Ertin Fuhrmann, 2003 ]. They maximise mutual information variable being measured variable interest. Fuhrmann Fuhrmann, 2003 applies this linear Gaussian models acoustic arrays, Ertin al. Ertin communications channel. Although related, objectives do not work with parameters are not applied classication. Guestrin 2005 Krause 2006 also use mutual information. They specify interest advance maximise expected mutual information predictive distributions observed locations. Although this objective promising regression, it not tractable models with input-dependent observation noise, such as classication or preference learning. Decision theoretic: We briey mention decision theoretic approaches ac- tive learning. Two closely related algorithms, Kapoor 2007 Zhu 2003 ], seek minimize expected cost i.e. loss weighted misclassication probability on all seen future data. These methods observe locations test points their objective functions become monotonic predictive entropies at test points. Kapoor 2007 ] also includes an empirical error term 8",
  "original_tokens": 527,
  "compressed_tokens": 368,
  "compression_ratio": 0.698292220113852,
  "metadata": {
    "paper_id": "benchmark_100_paper_8",
    "dictionary_entries": null,
    "substitutions": null,
    "processing_time_ms": 0.2006
  }
}