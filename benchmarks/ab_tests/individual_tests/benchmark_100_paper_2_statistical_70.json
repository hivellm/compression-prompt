{
  "test_id": "benchmark_100_paper_2_statistical_70",
  "technique": "statistical_70%",
  "original_prompt": "losses encountered after making decisions based on the data collected i.e. min- imize the Bayes posterior risk [ Roy and McCallum, 2001 ]. Maximising perfor- mance under test is the ultimate objective of most learners, however, evaluat- ing this objective can be very hard. For example, the methods proposed in [ Kapoor et al., 2007 , Zhu et al., 2003 ] for classication are in general expensive to compute. Furthermore, one may not know the loss function or test distribution in advance, or may want the model to perform well on a variety of loss functions. In extreme scenarios, such as exploratory data analysis, or visualisation, losses may be very hard to quantify. This motivates information theoretic approaches to active learning, which are agnostic to the decision task at hand and particular test data, this is known an inductive approach. They seek to reduce the number of feasible models as quickly as possible, using either heuristics (e.g. margin sampling [ Tong and Koller, 2001 ]) or by formalising uncertainty using well studied quantities, such as Shannons entropy and the KL-divergence [ Cover et al., 1991 ]. Although the latter approach was proposed several decades ago [ Lindley, 1956 , Bernardo, 1979 ], it is not always straightforward to apply the criteria to complicated models such as nonparametric processes with innite parameter spaces. As a result many algorithms exist which compute approximate posterior entropies, perform sampling, or work with related quantities in non-probabilistic models. We return to this problem, presenting the full information criterion and demonstrate how to apply it to Gaussian Processes Classication (GPC), yielding a novel active learning algorithm that makes minimal approximations. GPC is a powerful, non-parametric kernel-based model, and poses an interesting problem for information-theoretic active learning because the parameter space is innite dimensional and the posterior distribution is analytically intractable. We present the information theoretic approach to active learning in Section 2. In Section 3 we apply it to GPC, and show how to extended our method to preference learning. In Section 4 we review other approaches and how they compare to our algorithm. We take particular care to contrast our approach to the Informative Vector Machine, that addresses data point selection for GPs directly. We present results on a wide variety of datasets in Section 5 and conclude in Section 6. 2 Bayesian Information Theoretic Active Learn- ing We consider a fully discriminative model where the goal of active learning is to discover the dependence of some variable y 2 Y on an input variable x 2 X . The key idea in active learning is that the learner chooses the input queries x i 2 X and observes the system’s response y i , rather than passively receiving ( x i y i ) pairs. Within a Bayesian framework we assume existence of some latent param- eters, , that control the dependence between inputs and outputs, p ( y j x ; ). Having observed data D = f ( x i ; y i ) g n i =1 , a posterior distribution over the pa- 2",
  "compressed_prompt": "losses encountered after making decisions based data collected i.e. min- imize Bayes posterior risk Roy McCallum, 2001 ]. Maximising perfor- mance under test ultimate objective most learners, however, evaluat- ing this objective can be very hard. For example, methods proposed Kapoor et al., 2007 Zhu et al., 2003 ] for classication are general expensive compute. Furthermore, one may not know loss function test distribution advance, may want model perform well variety loss functions. In extreme scenarios, such exploratory data analysis, visualisation, losses may be very hard quantify. This motivates information theoretic approaches learning, which are agnostic decision task at hand particular test data, this known inductive approach. They seek reduce number feasible models quickly possible, using either heuristics (e.g. margin sampling Tong Koller, 2001 ]) by formalising uncertainty using well studied quantities, such Shannons entropy KL-divergence Cover et al., 1991 ]. Although latter approach was proposed several decades ago Lindley, 1956 Bernardo, 1979 ], it not always straightforward apply criteria complicated models such nonparametric processes with innite parameter spaces. As result many algorithms exist which compute approximate entropies, perform sampling, work with related quantities non-probabilistic models. We return this problem, presenting full information criterion demonstrate how apply it Gaussian Processes Classication (GPC), yielding novel algorithm makes minimal approximations. GPC powerful, non-parametric kernel-based model, poses interesting problem information-theoretic because parameter space innite dimensional distribution analytically intractable. We present information theoretic approach Section 2. Section 3 we apply it GPC, show how extended our method preference learning. Section 4 we review other approaches how they compare our algorithm. We take particular care contrast our approach Informative Vector Machine, addresses point selection GPs directly. We present results wide variety datasets Section 5 conclude Section 6. Bayesian Information Theoretic Active Learn- ing We consider fully discriminative model where goal discover dependence some variable Y input variable X . The key idea learner chooses input queries X observes system’s response rather than passively receiving ( ) pairs. Within Bayesian framework we assume existence some latent param- eters, that control dependence between inputs outputs, p ( j ; ). Having observed data D = f ( ; ) g n =1 posterior distribution over pa-",
  "original_tokens": 514,
  "compressed_tokens": 359,
  "compression_ratio": 0.6984435797665369,
  "metadata": {
    "paper_id": "benchmark_100_paper_2",
    "dictionary_entries": null,
    "substitutions": null,
    "processing_time_ms": 0.19
  }
}