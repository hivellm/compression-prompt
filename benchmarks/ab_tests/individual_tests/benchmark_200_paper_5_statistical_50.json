{
  "test_id": "benchmark_200_paper_5_statistical_50",
  "technique": "statistical_50%",
  "original_prompt": "two entropy quantities. The rst term in Eqn. (2) , H[ y j x ; D ] can be handled analytically for the probit case: H[ y j x ; D ] 1 h Z ( f x ) N ( f x j x ; D ; 2 x ; D ) df x = h 0 @ 0 @ x ; D 2 x ; D + 1 1 A 1 A (3) The second term, E f p ( f jD ) [H[ y j x ; f ]] can be computed approximately as follows: E f p ( f jD ) [H[ y j x ; f ]] 1 Z h(( f x )) N ( f x j x ; D ; 2 x ; D ) df x (4) 2 Z exp f 2 x ln 2 N ( f x j x ; D ; 2 x ; D ) df x = C 2 x ; D + C 2 exp 0 @ 2 x ; D 2 2 x ; D + C 2 1 A where C = q\n2 . The rst approximation, 1 , reects the Gaussian ap- proximation to the posterior. The integral in the left hand side of Eqn. (4) is intractable. By performing a Taylor expansion on ln h(( f x )) (see supplementary material) we can see that it can be approximated up to O ( f 4 x ) by a squared exponential curve, exp ( f 2 x = ln 2). We will refer to this approximation as 2 . Now we can apply the standard convolution formula for Gaussians to nally get a closed form expression for both terms of Eqn. (2). Fig. 1 depicts the striking accuracy of this simple approximation. The max- imum possible error that will be incurred when using this approximation is if N ( f x j x ; D ; 2 x ; D ) is centred at x ; D = 2 : 05 with 2 x ; D tending to zero (see Fig. 1, absolute error ), yielding only a 0.27% error in the integral in Eqn. (4) . The authors are unaware of previous use of this simple and useful approximation in this context. In Section 5 we investigate experimentally the information lost from approximations 1 and 2 as compared to the golden standard of extensive Monte Carlo simulation. To summarise, the BALD algorithm for Gaussian process classication con- sists of two steps. First it applies any standard approximate inference algorithm for GPCs (such as EP) to obtain the posterior predictive mean x ; D and x ; D for each point of interest x . Then, it selects a query x that maximises the following objective function: h 0 @ 0 @ x ; D 2 x ; D + 1 1 A 1 A C exp 2 x ; D 2 ( 2 x ; D + C 2 ) 2 x ; D + C 2 (5) 5",
  "compressed_prompt": "two entropy quantities. The rst term in Eqn. (2) , H[ y ] can be handled analytically probit case: H[ y ] h Z N df = h @ @ (3) second term, E p jD [H[ y ]] can be computed approximately as follows: E p jD [H[ y ]] Z h(( )) N df (4) Z ln N df exp @ where q . rst approximation, , reects Gaussian ap- proximation posterior. integral left hand side Eqn. (4) is intractable. By performing Taylor expansion on ln h(( )) (see supplementary material) we see that it approximated up O 4 by squared exponential curve, exp ln 2). We will refer approximation . Now we apply standard convolution formula Gaussians nally get closed form expression both terms Eqn. (2). Fig. depicts striking accuracy simple approximation. max- imum possible error that will incurred when using approximation is if N centred at : 05 with tending zero (see Fig. 1, absolute error ), yielding only 0.27% error integral Eqn. (4) . authors are unaware previous use simple and useful approximation context. In Section 5 we investigate experimentally information lost from approximations and compared golden standard extensive Monte Carlo simulation. To summarise, BALD algorithm Gaussian process classication con- sists two steps. First it applies any standard approximate inference algorithm GPCs (such as EP) obtain posterior predictive mean and each point interest . Then, it selects query that maximises following objective function: h 0 @ 0 @ + A A C exp + C + C (5) 5",
  "original_tokens": 509,
  "compressed_tokens": 254,
  "compression_ratio": 0.49901768172888017,
  "metadata": {
    "paper_id": "benchmark_200_paper_5",
    "dictionary_entries": null,
    "substitutions": null,
    "processing_time_ms": 0.1622
  }
}