{
  "test_id": "benchmark_100_paper_3_statistical_70",
  "technique": "statistical_70%",
  "original_prompt": "rameters is inferred, p ( jD ). The central goal of information theoretic ac- tive learning is to reduce the number possible hypotheses maximally fast, i.e. to minimize the uncertainty about the parameters using Shannon’s entropy [ Cover et al., 1991 ]. Data points D 0 are selected that satisfy arg min D 0 H[ jD 0 ] = R p ( jD 0 ) log p ( jD 0 )d . Solving this problem in general is NP-hard; however, as is common in sequential decision making tasks a myopic (greedy) approxi- mation is made [ Heckerman et al., 1995 ]. It has been shown that the myopic policy can perform near-optimally [ Golovin and Krause, 2010 , Dasgupta, 2005 ]. Therefore, the objective is to seek the data point x that maximises the decrease in expected posterior entropy: arg max x H[ jD ] E y p ( y j x D ) [H[ j y; x ; D ]] (1) Note that expectation over the unseen output y is required. Many works e.g. [ MacKay, 1992 , Krishnapuram et al., , Lawrence et al., 2003 ] propose using this objective directly. However, parameter posteriors are often high dimen- sional and computing their entropies is usually intractable. Furthermore, for nonparametric processes the parameter space is innite dimensional so Eqn. (1) becomes poorly dened. To avoid gridding parameter space (exponentially hard with dimensionality), or sampling (from which it is notoriously hard to estimate entropies without introducing bias [ Panzeri and Petersen, 2007 ]), these papers make Gaussian or low dimensional approximations and calculate the entropy of the approximate posterior. A second computational diculty arises; if N x data points are under consideration, and N y responses may be seen, then O ( N x N y ), potentially expensive, posterior updates are required to calculate Eqn. (1). An important insight arises if we note that the objective in Eqn. (1) is equivalent to the conditional mutual information between the unknown output and the parameters, I[ ; y j x ; D ]. Using this insight it is simple to show that the objective can be rearranged to compute entropies in y space: arg max x H[ y j x ; D ] E p ( jD ) [H[ y j x ; ]] (2) Eqn. (2) overcomes the challenges we described for Eqn. (1) . Entropies are now calculated in, usually low dimensional, output space. For binary classication, these are just entropies of Bernoulli variables. Also is now conditioned only on D , so only O (1) posterior updates are required. Eqn. (2) also provides us with an interesting intuition about the objective; we seek the x for which the model is marginally most uncertain about y (high H[ y j x ; D ]), but for which individual settings of the parameters are condent (low E p ( jD ) [H[ y j x ; ]] ). This can be interpreted as seeking the x for which the parameters under the posterior disagree about the outcome the most, so we refer to this objective as Bayesian Active Learning by Disagreement (BALD). We present a method to apply Eqn. (2) directly to GPC and preference learning. We no longer need to build our entropy calculation around the type of posterior approximation (as 3",
  "compressed_prompt": "rameters inferred, p ). The central goal of information theoretic ac- tive learning reduce number possible hypotheses maximally fast, i.e. minimize uncertainty about parameters using Shannon’s entropy [ Cover et al., 1991 ]. Data points D 0 selected that satisfy arg min D 0 H[ 0 ] = R 0 ) log 0 )d . Solving this problem general NP-hard; however, as common sequential decision making tasks a myopic (greedy) approxi- mation made [ Heckerman et al., 1995 ]. It has been shown myopic policy can perform near-optimally Golovin Krause, 2010 , Dasgupta, 2005 ]. Therefore, seek data point maximises decrease expected posterior entropy: arg max H[ ] E ) [H[ y; ]] Note expectation over unseen output required. Many works e.g. MacKay, 1992 , Krishnapuram et Lawrence et al., 2003 ] propose using this directly. However, parameter posteriors often high dimen- sional computing their entropies usually intractable. Furthermore, nonparametric processes parameter space innite dimensional so Eqn. becomes poorly dened. To avoid gridding parameter space (exponentially hard with dimensionality), or sampling (from which it notoriously hard estimate entropies without introducing bias Panzeri Petersen, 2007 ]), these papers make Gaussian or low dimensional approximations calculate entropy approximate posterior. A second computational diculty arises; if N data points under consideration, N responses may be seen, then O N N ), potentially expensive, updates required calculate Eqn. (1). An important insight arises if we note Eqn. equivalent conditional mutual information between unknown output parameters, I[ ]. Using this insight it simple show can be rearranged compute entropies space: arg max H[ ] E ) [H[ ]] (2) Eqn. (2) overcomes challenges we described Eqn. . Entropies now calculated in, usually low dimensional, output space. For binary classication, these just entropies Bernoulli variables. Also now conditioned only on , so only O updates required. Eqn. (2) also provides us with an interesting intuition about objective; we seek which model marginally most uncertain about (high H[ D ]), but which individual settings parameters condent (low E ) [H[ ]] ). This can be interpreted as seeking which parameters under posterior disagree about outcome most, so we refer this objective as Bayesian Active Learning by Disagreement (BALD). We present a method apply Eqn. (2) directly GPC preference learning. We no longer need build our entropy calculation around type of posterior approximation (as 3",
  "original_tokens": 553,
  "compressed_tokens": 387,
  "compression_ratio": 0.6998191681735986,
  "metadata": {
    "paper_id": "benchmark_100_paper_3",
    "dictionary_entries": null,
    "substitutions": null,
    "processing_time_ms": 0.1946
  }
}