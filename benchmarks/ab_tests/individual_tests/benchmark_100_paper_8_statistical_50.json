{
  "test_id": "benchmark_100_paper_8_statistical_50",
  "technique": "statistical_50%",
  "original_prompt": "Z q t ( jD ) p ( y t +1 j f; x t +1 ). If the posterior at t + 1 is approximated directly one gets q t +1 ( jD ; x t +1 ; y t +1 ). BALD calculates the entropy dierence between q t and ^ p t +1 , without having to compute q t +1 for each candidate x . In contrast, the IVM calculates the entropy change between q t and q t +1 . The IVM’s ap- proach cannot calculate the entropy of the full innite dimensional posterior, and requires O ( N x N y ) posterior updates. To do these updates eciently, ap- proximate inference is performed using Assumed Density Filtering (ADF). Using ADF means that q t +1 is a direct approximation to ^ p t +1 , indicating that the IVM makes a further approximation to BALD. Since BALD only requires O (1) posterior updates it can aord to use more accurate, iterative procedures, such as EP. Information Theoretic approaches: Maximum Entropy Sampling (MES) [ Sebastiani and Wynn, 2000 ] explicitly works in dataspace (Eqn. (2) ). MES was proposed for regression models with input-independent observation noise. Al- though Eqn. (2) is used, the second term is constant because of input independent noise and is ignored. One cannot, however, use MES for heteroscedastic re- gression or classication; it fails to dierentiate between model uncertainty and observation uncertainty (about which our model may be condent). Some toy demonstrations show this ‘information based’ active learning criterion performing pathologically in classication by repeatedly querying points close the decision boundary or in regions of high observation uncertainty e.g. [ Huang et al., 2010 ]. This is because MES is inappropriate in this domain; BALD distinguishes be- tween observation and model uncertainty and eliminates these problems as we will show. Mutual-information based objective functions are presented in [ Ertin et al., , Fuhrmann, 2003 ]. They maximise the mutual information between the variable being measured and the variable of interest. Fuhrmann [ Fuhrmann, 2003 ] applies this to linear Gaussian models and acoustic arrays, Ertin et al. [ Ertin et al., ] to a communications channel. Although related, these objectives do not work with the model parameters and are not applied to classication. [ Guestrin et al., 2005 , Krause et al., 2006 ] also use mutual information. They specify interest points in advance and maximise the expected mutual information between the predictive distributions at these points and at the observed locations. Although this is a objective is promising for regression, it is not tractable for models with input-dependent observation noise, such as classication or preference learning. Decision theoretic: We briey mention decision theoretic approaches to ac- tive learning. Two closely related algorithms, [ Kapoor et al., 2007 , Zhu et al., 2003 ], seek to minimize the expected cost i.e. loss weighted misclassication probability on all seen and future data. These methods observe the locations of the test points and their objective functions become monotonic in the predictive entropies at the test points. [ Kapoor et al., 2007 ] also includes an empirical error term 8",
  "compressed_prompt": "Z jD ) j f; If posterior + 1 approximated directly one gets jD ; ; BALD calculates entropy dierence ^ without having compute each candidate . In contrast, IVM calculates change . The IVM’s ap- proach cannot calculate full innite dimensional posterior, requires O N N updates. To updates eciently, proximate inference performed using Assumed Density Filtering (ADF). Using ADF means direct indicating IVM makes further BALD. Since BALD only O (1) can aord more accurate, iterative procedures, EP. Information Theoretic approaches: Maximum Entropy Sampling (MES) Sebastiani Wynn, 2000 explicitly works dataspace (Eqn. MES was proposed regression input-independent noise. Al- though Eqn. used, second constant input independent noise ignored. One cannot, however, MES heteroscedastic re- gression classication; fails dierentiate (about which our may be condent). Some toy demonstrations show ‘information based’ active learning criterion performing pathologically by repeatedly querying close boundary regions high e.g. Huang 2010 This MES inappropriate domain; BALD distinguishes be- tween eliminates problems we will show. Mutual-information based presented Fuhrmann, They being measured interest. Fuhrmann Fuhrmann, applies linear Gaussian acoustic arrays, al. communications channel. Although related, objectives work parameters applied classication. Guestrin 2005 Krause 2006 information. They specify interest advance distributions observed locations. Although promising regression, tractable input-dependent noise, such classication preference learning. Decision theoretic: We briey mention decision theoretic approaches ac- tive learning. Two closely related algorithms, Kapoor 2007 Zhu ], seek minimize expected cost i.e. loss weighted misclassication probability on all seen future data. These methods observe locations test their objective functions become monotonic predictive entropies test points. Kapoor 2007 also includes an empirical error term 8",
  "original_tokens": 527,
  "compressed_tokens": 263,
  "compression_ratio": 0.4990512333965844,
  "metadata": {
    "paper_id": "benchmark_100_paper_8",
    "dictionary_entries": null,
    "substitutions": null,
    "processing_time_ms": 0.195
  }
}