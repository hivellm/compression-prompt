{
  "test_id": "benchmark_200_paper_7_statistical_70",
  "technique": "statistical_70%",
  "original_prompt": "2 noise = 1. The likelihood only depends on the dierence between f ( u ) and f ( v ). We therefore dene g ( u ; v ) = f ( u ) f ( v ), and do inference entirely in terms of g , for which the likelihood becomes the same as for probit classication: y j u ; v ; f Bernoulli (( g ( u ; v ))). We observe that a GP prior is induced on g because it is formed by performing a linear operation on f , for which we have a GP prior already f GP (0 ; k ). We can derive the induced covariance function of g as (derivation in the Supplementary material) as: k pref (( u i ; v i ) ; ( u j ; v j )) = k ( u i ; u j ) + k ( v i ; v j ) k ( u i ; v j ) k ( v i ; u j ). Note that this kernel k pref respects the anti-symmetry properties desired for a preference learning scenario, i.e. the value g ( u; v ) is perfectly anti-correlated with g ( v; u ), ensuring P [ u v ] = 1 P [ v u ] holds. Thus, we can conclude that the GP preference learning framework of [ Chu and Ghahramani, 2005 ], is equivalent to GPC with a particular class of kernels, that we may call the preference judgement kernels . Therefore, our active learning algorithm presented in Section 3 for GPC can readily be applied to pairwise preference learning also. 4 Related Methodologies There are a number of closely related algorithms for active classication which we now review. The Informative Vector Machine (IVM): Perhaps the most closely re- lated approach is the IVM [ Lawrence et al., 2003 ]. This popular,and successful approach to active learning was designed specically for GPs; it uses an infor- mation theoretic approach and so appears very similar to BALD. The IVM algorithm was designed for subsampling a dataset for training a GP, so it is privy to the y values before including a measurement; it cannot therefore work explic- itly in output space i.e. with Eqn. (2) . The IVM uses Eqn. (1) , but parameter entropies are calculated approximately in the marginal subspace corresponding to the observed data points. The entropy decrease after inclusion of a new data point can then be calculated eciently using the GP covariance matrix. Although the IVM and BALD are motivated by the same objective, they work fundamentally dierently when approximate inference is carried out. At any time 7",
  "compressed_prompt": "2 noise = 1. The likelihood only depends on dierence between f ) and f ). We therefore dene g ) = f ) f ), and do inference entirely in terms of g , which likelihood becomes same as probit classication: y j f Bernoulli (( g ))). We observe that GP prior induced on g because it formed by performing linear operation on , which we have GP prior already GP (0 ). We can derive induced covariance function as (derivation Supplementary material) as: pref (( i )) = + ). Note that this kernel pref respects anti-symmetry properties desired preference learning scenario, i.e. value u; perfectly anti-correlated with v; ), ensuring P [ ] = 1 P [ ] holds. Thus, we can conclude that GP preference learning framework [ Chu Ghahramani, 2005 ], equivalent GPC with particular class kernels, that we may call preference judgement kernels . Therefore, our active learning algorithm presented Section 3 GPC can readily be applied pairwise preference learning also. 4 Related Methodologies There are number closely related algorithms active classication which we now review. The Informative Vector Machine (IVM): Perhaps most closely re- lated approach IVM [ Lawrence et al., 2003 ]. This popular,and successful approach active learning was designed specically GPs; it uses an infor- mation theoretic approach so appears very similar BALD. The IVM algorithm was designed subsampling dataset training GP, so it privy to y values before including measurement; it cannot therefore work explic- itly in output space i.e. with Eqn. (2) . The IVM uses Eqn. (1) , but parameter entropies are calculated approximately in marginal subspace corresponding observed data points. The entropy decrease after inclusion of new data point can then be calculated eciently using GP covariance matrix. Although IVM and BALD are motivated by same objective, they work fundamentally dierently when approximate inference is carried out. At any time 7",
  "original_tokens": 451,
  "compressed_tokens": 315,
  "compression_ratio": 0.6984478935698448,
  "metadata": {
    "paper_id": "benchmark_200_paper_7",
    "dictionary_entries": null,
    "substitutions": null,
    "processing_time_ms": 0.15669999999999998
  }
}