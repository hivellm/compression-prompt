{
  "test_id": "benchmark_200_paper_9_statistical_70",
  "technique": "statistical_70%",
  "original_prompt": "EP ( 1 ) Laplace ( 1 ) 7 : 51 2 : 51 41 : 57 4 : 02 0 : 16 0 : 05 7 : 43 2 : 40 40 : 45 3 : 67 Figure 2: Percentage approximation error ( 1 s.d.) for dierent methods of approximate inference ( columns ) and approximation methods for evaluating Eqn. (4) ( rows ). The results indicate that 2 is a very accurate approximation; EP causes some loss and Laplace signicantly more, which is in line with the comparison presented in [ Kuss and Rasmussen, 2005 ]. For our experiments we use EP. that can yield pathological behaviour (we investigate this experimentally). These approaches are computationally expensive, requiring O ( N x N y ) posterior updates. Also, they must know the locations of the test data (and thus are transductive approaches); designing an inductive, decision-theoretic algorithm is an open, hard problem as it would require expensive integration over possible test data distributions. Non-probabilistic Some non-probabilistic methods have close analogues to information theoretic active learning. Perhaps the most ubiquitous is active learning for SVMs [ Tong and Koller, 2001 , Seung et al., 1992 ], where the volume of Version Space (VS) is used as a proxy for the posterior entropy. If a uniform (improper) prior is used with a deterministic classication likelihood, the log volume of VS and Bayesian posterior entropy are in fact equivalent. Just as Bayesian posteriors become intractable after observing many data points, VS can become complicated. [ Tong and Koller, 2001 ] proposes methods for approximat- ing VS with a simple shapes, such as hyperspheres (their simplest approximation reduces to margin sampling). This closely resembles approximating a Bayesian posterior using a Gaussian distribution via the Laplace or EP approximations. [ Seung et al., 1992 ] sidesteps the problem by working with predictions. The al- gorithm, Query by Committee (QBC), samples parameters from VS (committee members), they vote on the outcome of each possible x . The x with the most balanced vote is selected; this is termed the ‘principle of maximal disagreement’. If BALD is used with a sampled posterior, query by committee is implemented but with a probabilistic measure of disagreement. QBC’s deterministic vote criterion discards condence in the predictions and so can exhibit the same pathologies as MES. 5 Experiments Quantifying Approximation Losses: To obtain (5) we made two approx- imations: we perform approximate inference ( 1 ), and we approximated the binary entropy of the Gaussian CDF by a squared exponential ( 2 ). Both of these can be substituted with Monte Carlo sampling, enabling us to compute 9",
  "compressed_prompt": "EP 1 ) Laplace 1 ) 7 51 2 51 41 57 4 02 0 16 0 05 7 43 2 40 40 45 3 67 Figure 2: Percentage approximation error s.d.) dierent methods approximate inference columns approximation methods evaluating Eqn. (4) rows ). The results indicate that very accurate approximation; EP causes some loss Laplace signicantly more, which line comparison presented Kuss Rasmussen, 2005 ]. For our experiments use EP. that yield pathological behaviour (we investigate this experimentally). These approaches computationally expensive, requiring O N N y updates. Also, they must know locations test (and thus transductive approaches); designing an inductive, decision-theoretic algorithm an open, hard problem it would require expensive integration over possible test data distributions. Non-probabilistic Some non-probabilistic have close analogues information theoretic active learning. Perhaps most ubiquitous active learning SVMs Tong Koller, 2001 , Seung et al., 1992 ], where volume Version Space (VS) proxy entropy. If uniform (improper) prior deterministic classication likelihood, log volume VS Bayesian entropy fact equivalent. Just Bayesian posteriors become intractable after observing many points, VS become complicated. Tong Koller, 2001 ] proposes approximat- ing VS simple shapes, such hyperspheres (their simplest approximation reduces margin sampling). This closely resembles approximating Bayesian using Gaussian distribution via Laplace or EP approximations. Seung et al., 1992 ] sidesteps problem working predictions. al- gorithm, Query Committee (QBC), samples parameters from VS (committee members), they on outcome each possible . most balanced selected; this termed ‘principle maximal disagreement’. If BALD used sampled posterior, query committee implemented but probabilistic measure disagreement. QBC’s deterministic vote criterion discards condence predictions so exhibit same pathologies MES. 5 Experiments Quantifying Approximation Losses: To obtain (5) made two approx- imations: we perform approximate inference 1 ), we approximated binary entropy Gaussian CDF squared exponential 2 ). Both these can be substituted Monte Carlo sampling, enabling us to compute 9",
  "original_tokens": 438,
  "compressed_tokens": 306,
  "compression_ratio": 0.6986301369863014,
  "metadata": {
    "paper_id": "benchmark_200_paper_9",
    "dictionary_entries": null,
    "substitutions": null,
    "processing_time_ms": 0.15669999999999998
  }
}