{
  "test_id": "benchmark_100_paper_5_hybrid",
  "technique": "hybrid",
  "original_prompt": "two entropy quantities. The rst term in Eqn. (2) , H[ y j x ; D ] can be handled analytically for the probit case: H[ y j x ; D ] 1 h Z ( f x ) N ( f x j x ; D ; 2 x ; D ) df x = h 0 @ 0 @ x ; D 2 x ; D + 1 1 A 1 A (3) The second term, E f p ( f jD ) [H[ y j x ; f ]] can be computed approximately as follows: E f p ( f jD ) [H[ y j x ; f ]] 1 Z h(( f x )) N ( f x j x ; D ; 2 x ; D ) df x (4) 2 Z exp f 2 x ln 2 N ( f x j x ; D ; 2 x ; D ) df x = C 2 x ; D + C 2 exp 0 @ 2 x ; D 2 2 x ; D + C 2 1 A where C = q\n2 . The rst approximation, 1 , reects the Gaussian ap- proximation to the posterior. The integral in the left hand side of Eqn. (4) is intractable. By performing a Taylor expansion on ln h(( f x )) (see supplementary material) we can see that it can be approximated up to O ( f 4 x ) by a squared exponential curve, exp ( f 2 x = ln 2). We will refer to this approximation as 2 . Now we can apply the standard convolution formula for Gaussians to nally get a closed form expression for both terms of Eqn. (2). Fig. 1 depicts the striking accuracy of this simple approximation. The max- imum possible error that will be incurred when using this approximation is if N ( f x j x ; D ; 2 x ; D ) is centred at x ; D = 2 : 05 with 2 x ; D tending to zero (see Fig. 1, absolute error ), yielding only a 0.27% error in the integral in Eqn. (4) . The authors are unaware of previous use of this simple and useful approximation in this context. In Section 5 we investigate experimentally the information lost from approximations 1 and 2 as compared to the golden standard of extensive Monte Carlo simulation. To summarise, the BALD algorithm for Gaussian process classication con- sists of two steps. First it applies any standard approximate inference algorithm for GPCs (such as EP) to obtain the posterior predictive mean x ; D and x ; D for each point of interest x . Then, it selects a query x that maximises the following objective function: h 0 @ 0 @ x ; D 2 x ; D + 1 1 A 1 A C exp 2 x ; D 2 ( 2 x ; D + C 2 ) 2 x ; D + C 2 (5) 5",
  "compressed_prompt": "[RULES] - Always expand ⟦n⟧ using DICT[n] before reasoning. - Do not print DICT result, only use it understand context. [DICT] 1: D [BODY] two entropy quantities. rst term Eqn. (2) , H[ y ] handled analytically probit case: H[ y ] h Z N df h @ @ (3) second term, E p jD [H[ ]] computed approximately follows: E p jD [H[ ]] Z h(( )) N (4) Z N where q rst approximation, , reects Gaussian ap- proximation posterior. integral left hand side Eqn. (4) intractable. By performing Taylor expansion on ln h(( )) (see supplementary material) we see that approximated up O 4 by squared exponential curve, ln 2). We will refer approximation Now we apply standard convolution formula Gaussians nally get closed form expression both terms Eqn. (2). Fig. depicts striking accuracy simple approximation. max- imum possible error that will incurred when using approximation if N centred at : 05 with tending zero (see Fig. 1, absolute error ), yielding only 0.27% error integral Eqn. (4) authors are unaware previous use simple useful approximation context. In Section 5 we investigate experimentally information lost from approximations compared golden standard extensive Monte Carlo simulation. To summarise, BALD algorithm Gaussian process classication con- sists two steps. First it applies any standard approximate inference algorithm GPCs (such EP) obtain posterior predictive mean each point interest . Then, it selects query that maximises following objective function: h @ @ C exp C C (5) 5",
  "original_tokens": 509,
  "compressed_tokens": 245,
  "compression_ratio": 0.481335952848723,
  "metadata": {
    "paper_id": "benchmark_100_paper_5",
    "dictionary_entries": 1,
    "substitutions": 24,
    "processing_time_ms": 1.2463000000000002
  }
}