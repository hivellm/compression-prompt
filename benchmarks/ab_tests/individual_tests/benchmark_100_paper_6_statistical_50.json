{
  "test_id": "benchmark_100_paper_6_statistical_50",
  "technique": "statistical_50%",
  "original_prompt": "h (( )) exp( t 2 log(2) ) 5 10 3 dierence Figure 1: Analytic approximation ( 1 ) to the binary entropy of the error function ( ) by a squared exponential ( ). The absolute error ( ) remains under 3 10 3 . For most practically relevant kernels, the objective (5) is a smooth and dierentiable function of x , so gradient-based optimisation procedures can be used to nd the maximally informative query. 3.1 Extension: Learning Hyperparameters In many applications the parameter set naturally divides into parameters of interest, + , and nuisance parameters , i.e. = f + ; g . In such settings, the active learning may want to query points that are maximally informative about + , while not caring about . By integrating Eqn. (1) over the nuisance parameters, , BALD’s objective is re-derived as: H E p ( + ; jD ) y j x ; + ; E p ( + jD ) H E p ( j + ; D ) [ y j x ; + ; ] (6) In the context of GP models, hyperparameters typically control the smooth- ness or spatial length-scale of functions. If we maintain a posterior distribution over these hyperparameters, which we can do e. g. via Hamiltonian Monte Carlo, we can choose either to treat them as nuisance parameters and use Eq. 6, or to include them in + and perform active learning over them as well. In certain cases, such as automatic relevance determination [ Rasmussen and Williams, 2005 ], it may even make sense to treat hyperparameters as variables of primary interest, and the function f itself as nuisance parameter . 3.2 Preference Learning Our active learning framework for GPC can be extended to the important problem of preference learning [ Furnkranz and Hullermeier, 2003 , Chu and Ghahramani, 2005 ]. In preference learning the dataset consists for pairs of items ( u i ; v i ) 2 X 2 with binary labels, y i 2 f 0 ; 1 g . y i = 1 means instance u i is preferred to v i , denoted u i v i . The task is to predict the preference relation between any ( u ; v ). We can view this as a special case of building a classier on pairs of inputs 6",
  "compressed_prompt": "h (( )) exp( t log(2) 5 10 3 dierence Figure 1: Analytic approximation 1 binary entropy error function by squared exponential ). The absolute error remains under 10 For most practically relevant kernels, objective (5) smooth dierentiable so gradient-based optimisation procedures used nd maximally informative query. 3.1 Extension: Learning Hyperparameters many applications parameter set naturally divides into interest, i.e. = g such settings, may want query points that are maximally informative about while not caring about By integrating Eqn. (1) parameters, BALD’s objective re-derived as: H E jD E H E D ] (6) context GP models, hyperparameters typically control smooth- ness spatial length-scale functions. If maintain posterior distribution these hyperparameters, which do e. g. via Hamiltonian Monte Carlo, choose either treat use Eq. 6, include in perform well. certain cases, automatic relevance determination Rasmussen Williams, ], it even make sense hyperparameters variables primary interest, itself parameter 3.2 Preference Learning Our framework GPC extended important problem Furnkranz Hullermeier, 2003 Chu Ghahramani, ]. dataset consists pairs items X with binary labels, 0 g = means instance preferred denoted The task predict preference relation between any ). We view this special case building classier on pairs inputs 6",
  "original_tokens": 394,
  "compressed_tokens": 197,
  "compression_ratio": 0.5,
  "metadata": {
    "paper_id": "benchmark_100_paper_6",
    "dictionary_entries": null,
    "substitutions": null,
    "processing_time_ms": 0.14229999999999998
  }
}