{
  "test_id": "benchmark_200_paper_1_statistical_50",
  "technique": "statistical_50%",
  "original_prompt": "Bayesian Active Learning for Classication and Preference Learning Neil Houlsby, Ferenc Huszar, Zoubin Ghahramani, Mate Lengyel Computational and Biological Learning Laboratory University of Cambridge November 27, 2024 Abstract Information theoretic active learning has been widely studied for prob- abilistic models. For simple regression an optimal myopic policy is easily tractable. However, for other tasks and with more complex models, such as classication with nonparametric models, the optimal solution is harder to compute. Current approaches make approximations to achieve tractabil- ity. We propose an approach that expresses information gain in terms of predictive entropies, and apply this method to the Gaussian Process Classier (GPC). Our approach makes minimal approximations to the full information theoretic objective. Our experimental performance compares favourably to many popular active learning algorithms, and has equal or lower computational complexity. We compare well to decision theoretic approaches also, which are privy to more information and require much more computational time. Secondly, by developing further a reformulation of binary preference learning to a classication problem, we extend our algorithm to Gaussian Process preference learning. 1 Introduction In most machine learning systems, the learner passively collects data with which it makes inferences about its environment. In active learning, however, the learner seeks the most useful measurements to be trained upon. The goal of active learning is to produce the best model with the least possible data; this is closely related to the statistical eld of optimal experimental design. With the advent of the internet and expansion of storage facilities, vast quantities of unlabelled data have become available, but it can be costly to obtain labels. Finding the most useful data in this vast space calls for ecient active learning algorithms. Two approaches to active learning are to use decision and information the- ory [ Kapoor et al., 2007 , Lindley, 1956 ]. The former minimizes the expected 1",
  "compressed_prompt": "Bayesian Active Classication Preference Neil Houlsby, Ferenc Huszar, Zoubin Ghahramani, Mate Lengyel Computational Biological Laboratory University Cambridge November 27, 2024 Abstract Information been widely studied prob- abilistic models. For simple regression myopic policy easily tractable. However, other tasks complex such as nonparametric solution harder compute. Current make achieve tractabil- ity. propose that expresses gain terms predictive entropies, apply method Classier (GPC). minimal full objective. performance compares favourably many popular algorithms, equal lower complexity. compare well also, privy require much time. Secondly, developing further reformulation binary problem, we extend our algorithm learning. Introduction machine systems, passively collects inferences about its environment. learning, however, seeks measurements trained upon. goal produce best model least possible data; closely related statistical eld design. advent internet expansion storage facilities, quantities unlabelled have become available, but can costly obtain labels. Finding space calls ecient algorithms. Two use the- ory [ Kapoor et al., 2007 , Lindley, 1956 ]. former minimizes expected",
  "original_tokens": 308,
  "compressed_tokens": 154,
  "compression_ratio": 0.5,
  "metadata": {
    "paper_id": "benchmark_200_paper_1",
    "dictionary_entries": null,
    "substitutions": null,
    "processing_time_ms": 0.1307
  }
}