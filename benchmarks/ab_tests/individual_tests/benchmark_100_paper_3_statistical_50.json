{
  "test_id": "benchmark_100_paper_3_statistical_50",
  "technique": "statistical_50%",
  "original_prompt": "rameters is inferred, p ( jD ). The central goal of information theoretic ac- tive learning is to reduce the number possible hypotheses maximally fast, i.e. to minimize the uncertainty about the parameters using Shannon’s entropy [ Cover et al., 1991 ]. Data points D 0 are selected that satisfy arg min D 0 H[ jD 0 ] = R p ( jD 0 ) log p ( jD 0 )d . Solving this problem in general is NP-hard; however, as is common in sequential decision making tasks a myopic (greedy) approxi- mation is made [ Heckerman et al., 1995 ]. It has been shown that the myopic policy can perform near-optimally [ Golovin and Krause, 2010 , Dasgupta, 2005 ]. Therefore, the objective is to seek the data point x that maximises the decrease in expected posterior entropy: arg max x H[ jD ] E y p ( y j x D ) [H[ j y; x ; D ]] (1) Note that expectation over the unseen output y is required. Many works e.g. [ MacKay, 1992 , Krishnapuram et al., , Lawrence et al., 2003 ] propose using this objective directly. However, parameter posteriors are often high dimen- sional and computing their entropies is usually intractable. Furthermore, for nonparametric processes the parameter space is innite dimensional so Eqn. (1) becomes poorly dened. To avoid gridding parameter space (exponentially hard with dimensionality), or sampling (from which it is notoriously hard to estimate entropies without introducing bias [ Panzeri and Petersen, 2007 ]), these papers make Gaussian or low dimensional approximations and calculate the entropy of the approximate posterior. A second computational diculty arises; if N x data points are under consideration, and N y responses may be seen, then O ( N x N y ), potentially expensive, posterior updates are required to calculate Eqn. (1). An important insight arises if we note that the objective in Eqn. (1) is equivalent to the conditional mutual information between the unknown output and the parameters, I[ ; y j x ; D ]. Using this insight it is simple to show that the objective can be rearranged to compute entropies in y space: arg max x H[ y j x ; D ] E p ( jD ) [H[ y j x ; ]] (2) Eqn. (2) overcomes the challenges we described for Eqn. (1) . Entropies are now calculated in, usually low dimensional, output space. For binary classication, these are just entropies of Bernoulli variables. Also is now conditioned only on D , so only O (1) posterior updates are required. Eqn. (2) also provides us with an interesting intuition about the objective; we seek the x for which the model is marginally most uncertain about y (high H[ y j x ; D ]), but for which individual settings of the parameters are condent (low E p ( jD ) [H[ y j x ; ]] ). This can be interpreted as seeking the x for which the parameters under the posterior disagree about the outcome the most, so we refer to this objective as Bayesian Active Learning by Disagreement (BALD). We present a method to apply Eqn. (2) directly to GPC and preference learning. We no longer need to build our entropy calculation around the type of posterior approximation (as 3",
  "compressed_prompt": "rameters inferred, ). The central goal information theoretic ac- tive learning reduce number possible hypotheses maximally fast, i.e. minimize uncertainty parameters using Shannon’s entropy Cover 1991 Data points selected satisfy arg min H[ = R log )d . Solving problem general NP-hard; however, common sequential decision making tasks a myopic (greedy) approxi- mation made Heckerman 1995 It has been shown myopic policy perform near-optimally Golovin Krause, 2010 Dasgupta, 2005 Therefore, data point maximises decrease expected entropy: max E y; Note expectation over unseen required. Many works e.g. MacKay, 1992 Krishnapuram Lawrence 2003 propose using directly. However, posteriors often high dimen- sional computing their usually intractable. Furthermore, nonparametric processes space innite dimensional becomes poorly dened. To avoid gridding space (exponentially hard dimensionality), sampling (from it notoriously hard estimate without introducing bias Panzeri Petersen, 2007 ]), these papers make Gaussian low dimensional approximations calculate approximate posterior. A second computational diculty arises; if data points consideration, responses may seen, then O ), potentially expensive, updates required calculate (1). An important insight arises if note equivalent conditional mutual information between unknown parameters, I[ Using insight it simple show rearranged compute space: E overcomes challenges described . Entropies now calculated in, usually low dimensional, space. For binary classication, these just Bernoulli variables. Also now conditioned only on O updates required. also provides us an interesting intuition objective; seek model marginally most uncertain (high H[ ]), but individual settings parameters condent (low E ). This interpreted seeking parameters under disagree outcome most, so refer Bayesian Active Learning by Disagreement (BALD). We present a method apply directly GPC preference learning. We no longer need build our entropy calculation around type approximation (as 3",
  "original_tokens": 553,
  "compressed_tokens": 276,
  "compression_ratio": 0.49909584086799275,
  "metadata": {
    "paper_id": "benchmark_100_paper_3",
    "dictionary_entries": null,
    "substitutions": null,
    "processing_time_ms": 0.2041
  }
}