{
  "test_id": "benchmark_100_paper_5_dictionary",
  "technique": "dictionary",
  "original_prompt": "two entropy quantities. The rst term in Eqn. (2) , H[ y j x ; D ] can be handled analytically for the probit case: H[ y j x ; D ] 1 h Z ( f x ) N ( f x j x ; D ; 2 x ; D ) df x = h 0 @ 0 @ x ; D 2 x ; D + 1 1 A 1 A (3) The second term, E f p ( f jD ) [H[ y j x ; f ]] can be computed approximately as follows: E f p ( f jD ) [H[ y j x ; f ]] 1 Z h(( f x )) N ( f x j x ; D ; 2 x ; D ) df x (4) 2 Z exp f 2 x ln 2 N ( f x j x ; D ; 2 x ; D ) df x = C 2 x ; D + C 2 exp 0 @ 2 x ; D 2 2 x ; D + C 2 1 A where C = q\n2 . The rst approximation, 1 , reects the Gaussian ap- proximation to the posterior. The integral in the left hand side of Eqn. (4) is intractable. By performing a Taylor expansion on ln h(( f x )) (see supplementary material) we can see that it can be approximated up to O ( f 4 x ) by a squared exponential curve, exp ( f 2 x = ln 2). We will refer to this approximation as 2 . Now we can apply the standard convolution formula for Gaussians to nally get a closed form expression for both terms of Eqn. (2). Fig. 1 depicts the striking accuracy of this simple approximation. The max- imum possible error that will be incurred when using this approximation is if N ( f x j x ; D ; 2 x ; D ) is centred at x ; D = 2 : 05 with 2 x ; D tending to zero (see Fig. 1, absolute error ), yielding only a 0.27% error in the integral in Eqn. (4) . The authors are unaware of previous use of this simple and useful approximation in this context. In Section 5 we investigate experimentally the information lost from approximations 1 and 2 as compared to the golden standard of extensive Monte Carlo simulation. To summarise, the BALD algorithm for Gaussian process classication con- sists of two steps. First it applies any standard approximate inference algorithm for GPCs (such as EP) to obtain the posterior predictive mean x ; D and x ; D for each point of interest x . Then, it selects a query x that maximises the following objective function: h 0 @ 0 @ x ; D 2 x ; D + 1 1 A 1 A C exp 2 x ; D 2 ( 2 x ; D + C 2 ) 2 x ; D + C 2 (5) 5",
  "compressed_prompt": "[RULES]\n- Always expand ⟦n⟧ using DICT[n] before reasoning.\n- Do not print DICT in the result, only use it to understand context.\n\n[DICT]\n1: x ; D\n\n\n[BODY]\ntwo entropy quantities. The rst term in Eqn. (2) , H[ y j ⟦1⟧ ] can be handled analytically for the probit case: H[ y j ⟦1⟧ ] 1 h Z ( f x ) N ( f x j ⟦1⟧ ; 2 ⟦1⟧ ) df x = h 0 @ 0 @ ⟦1⟧ 2 ⟦1⟧ + 1 1 A 1 A (3) The second term, E f p ( f jD ) [H[ y j x ; f ]] can be computed approximately as follows: E f p ( f jD ) [H[ y j x ; f ]] 1 Z h(( f x )) N ( f x j ⟦1⟧ ; 2 ⟦1⟧ ) df x (4) 2 Z exp f 2 x ln 2 N ( f x j ⟦1⟧ ; 2 ⟦1⟧ ) df x = C 2 ⟦1⟧ + C 2 exp 0 @ 2 ⟦1⟧ 2 2 ⟦1⟧ + C 2 1 A where C = q\n2 . The rst approximation, 1 , reects the Gaussian ap- proximation to the posterior. The integral in the left hand side of Eqn. (4) is intractable. By performing a Taylor expansion on ln h(( f x )) (see supplementary material) we can see that it can be approximated up to O ( f 4 x ) by a squared exponential curve, exp ( f 2 x = ln 2). We will refer to this approximation as 2 . Now we can apply the standard convolution formula for Gaussians to nally get a closed form expression for both terms of Eqn. (2). Fig. 1 depicts the striking accuracy of this simple approximation. The max- imum possible error that will be incurred when using this approximation is if N ( f x j ⟦1⟧ ; 2 ⟦1⟧ ) is centred at ⟦1⟧ = 2 : 05 with 2 ⟦1⟧ tending to zero (see Fig. 1, absolute error ), yielding only a 0.27% error in the integral in Eqn. (4) . The authors are unaware of previous use of this simple and useful approximation in this context. In Section 5 we investigate experimentally the information lost from approximations 1 and 2 as compared to the golden standard of extensive Monte Carlo simulation. To summarise, the BALD algorithm for Gaussian process classication con- sists of two steps. First it applies any standard approximate inference algorithm for GPCs (such as EP) to obtain the posterior predictive mean ⟦1⟧ and ⟦1⟧ for each point of interest x . Then, it selects a query x that maximises the following objective function: h 0 @ 0 @ ⟦1⟧ 2 ⟦1⟧ + 1 1 A 1 A C exp 2 ⟦1⟧ 2 ( 2 ⟦1⟧ + C 2 ) 2 ⟦1⟧ + C 2 (5) 5",
  "original_tokens": 509,
  "compressed_tokens": 490,
  "compression_ratio": 0.9626718759536743,
  "metadata": {
    "paper_id": "benchmark_100_paper_5",
    "dictionary_entries": 1,
    "substitutions": 24,
    "processing_time_ms": 1.0406
  }
}