{
  "test_id": "benchmark_100_paper_4_statistical_70",
  "technique": "statistical_70%",
  "original_prompt": "in [ MacKay, 1992 , Krishnapuram et al., , Lawrence et al., 2003 ]) but are free to choose from many of the available algorithms. Minimal additional approximations are introduced, and so, to our knowledge our algorithm represents the most exact and fastest way to perform full information-theoretic active learning in non-parametric discriminative models. 3 Gaussian Processes for Classication and Pref- erence Learning In this section we derive the BALD algorithm for Gaussian Process classication (GPC). GPs are a powerful and popular non-parametric tool for regression and classication. GPC appears to be an especially challenging problem for information-theoretic active learning because the parameter space is innite, however, by using (2) we are able to calculate fully the relevant information quantities without having to work out entropies of innite dimensional objects. The probabilistic model underlying GPC is as follows: f GP( ( ) ; k ( ; )) y j x ; f Bernoulli(( f ( x ))) The latent parameter, now called f is a function X ! R , and is assigned a Gaussian process prior with mean ( ) and covariance function or kernel k ( ; ). We consider the probit case where given the value of f , y takes a Bernoulli distribution with probability ( f ( x )), and is the Gaussian CDF. For further details on GPs see [Rasmussen and Williams, 2005]. Inference in the GPC model is intractable; given some observations D , the posterior over f becomes non-Gaussian and complicated. The most commonly used approximate inference methods { EP, Laplace approximation, Assumed Density Filtering and sparse methods { all approximate the posterior by a Gaussian [ Rasmussen and Williams, 2005 ]. Throughout this section we will assume that we are provided with such a Gaussian approximation from one of these methods, though the active learning algorithm does not care which one. In our derivation we will use 1 to indicate where such an approximation is exploited. The informativeness of a query x is computed using Eqn. (2) . The entropy of the binary output variable y given a xed f can be expressed in terms of the binary entropy function h: H[ y j x ; f ] = h (( f ( x )) h( p ) = p log p (1 p ) log(1 p ) Expectations over the posterior need to be computed. Using a Gaussian approxi- mation to the posterior, for each x , f x = f ( x ) will follow a Gaussian distribution with mean x ; D and variance 2 x ; D . To compute Eqn. (2) we have to compute 4",
  "compressed_prompt": "in [ MacKay, 1992 , Krishnapuram et al., , Lawrence et al., 2003 ]) but are free choose from many available algorithms. Minimal additional approximations are introduced, so, our knowledge our algorithm represents most exact fastest way perform full information-theoretic active learning in non-parametric discriminative models. 3 Gaussian Processes Classication Pref- erence Learning In this section we derive BALD algorithm Gaussian Process classication (GPC). GPs powerful popular non-parametric tool regression classication. GPC appears be an especially challenging problem information-theoretic active learning because parameter space innite, however, by using (2) able calculate fully relevant information quantities without having work out entropies innite dimensional objects. probabilistic model underlying GPC as follows: GP( k )) y j Bernoulli(( ))) latent parameter, now called function X ! R assigned Gaussian process prior mean covariance function or kernel k ). We consider probit case where given value y takes Bernoulli distribution probability )), Gaussian CDF. For further details on GPs see [Rasmussen Williams, 2005]. Inference GPC model intractable; given some observations D posterior over becomes non-Gaussian complicated. most commonly used approximate inference methods { EP, Laplace approximation, Assumed Density Filtering sparse methods { all approximate posterior by [ Rasmussen Williams, 2005 ]. Throughout this section will assume that provided such approximation from one these methods, though active learning algorithm does not care which one. In our derivation will use 1 indicate where such an approximation exploited. informativeness query computed using Eqn. (2) . entropy binary output variable y given xed can be expressed terms binary entropy function h: H[ y j ] = h (( )) h( p = p log p (1 p log(1 p Expectations over posterior need be computed. Using Gaussian approxi- mation posterior, for each , = ) will follow Gaussian distribution with mean ; D variance 2 ; D . To compute Eqn. (2) we have compute 4",
  "original_tokens": 440,
  "compressed_tokens": 308,
  "compression_ratio": 0.7,
  "metadata": {
    "paper_id": "benchmark_100_paper_4",
    "dictionary_entries": null,
    "substitutions": null,
    "processing_time_ms": 0.1664
  }
}