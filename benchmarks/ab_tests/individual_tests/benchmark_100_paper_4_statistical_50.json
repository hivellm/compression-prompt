{
  "test_id": "benchmark_100_paper_4_statistical_50",
  "technique": "statistical_50%",
  "original_prompt": "in [ MacKay, 1992 , Krishnapuram et al., , Lawrence et al., 2003 ]) but are free to choose from many of the available algorithms. Minimal additional approximations are introduced, and so, to our knowledge our algorithm represents the most exact and fastest way to perform full information-theoretic active learning in non-parametric discriminative models. 3 Gaussian Processes for Classication and Pref- erence Learning In this section we derive the BALD algorithm for Gaussian Process classication (GPC). GPs are a powerful and popular non-parametric tool for regression and classication. GPC appears to be an especially challenging problem for information-theoretic active learning because the parameter space is innite, however, by using (2) we are able to calculate fully the relevant information quantities without having to work out entropies of innite dimensional objects. The probabilistic model underlying GPC is as follows: f GP( ( ) ; k ( ; )) y j x ; f Bernoulli(( f ( x ))) The latent parameter, now called f is a function X ! R , and is assigned a Gaussian process prior with mean ( ) and covariance function or kernel k ( ; ). We consider the probit case where given the value of f , y takes a Bernoulli distribution with probability ( f ( x )), and is the Gaussian CDF. For further details on GPs see [Rasmussen and Williams, 2005]. Inference in the GPC model is intractable; given some observations D , the posterior over f becomes non-Gaussian and complicated. The most commonly used approximate inference methods { EP, Laplace approximation, Assumed Density Filtering and sparse methods { all approximate the posterior by a Gaussian [ Rasmussen and Williams, 2005 ]. Throughout this section we will assume that we are provided with such a Gaussian approximation from one of these methods, though the active learning algorithm does not care which one. In our derivation we will use 1 to indicate where such an approximation is exploited. The informativeness of a query x is computed using Eqn. (2) . The entropy of the binary output variable y given a xed f can be expressed in terms of the binary entropy function h: H[ y j x ; f ] = h (( f ( x )) h( p ) = p log p (1 p ) log(1 p ) Expectations over the posterior need to be computed. Using a Gaussian approxi- mation to the posterior, for each x , f x = f ( x ) will follow a Gaussian distribution with mean x ; D and variance 2 x ; D . To compute Eqn. (2) we have to compute 4",
  "compressed_prompt": "[ MacKay, 1992 Krishnapuram et al., Lawrence et al., 2003 ]) but free choose from many available algorithms. Minimal additional approximations introduced, so, knowledge algorithm represents most exact fastest way perform full information-theoretic non-parametric discriminative models. 3 Processes Classication Pref- erence Learning In this section derive BALD Process classication (GPC). GPs powerful popular non-parametric tool regression classication. GPC appears especially challenging problem information-theoretic because parameter space innite, however, able calculate fully relevant information quantities without having work out entropies innite dimensional objects. probabilistic underlying GPC as follows: GP( Bernoulli(( ))) latent parameter, now called X ! R assigned process prior covariance or kernel ). We consider probit case value takes Bernoulli distribution probability )), CDF. For further details on GPs see [Rasmussen Williams, 2005]. Inference GPC intractable; some observations D becomes non-Gaussian complicated. commonly used approximate inference methods EP, Laplace approximation, Assumed Density Filtering sparse methods all approximate Rasmussen Williams, 2005 ]. Throughout section assume that provided approximation one these methods, though does not care which one. derivation use 1 indicate approximation exploited. informativeness query computed Eqn. entropy output variable xed can expressed terms binary entropy h: H[ j ] h (( )) h( log (1 log(1 Expectations over need computed. Using approxi- mation posterior, each follow distribution mean D variance 2 D . To compute Eqn. have compute 4",
  "original_tokens": 440,
  "compressed_tokens": 220,
  "compression_ratio": 0.5,
  "metadata": {
    "paper_id": "benchmark_100_paper_4",
    "dictionary_entries": null,
    "substitutions": null,
    "processing_time_ms": 0.1659
  }
}