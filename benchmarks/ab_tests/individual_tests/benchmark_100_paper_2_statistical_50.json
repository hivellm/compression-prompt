{
  "test_id": "benchmark_100_paper_2_statistical_50",
  "technique": "statistical_50%",
  "original_prompt": "losses encountered after making decisions based on the data collected i.e. min- imize the Bayes posterior risk [ Roy and McCallum, 2001 ]. Maximising perfor- mance under test is the ultimate objective of most learners, however, evaluat- ing this objective can be very hard. For example, the methods proposed in [ Kapoor et al., 2007 , Zhu et al., 2003 ] for classication are in general expensive to compute. Furthermore, one may not know the loss function or test distribution in advance, or may want the model to perform well on a variety of loss functions. In extreme scenarios, such as exploratory data analysis, or visualisation, losses may be very hard to quantify. This motivates information theoretic approaches to active learning, which are agnostic to the decision task at hand and particular test data, this is known an inductive approach. They seek to reduce the number of feasible models as quickly as possible, using either heuristics (e.g. margin sampling [ Tong and Koller, 2001 ]) or by formalising uncertainty using well studied quantities, such as Shannons entropy and the KL-divergence [ Cover et al., 1991 ]. Although the latter approach was proposed several decades ago [ Lindley, 1956 , Bernardo, 1979 ], it is not always straightforward to apply the criteria to complicated models such as nonparametric processes with innite parameter spaces. As a result many algorithms exist which compute approximate posterior entropies, perform sampling, or work with related quantities in non-probabilistic models. We return to this problem, presenting the full information criterion and demonstrate how to apply it to Gaussian Processes Classication (GPC), yielding a novel active learning algorithm that makes minimal approximations. GPC is a powerful, non-parametric kernel-based model, and poses an interesting problem for information-theoretic active learning because the parameter space is innite dimensional and the posterior distribution is analytically intractable. We present the information theoretic approach to active learning in Section 2. In Section 3 we apply it to GPC, and show how to extended our method to preference learning. In Section 4 we review other approaches and how they compare to our algorithm. We take particular care to contrast our approach to the Informative Vector Machine, that addresses data point selection for GPs directly. We present results on a wide variety of datasets in Section 5 and conclude in Section 6. 2 Bayesian Information Theoretic Active Learn- ing We consider a fully discriminative model where the goal of active learning is to discover the dependence of some variable y 2 Y on an input variable x 2 X . The key idea in active learning is that the learner chooses the input queries x i 2 X and observes the system’s response y i , rather than passively receiving ( x i y i ) pairs. Within a Bayesian framework we assume existence of some latent param- eters, , that control the dependence between inputs and outputs, p ( y j x ; ). Having observed data D = f ( x i ; y i ) g n i =1 , a posterior distribution over the pa- 2",
  "compressed_prompt": "losses encountered after making decisions based collected i.e. min- imize Bayes risk Roy McCallum, 2001 ]. Maximising perfor- mance under ultimate objective most learners, however, evaluat- ing objective can very hard. For example, methods proposed Kapoor 2007 Zhu 2003 ] classication general expensive compute. Furthermore, one not know loss function advance, want model perform well variety loss functions. extreme scenarios, exploratory analysis, visualisation, hard quantify. This motivates learning, agnostic decision task at hand data, known inductive approach. They seek reduce number feasible quickly possible, either heuristics (e.g. margin sampling Tong Koller, ]) by formalising uncertainty studied quantities, Shannons entropy KL-divergence Cover 1991 Although latter was several decades ago Lindley, 1956 Bernardo, 1979 ], always straightforward criteria complicated nonparametric processes spaces. As result many algorithms exist compute approximate entropies, sampling, work related quantities non-probabilistic models. return problem, presenting full criterion demonstrate Gaussian Processes Classication (GPC), yielding novel algorithm makes minimal approximations. GPC powerful, non-parametric kernel-based model, poses interesting problem information-theoretic because space dimensional analytically intractable. 2. 3 GPC, show extended method preference learning. 4 review other they compare algorithm. take care contrast Informative Vector Machine, addresses point selection GPs directly. results wide datasets 5 conclude 6. Bayesian Information Theoretic Active Learn- consider fully discriminative where goal discover variable Y input variable X . The key idea learner chooses queries X observes system’s response rather than passively receiving pairs. Within Bayesian framework assume existence some latent param- eters, control dependence between inputs outputs, p j ; ). Having observed D = f ; ) g n =1 over pa-",
  "original_tokens": 514,
  "compressed_tokens": 257,
  "compression_ratio": 0.5,
  "metadata": {
    "paper_id": "benchmark_100_paper_2",
    "dictionary_entries": null,
    "substitutions": null,
    "processing_time_ms": 0.29619999999999996
  }
}