4.2 Convergence of Algorithm 1 Proposition 2. If G and D have enough capacity, and at each step of Algorithm 1, discriminator is allowed to reach its optimum given G , and is updated so as to improve criterion E data [log D G )] + E [log(1 D G ))] then converges to data Proof. Consider V G; D = U ; D as a function as done in above criterion. Note that U ; D convex in . The subderivatives a supremum convex functions include derivative function at point where maximum attained. In other words, if f = sup 2A f and f convex in for every , then @f 2 @f if = arg sup 2A f . This equivalent to computing a gradient descent update for at optimal D given cor- responding G . sup D U ; D ) is convex in with a unique global optima as proven in Thm 1, therefore with sufciently small updates of , converges to x , concluding proof.