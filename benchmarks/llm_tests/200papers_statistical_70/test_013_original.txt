best, or amongst the best performing algorithms on all datasets. On any individ- ual dataset BALD’s performance is often matched because we compare to many methods, and the more approximate algorithms can have good performance under dierent conditions. Fig. 5 reveals that BALD has the best overall perfor- mance; on average, all other methods require more data points to achieve the same classication accuracy. Zhu et al. ’s decision theoretic approach is closest, the median increase in the number of data points required is 1 : 4 and zero (i.e. equivalent to BALD) is within the inter-quartile range. This algorithm, however, requires much more computational time and has access to the full set of test inputs, which BALD does not have. MES and QBC appear close in performance to BALD, but the zero line falls outside both of their inter-quartile ranges. As expected, MES performs poorly on the noisy dataset (Fig. 3(a)) because it discards knowledge of observation noise. When there is zero observation noise it is equivalent to BALD e.g. Fig. 3(c). On many of the real-world datasets MES performs as well as BALD e.g. Fig. 4(b, e), indicating that these datasets are mostly noise-free. The IVM performs well on Fig. 3(c), but pathologically on 3(a); this is due to the fact that it biases selection towards points from only one class in the noisy cluster, reducing the posterior entropy rapidly but articially. However, it also performs signicantly worse than BALD on noise-free (indicated by MES’s strong performance) datasets e.g. Fig. 4(b). This implies that the IVM’s posterior approximation or the ADF update are detrimental to the algorithm’s performance. QBC often yields only a small decrement in performance, the sampling approximation is often not too detrimental. However, it performs poorly on the noisy articial dataset (Fig. 3(a)) because the vote criterion is not maintaining a notion of inherent uncertainty, like MES. The SVM-based approach exhibits variable performance (it does well on Fig. 4(d), but very poorly on 4(f)). The performance is greatly eected by the approximation used, for consistency we present here one that yielded the most consistent good performance. Decision theoretic approaches sometimes perform well, on 3(c) they choose the rst 16 points from the centre of each cluster as they are inuenced by the surrounding unlabelled points. BALDdoes not observe the unlabelled points so may not pick points from the centres. Fig. 5 reveals that BALD is performing as well as the method in [ Zhu et al., 2003 ], and outperforms the approach in [ Kapoor et al., 2007 ], despite not having access to the locations of the test points and having a signicantly lower computational cost. The objective in [ Kapoor et al., 2007 ] can fail, this is because one term in their objective function is the empirical error. The weight given to this term is determined by the relative sizes of the training and test set (and the associated losses). Directly minimizing empirical error usually performs very pathologically, picking only ‘safe’ points. When the method in [ Kapoor et al., 2007 ] assigns too much weight to this term, it can fail also. Finally we note that BALD may occasionally perform poorly on the rst few data points (e.g. Fig. 4(l)). This is may be because the hyperparameters are xed throughout the experiments to provide a fair comparison to algorithms 13