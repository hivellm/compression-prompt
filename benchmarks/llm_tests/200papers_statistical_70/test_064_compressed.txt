Table 1: upper table compares average total reward various methods by running -greedy policy = 0 : 05 xed number steps. lower table reports single best performing episode HNeat DQN. HNeat produces deterministic policies always get same score while DQN used -greedy policy = 0 : 05 . types objects Atari screen. HNeat Pixel score obtained by special color channel representation Atari emulator represents object label map at each channel. This method relies heavily nding deterministic sequence states represents successful exploit. It unlikely strategies learnt this way will generalize random perturbations; therefore algorithm evaluated highest scoring single episode. contrast, algorithm evaluated control sequences, must therefore generalize across wide variety possible situations. Nevertheless, show all games, except Space Invaders, not max evaluation (row ), but also average (row 4 ) achieve better performance. Finally, show method achieves better performance than expert player Breakout, Enduro Pong it achieves close performance Beam Rider. games Q*bert, Seaquest, Space Invaders, which far from performance, more chal- lenging because they require network nd strategy extends over long time scales. 6 Conclusion This paper introduced new deep model reinforcement learning, demonstrated its ability master difcult control policies Atari 2600 computer games, raw pixels as input. We also presented variant online Q-learning combines stochastic minibatch up- dates experience replay memory ease training deep networks RL. Our approach gave state-of-the-art six seven games it tested on, no adjustment architecture or hyperparameters. References [1] Leemon Baird. Residual algorithms: Reinforcement function approximation. Proceedings 12th International Conference Machine Learning (ICML 1995) 3037. Morgan Kaufmann, 1995. [2] Sketch-based linear value function ap- proximation. Advances Neural Information Processing Systems 25 22222230, 2012. [3] Marc G Bellemare, Yavar Naddaf, Joel Veness, Michael Bowling. arcade environment: An evaluation platform general agents. Journal Articial Intelligence Research 47:253279, 2013. [4] Marc G Bellemare, Joel Veness, Michael Bowling. Investigating contingency awareness using atari 2600 games. AAAI 2012. [5] Marc G. Bellemare, Joel Veness, Michael Bowling. Bayesian recursively fac- tored environments. Proceedings Thirtieth International Conference Machine Learning (ICML 2013) pages 12111219, 2013. 8