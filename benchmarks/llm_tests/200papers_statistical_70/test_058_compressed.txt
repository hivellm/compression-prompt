More recently, there has revival interest combining deep learning reinforcement learning. Deep networks have used estimate environment E restricted Boltzmann machines have used estimate value function [21]; or policy [9]. In addition, divergence issues Q-learning have partially addressed gradient temporal-difference methods. These methods are proven converge when evaluating xed policy nonlinear function approximator [14]; or when learning control policy linear function approximation restricted variant Q-learning [15]. However, these methods have not yet extended nonlinear control. Perhaps most similar prior work our own approach tted Q-learning (NFQ) [20]. NFQ optimises sequence loss functions Equation 2, RPROP algorithm update parameters Q-network. However, it uses batch update that has computational cost per iteration that proportional size data set, whereas we consider stochastic gradient updates low constant cost per iteration scale large data-sets. NFQ also successfully applied simple real-world control tasks purely visual input, rst autoencoders learn low dimensional representation task, then applying NFQ this representation [12]. In contrast our approach applies end-to-end, directly visual inputs; result it may learn features are directly relevant discriminating action-values. Q-learning also previously combined experience replay simple [13], but again starting low-dimensional state rather than raw visual inputs. The use Atari 2600 emulator platform was introduced [3], who applied standard algorithms linear function approximation generic visual features. Subsequently, results were improved larger number features, tug-of-war hashing randomly project features into lower-dimensional space [2]. The HyperNEAT evolutionary architecture [8] also applied Atari platform, where was used evolve (separately, each distinct game) representing strategy game. When trained repeatedly against deterministic sequences emulator’s reset facility, these strategies were able exploit design aws several Atari games. 4 Deep Reinforcement Learning Recent breakthroughs computer vision speech recognition relied efciently training networks very large training sets. The most successful approaches are trained directly raw inputs, lightweight updates based stochastic gradient descent. By feeding sufcient data into networks, it often possible learn better representations than handcrafted features [11]. These successes motivate our approach learning. Our goal connect algorithm which operates directly RGB images efciently process training data stochastic gradient updates. Tesauro’s TD-Gammon architecture provides starting point such an approach. This architec- ture updates parameters estimates value function, directly on-policy samples experience, s r s +1 +1 , drawn algorithm’s interactions envi- ronment (or self-play, case backgammon). Since this approach was able outperform best human backgammon players 20 years ago, it natural wonder whether two decades hardware improvements, coupled modern architectures scalable RL algorithms might produce signicant progress. In contrast TD-Gammon similar online approaches, we utilize technique known expe- rience replay [13] where store agent’s experiences at each time-step, e = ( s r s +1 ) data-set D = e 1 :::; e N , pooled over many episodes into replay memory . During inner loop algorithm, we apply Q-learning updates, or minibatch updates, samples experience, e D , drawn at random pool stored samples. After performing experience replay, agent selects executes an action according an -greedy policy. Since histories arbitrary length as inputs network can be difcult, our Q-function instead works on xed length representation histories produced function . The full algorithm, which we call deep Q-learning , presented Algorithm 1. This approach has several advantages over standard online Q-learning [23]. First, each step experience potentially used many weight updates, which allows for greater data efciency. 4