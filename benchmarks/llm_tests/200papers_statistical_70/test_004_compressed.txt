in [ MacKay, 1992 , Krishnapuram et al., , Lawrence et al., 2003 ]) but are free choose from many available algorithms. Minimal additional approximations are introduced, so, our knowledge our algorithm represents most exact fastest way perform full information-theoretic active learning in non-parametric discriminative models. 3 Gaussian Processes Classication Pref- erence Learning In this section we derive BALD algorithm Gaussian Process classication (GPC). GPs powerful popular non-parametric tool regression classication. GPC appears be an especially challenging problem information-theoretic active learning because parameter space innite, however, by using (2) able calculate fully relevant information quantities without having work out entropies innite dimensional objects. probabilistic model underlying GPC as follows: GP( k )) y j Bernoulli(( ))) latent parameter, now called function X ! R assigned Gaussian process prior mean covariance function or kernel k ). We consider probit case where given value y takes Bernoulli distribution probability )), Gaussian CDF. For further details on GPs see [Rasmussen Williams, 2005]. Inference GPC model intractable; given some observations D posterior over becomes non-Gaussian complicated. most commonly used approximate inference methods { EP, Laplace approximation, Assumed Density Filtering sparse methods { all approximate posterior by [ Rasmussen Williams, 2005 ]. Throughout this section will assume that provided such approximation from one these methods, though active learning algorithm does not care which one. In our derivation will use 1 indicate where such an approximation exploited. informativeness query computed using Eqn. (2) . entropy binary output variable y given xed can be expressed terms binary entropy function h: H[ y j ] = h (( )) h( p = p log p (1 p log(1 p Expectations over posterior need be computed. Using Gaussian approxi- mation posterior, for each , = ) will follow Gaussian distribution with mean ; D variance 2 ; D . To compute Eqn. (2) we have compute 4