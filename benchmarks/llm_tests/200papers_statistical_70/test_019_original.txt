Figure 2. Unrolling recurrent neural networks in time by creating a copy of the model for each time step. We denote by x t the hidden state of the network at time t , by u t the input of the network at time t and by E t the error obtained from the output at time t . We will diverge from the classical BPTT equations at this point and re-write the gradients (see equations (3), (4) and (5)) in order to better highlight the exploding gradients problem. These equations were obtained by writing the gradients in a sum-of-products form. @ E