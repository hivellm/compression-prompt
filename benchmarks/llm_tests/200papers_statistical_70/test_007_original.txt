2 noise = 1. The likelihood only depends on the dierence between f ( u ) and f ( v ). We therefore dene g ( u ; v ) = f ( u ) f ( v ), and do inference entirely in terms of g , for which the likelihood becomes the same as for probit classication: y j u ; v ; f Bernoulli (( g ( u ; v ))). We observe that a GP prior is induced on g because it is formed by performing a linear operation on f , for which we have a GP prior already f GP (0 ; k ). We can derive the induced covariance function of g as (derivation in the Supplementary material) as: k pref (( u i ; v i ) ; ( u j ; v j )) = k ( u i ; u j ) + k ( v i ; v j ) k ( u i ; v j ) k ( v i ; u j ). Note that this kernel k pref respects the anti-symmetry properties desired for a preference learning scenario, i.e. the value g ( u; v ) is perfectly anti-correlated with g ( v; u ), ensuring P [ u v ] = 1 P [ v u ] holds. Thus, we can conclude that the GP preference learning framework of [ Chu and Ghahramani, 2005 ], is equivalent to GPC with a particular class of kernels, that we may call the preference judgement kernels . Therefore, our active learning algorithm presented in Section 3 for GPC can readily be applied to pairwise preference learning also. 4 Related Methodologies There are a number of closely related algorithms for active classication which we now review. The Informative Vector Machine (IVM): Perhaps the most closely re- lated approach is the IVM [ Lawrence et al., 2003 ]. This popular,and successful approach to active learning was designed specically for GPs; it uses an infor- mation theoretic approach and so appears very similar to BALD. The IVM algorithm was designed for subsampling a dataset for training a GP, so it is privy to the y values before including a measurement; it cannot therefore work explic- itly in output space i.e. with Eqn. (2) . The IVM uses Eqn. (1) , but parameter entropies are calculated approximately in the marginal subspace corresponding to the observed data points. The entropy decrease after inclusion of a new data point can then be calculated eciently using the GP covariance matrix. Although the IVM and BALD are motivated by the same objective, they work fundamentally dierently when approximate inference is carried out. At any time 7