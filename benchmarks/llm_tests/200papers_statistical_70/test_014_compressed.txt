incapable incorporating hyperparameter learning. This may mean given little data GP model overts, leading BALD selecting abnormal query locations. Maintaining distribution over hyperparameters can be done using MCMC, although this signicantly increases time. Designing general do eciently subject further work. practice, simple heuristic such picking rst few points randomly, optimising hyperparameters will usually suce. 6 Conclusions We have demonstrated applies full theoretic criterion GP classication makes, far authors are aware, smallest number approximations date, has good complexity. We extend GPC model develop new preference kernel, which enables us apply our algorithm directly domain also. can handle naturally kernel hyperparameters, which hard, mostly unsolved problem, example SVM learning. One notable feature our approach it agnostic approximate inference used. This allows us choose from whole range approximate inference methods, including EP, Laplace approximation, ADF or even sparse online learning, thereby make trade o between complexity accuracy. Our experimental performance compares favourably many other classication, decision theoretic have access test data require much greater time. References [Bernardo, 1979] Bernardo, J. (1979). Expected expected utility. Annals Statistics 7(3):686{690. [Chu Ghahramani, Chu, W. Ghahramani, Z. (2005). Preference Gaussian processes. ICML pages 137{144. ACM. [Cover et 1991] Cover, T., Thomas, J., Wiley, J. (1991). Elements theory volume 6. Wiley Online Library. [Dasgupta, 2005] Dasgupta, S. (2005). Analysis greedy strategy. NIPS . [Ertin et al., ] Ertin, E., Fisher, J., Potter, L. Maximum mutual principle dynamic sensor query problems. Information Processing Sensor Networks Lecture Notes Computer Science. [Fuhrmann, 2003] Fuhrmann, D. (2003). Active Testing Surveillance Systems, or, Playing Twenty Questions with Radar . Defense Technical Information Center. 14