cuda-convnet toolbox 1 , although our net less wide, and we used additional jittering, based on zeroing-out random parts an image. Our weight layer conguration is: conv64-conv256- conv256-conv256-conv256-full4096-full4096-full1000, where convN denotes convolutional layer N lters, fullM fully-connected layer with M outputs. On ILSVRC-2013 validation set, network achieves top-1/top-5 classication error 39 : 7% = 17 : 7% , which slightly better than 40 7% / 18 2% reported [ 8 ] single ConvNet. 2 Class Model Visualisation In this section we describe technique visualising models, learnt clas- sication ConvNets. Given learnt classication ConvNet and interest, visualisation method consists numerically generating [ 5 ], which representative terms ConvNet scoring model. More formally, let S ( I ) be score computed classication ConvNet I . We would like nd L 2 -regularised image, such that score S high: arg max I S ( I ) k I k ; (1) where regularisation parameter. A locally-optimal I can be found back-propagation method. The procedure related ConvNet training procedure, where back-propagation used optimise weights. The difference that our case optimisation performed respect input image, while weights are xed those found during training stage. We initialised optimisation zero (in our case, ConvNet was trained on zero-centred data), and then added training set mean result. The model visualisations for several classes are shown Fig. 1 . It should be noted that we used (unnormalised) scores S , rather than posteriors, returned by soft-max layer: P = exp S