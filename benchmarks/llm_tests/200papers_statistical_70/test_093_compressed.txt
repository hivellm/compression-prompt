35.63 Table 1: BLEU scores trained models com- puted on test set. The second third columns show respectively scores on all sentences and, on sentences without any unknown word them- selves reference translations. Note RNNsearch-50 ? was trained much longer until performance development set stopped improv- ing. ( ) We disallowed models generate [UNK] tokens when only sentences having no unknown words were evaluated (last column). 5.2 Q UALITATIVE A NALYSIS 5.2.1 A LIGNMENT The proposed approach provides an intuitive way inspect (soft-)alignment between words generated translation those sentence. This done by visualizing annotation weights ij Eq. (6), as Fig. 3. Each row matrix each plot indicates weights associated with annotations. From which positions sentence were considered more important when generating target word. We can alignments Fig. 3 alignment between English French largely monotonic. We strong weights along diagonal each matrix. However, also observe number non-trivial, non-monotonic alignments. Adjectives nouns are typically ordered differently between French English, an example Fig. 3 (a). From gure, correctly translates phrase [European Economic Area] [zone · economique europ · een]. RNNsearch able correctly align [zone] with [Area], jumping over two ([European] [Economic]), then looked one word back at time complete whole phrase [zone · economique europ eenne]. strength soft-alignment, opposed hard-alignment, evident, for instance, Fig. 3 (d). Consider phrase [the man] which translated [l’ homme]. Any hard alignment will map [the] [l’] [man] [homme]. This not helpful for translation, as one must consider word following [the] determine whether it should be translated [le], [la], [les] or [l’]. Our soft-alignment solves issue naturally by letting look at both [man], example, able correctly translate [the] [l’]. We observe similar behaviors all presented cases Fig. 3. An additional benet soft align- ment it naturally deals with target phrases different lengths, without requiring counter-intuitive way mapping some or nowhere ([NULL]) (see, e.g., Chapters 4 5 Koehn, 2010). 5.2.2 L ONG S ENTENCES As clearly visible Fig. 2 proposed model (RNNsearch) much better than conventional model (RNNencdec) at translating long sentences. This likely due fact RNNsearch does not require encoding long sentence into xed-length vector perfectly, but only accurately encoding parts input sentence surround particular word. As an example, consider this source sentence test set: An admitting privilege right doctor admit patient hospital or medical centre