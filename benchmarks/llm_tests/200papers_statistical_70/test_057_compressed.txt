maximising expected value of r + Q s ; , Q s; = E s E h r + max Q s ; s; (1) The basic idea behind many reinforcement learning algorithms estimate action- value function, by using Bellman equation as an iterative update, Q +1 s; = E [ r + max Q s ; j s; ] . Such value iteration algorithms converge optimal action- value function, Q ! Q as ! 1 [23]. In practice, this basic approach totally impractical, because action-value function estimated separately for each sequence, without any generali- sation. Instead, it common use function approximator estimate action-value function, Q Q . In reinforcement learning community this typically linear approximator, but sometimes non-linear approximator used instead, such as neural network. We refer neural network approximator weights as Q-network. A Q-network can be trained by minimising sequence of loss functions L that changes at each iteration , L = E s;a h y Q )) 2 (2) where y = E s E [ r + max Q s 1 j ] target for iteration probability distribution over sequences s actions that we refer as behaviour distribution . The parameters from previous iteration 1 are held xed when optimising loss L . Note that targets depend on network weights; this in contrast targets used for supervised learning, which are xed before learning begins. Differentiating loss respect weights we arrive at following gradient, r L = E s;a ); s E h r + max Q s 1 Q r Q : (3) Rather than computing full expectations in above gradient, it often computationally expe- dient optimise loss by stochastic gradient descent. If weights are updated after every time-step, expectations are replaced by single samples from behaviour distribution emulator E respectively, then we arrive at familiar Q-learning algorithm [26]. Note that this algorithm model-free : it solves reinforcement learning task directly using sam- ples from emulator E , without explicitly constructing an estimate of E . It also off-policy : it learns about greedy strategy = max Q , while following behaviour distribution that ensures adequate exploration of state space. In practice, behaviour distribution often se- lected by an -greedy strategy that follows greedy strategy probability 1 selects random action probability . 3 Related Work Perhaps best-known success story reinforcement learning TD-gammon , backgammon- playing program which learnt entirely by reinforcement learning and self-play, and achieved super- human level of play [24]. TD-gammon used model-free reinforcement learning algorithm similar Q-learning, and approximated value function using multi-layer perceptron with one hidden layer 1 . However, early attempts follow up on TD-gammon, including applications of same method chess, Go and checkers were less successful. This led widespread belief that TD-gammon approach was special case that only worked in backgammon, perhaps because stochasticity in dice rolls helps explore state space and also makes value function particularly smooth [19]. Furthermore, it was shown that combining model-free reinforcement learning algorithms such as Q- learning with non-linear function approximators [25], or indeed with off-policy learning [1] could cause Q-network diverge. Subsequently, majority of work in reinforcement learning fo- cused on linear function approximators with better convergence guarantees [25].