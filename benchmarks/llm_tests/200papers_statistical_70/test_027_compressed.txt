Figure 6. We plot error surface single hidden unit recurrent network, highlighting existence high cur- vature walls. The solid lines depicts standard trajectories gradient descent might follow. Using dashed arrow diagram shows what would happen if gradients rescaled xed size when its norm above threshold. explode so does curvature along v , leading wall error surface , like one seen Fig. 6. If this holds, then it gives us simple solution exploding gradients problem depicted Fig. 6. If both leading eigenvector curvature are aligned exploding direction v , it follows error surface has steep wall perpen- dicular v (and consequently gradient). This means when stochastic descent (SGD) reaches wall does descent step, it will be forced jump across valley moving perpen- dicular steep walls, possibly leaving valley disrupting learning process. dashed arrows Fig. 6 correspond ignoring norm this large step , ensuring stays close wall. key insight all steps taken when explodes are aligned v ignore other descent direction (i.e. moves perpendicular wall). At wall, small-norm step direction there- fore merely pushes us back inside smoother low- curvature region besides wall, whereas regular step would bring us very far, thus slowing or preventing further training. Instead, bounded step, we get back smooth region near wall where SGD free explore other descent directions. important addition this scenario classical high curvature valley, assume val- ley wide, we have large region around wall where if land rely on rst order methods move towards local minima. This why just clipping might be sucient, requiring use second order method. Note this algo- rithm should work even when rate growth same one curvature (a case for which second order method would fail ratio between curvature could still explode). Our hypothesis could also help understand re- cent success Hessian-Free approach compared other second order methods. There are two key dif- ferences between Hessian-Free most other second- order algorithms. First, it uses full Hessian matrix hence deal exploding directions are necessarily axis-aligned. Second, computes new estimate Hessian matrix before each up- date step take into account abrupt changes curvature (such ones suggested by our hypothe- sis) while most other approaches use smoothness as- sumption, i.e., averaging 2nd order signals over many steps. 3. Dealing exploding vanishing 3.1. Previous solutions Using an L1 or L2 penalty on recurrent weights help exploding gradients. Given parame- ters initialized small values, spectral radius W rec probably smaller than 1, from which follows explode (see necessary condi- tion found section 2.1). regularization term ensure during training spectral radius never exceeds 1. This approach limits sim- ple regime (with single point attractor at origin), where any information inserted has die out exponentially fast time. In such regime train generator network, nor we exhibit long term memory traces. Doya (1993) proposes pre-program (to initialize right regime) or use teacher forcing . The rst proposal assumes if exhibits from beginning same kind asymptotic behaviour one required by target, then there no need cross bifurcation boundary. The downside one not always know required asymptotic behaviour, and, even if such information known, it not trivial initial- ize model this specic regime. We should also note such initialization does not prevent cross- ing boundary between basins attraction, which, as shown, could happen even though no bifurcation boundary crossed. Teacher forcing more interesting, yet not very well understood solution. It can be seen as way initializing model right regime right