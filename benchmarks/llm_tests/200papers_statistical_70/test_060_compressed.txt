Second, learning directly consecutive samples inefcient, due strong correlations between samples; randomizing samples breaks these correlations therefore reduces variance updates. Third, when learning on-policy current parameters determine next data sample that parameters trained on. For example, if maximizing action move left then training samples will be dominated samples left-hand side; if maximizing action then switches right then training distribution will also switch. It easy see how unwanted feedback loops may arise parameters could get stuck poor local minimum, or even diverge catastrophically [25]. By using experience replay behavior distribution averaged over many its previous states, smoothing out learning avoiding oscillations or divergence parameters. Note when learning experience replay, it necessary learn off-policy (because our current parameters different those used generate sample), motivates choice Q-learning. In practice, our algorithm only stores last N experience tuples replay memory, uniformly at random D when performing updates. This approach some respects limited since memory buffer does not differentiate important transitions always overwrites recent transitions due nite memory size N . Similarly, uniform sampling gives equal importance all transitions replay memory. A more sophisticated sampling strategy might emphasize transitions we can learn most, similar prioritized sweeping [17]. 4.1 Preprocessing Model Architecture Working directly raw Atari frames, 210 160 pixel images 128 color palette, can be computationally demanding, so we apply basic preprocessing step aimed at reducing dimensionality. raw frames preprocessed rst converting their RGB representation gray-scale down-sampling it 110 84 image. nal representation obtained cropping 84 84 region image roughly captures playing area. nal cropping stage only required because we use GPU implementation 2D convolutions [11], expects square inputs. For experiments paper, function algorithm 1 applies this preprocessing last 4 frames history stacks them produce Q -function. There several possible ways parameterizing Q using neural network. Since Q maps history- pairs scalar estimates their Q-value, history have been used as inputs neural network some previous approaches [20, 12]. main drawback this type architecture that separate forward pass required compute Q-value each action, resulting cost that scales linearly number actions. We instead use an architecture which there separate output unit for each possible action, only state representation an input neural network. outputs correspond predicted Q-values individual action for input state. The main advantage this type architecture ability compute Q-values for all possible actions given state only single forward pass through network. 5