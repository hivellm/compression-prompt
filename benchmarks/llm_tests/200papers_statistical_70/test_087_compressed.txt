The decoder is often trained to predict next word 0 given context vector c and all previously predicted words f 0 g . In other words, decoder denes probability over translation by decomposing joint probability into ordered conditionals: p ( ) = T Y =1 p ( j f c ) (2) where = T . With an RNN, each conditional probability is modeled as p ( j f g c ) = g ( s c ) (3) where g is nonlinear, potentially multi-layered, function that outputs probability of , s is hidden state of RNN. It should be noted that other architectures such as hybrid an RNN de-convolutional neural network can be used (Kalchbrenner Blunsom, 2013). 3 L EARNING TO A LIGN AND T RANSLATE In this section, we propose novel architecture for neural machine translation. The new architecture consists of bidirectional RNN as an encoder (Sec. 3.2) and decoder that emulates searching through source sentence during decoding translation (Sec. 3.1). 3.1 D ECODER : G ENERAL D ESCRIPTION