Figure 2. Unrolling recurrent neural networks in time creating a copy of model for each time step. We denote x hidden state of network , u input network E error obtained from output . We will diverge from classical BPTT equations this point and re-write gradients (see equations (3), (4) and (5)) in order to better highlight exploding gradients problem. These equations were obtained writing gradients in a sum-of-products form. @ E