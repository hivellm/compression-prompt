3. One can approximately model all conditionals p ( x S j x 6 S ) where S subset indices x training family conditional models that share parameters. Essentially, one can use adversarial nets implement stochastic extension deterministic MP-DBM [11]. 4. Semi-supervised learning : features discriminator or inference net could improve perfor- mance classiers when limited labeled data available. 5. Efciency improvements: training could be accelerated greatly divising better methods coordinating G D or determining better distributions sample z during training. This paper has demonstrated viability adversarial modeling framework, suggesting that these research directions could prove useful. Acknowledgments We acknowledge Patrice Marcotte, Olivier Delalleau, Kyunghyun Cho, Guillaume Alain Jason Yosinski helpful discussions. Yann Dauphin shared his Parzen window eval- uation code us. We developers Pylearn2 [12] Theano [7, 1], particularly Fr ed eric Bastien who rushed Theano feature specically benet this project. Ar- naud Bergeron provided much-needed support L A T E X typesetting. We also CIFAR, Canada Research Chairs funding, Compute Canada, Calcul Qu ebec providing computational resources. Ian Goodfellow supported 2013 Google Fellowship Learning. Finally, we Les Trois Brasseurs stimulating our creativity. References [1] Bastien, F., Lamblin, Pascanu, R., Bergstra, Bergeron, Bouchard, N., (2012). Theano: new features speed improvements. Unsupervised Feature NIPS 2012 Workshop. [2] (2009). architectures AI Now Publishers. [3] Mesnil, Dauphin, Rifai, S. (2013a). Better mixing via representations. ICML’13 [4] Yao, L., Alain, Vincent, P. (2013b). Generalized denoising auto-encoders as models. NIPS26 Nips Foundation. [5] Thibodeau-Laufer, Yosinski, J. (2014a). networks trainable backprop. ICML’14 [6] Thibodeau-Laufer, Alain, Yosinski, J. (2014b). net- works trainable backprop. Proceedings 30th International Conference on Machine (ICML’14) [7] Bergstra, Breuleux, O., Bastien, F., Lamblin, Pascanu, R., Desjardins, Turian, Warde-Farley, D., (2010). Theano: CPU GPU math expression compiler. Proceedings Python Scientic Computing Conference (SciPy) Oral Presentation. [8] Breuleux, O., Vincent, P. (2011). Quickly generating representative samples an RBM-derived process. Neural Computation 23 (8), 20532073. [9] Glorot, X., Bordes, (2011). sparse rectier AISTATS’2011 [10] Warde-Farley, D., Mirza, M., Courville, (2013a). Maxout ICML’2013 [11] Mirza, M., Courville, (2013b). Multi-prediction Boltzmann machines. NIPS’2013 [12] Goodfellow, I. Warde-Farley, D., Lamblin, P., Dumoulin, V., Mirza, M., Pascanu, R., Bergstra, Bastien, F., (2013c). Pylearn2: machine learning research library. arXiv preprint arXiv:1308.4214 [13] Gutmann, M. Hyvarinen, A. (2010). Noise-contrastive estimation: A new estimation principle unnormalized statistical models. AISTATS’2010 [14] Hinton, G., Deng, L., Dahl, G. E., Mohamed, A., Jaitly, N., Senior, A., Vanhoucke, V., Nguyen, P., Sainath, T., Kingsbury, B. (2012a). Deep neural networks acoustic modeling in speech recognition. IEEE Signal Processing Magazine 29 (6), 8297. [15] Hinton, G. E., Dayan, P., Frey, B. Neal, R. M. (1995). The wake-sleep algorithm unsupervised neural networks. Science 268 15581161. 8