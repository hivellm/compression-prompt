affect class score most. One can expect that such pixels correspond object location in image. We note that similar technique has been previously applied by [ 1 ] in context Bayesian classication. 3.1 Class Saliency Extraction Given an image I 0 (with m rows n columns) class c , class map M 2 R m n computed as follows. First, derivative w ( 4 ) found by back-propagation. After that, map obtained by rearranging elements vector w . In case grey-scale image, number elements w equal number pixels I 0 , so map can be computed as M ij = j w h ( i;j ) j , where h ( i; j ) index element w , corresponding image pixel i -th row -th column. In case multi-channel e.g. RGB) image, let us assume that colour channel c pixel i; image I corresponds element with index h i; j; c . To derive single value for each pixel i; , took maximum magnitude across all colour channels: M ij = max c h i;j;c . It important note that maps are extracted using classication ConvNet trained image labels, so no additional annotation required (such as bounding boxes or segmentation masks). The computation image-specic map for single extremely quick, since it only requires single back-propagation pass. We visualise maps highest-scoring (top-1 prediction) randomly se- lected ILSVRC-2013 test set images Fig. 2 . Similarly ConvNet classication procedure 8 ], where predictions are computed 10 cropped reected sub-images, computed 10 maps 10 sub-images, then averaged them. 3.2 Weakly Supervised Object Localisation The weakly supervised maps (Sect. 3.1 encode location given given image, thus can used for localisation (in spite being trained image labels only). Here briey describe simple localisation procedure, which used localisation task ILSVRC-2013 challenge 12 ]. Given an image corresponding map, compute segmentation mask using GraphCut colour segmentation 3 ]. The use colour segmentation motivated by fact that map might capture only most discriminative part an object, so thresholding might not able highlight whole object. Therefore, it important able propagate thresholded map other parts object, which aim achieve here using colour continuity cues. Foreground background colour models were set Gaussian Mixture Models. The foreground model was estimated from pixels with higher than threshold, set 95% quantile distribution image; background model was estimated from pixels with smaller than 30% quantile (Fig. 3 , right-middle). The GraphCut segmentation 3 ] was then performed using publicly available implementation 2 . Once image pixel labelling into foreground background computed, segmentation mask set largest connected component foreground pixels (Fig. 3 , right). We entered our localisation method into ILSVRC-2013 localisation challenge. Consid- ering that challenge requires bounding boxes reported, computed them as bounding boxes segmentation masks. The procedure was repeated for each top-5 predicted classes. The method achieved 46 : 4% top-5 error on test set ILSVRC-2013. It should be noted that method weakly supervised (unlike challenge winner with 29 : 9% error), localisation task was not taken into account during training. In spite its simplicity, method still outperformed our submission ILSVRC-2012 challenge (which used same dataset), which achieved 50 : 0% localisation error using fully-supervised algorithm based on part-based models [ 6 ] Fisher vector feature encoding [ 11 ]. 4 Relation Deconvolutional Networks In this section we establish connection between gradient-based visualisation DeconvNet architecture [ 13 ]. As we show below, DeconvNet-based reconstruction n -th layer input X n either equivalent or similar computing gradient visualised neuron ac-