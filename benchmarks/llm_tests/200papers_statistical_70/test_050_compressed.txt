(such as 50 - 100). Given Equation 4, increasing amount training data twice results about same increase computational complexity as increasing vector size twice. For experiments reported Tables 2 4, we used three epochs with stochastic gradi- ent descent backpropagation. We chose starting learning rate 0.025 decreased it linearly, so that it approaches zero at end last epoch. 4.3 Comparison Model Architectures First we compare different model architectures for deriving vectors dimensionality 640 vectors. In further experiments, we use full set questions new Semantic-Syntactic Word Relationship test set, i.e. unrestricted 30k vocabulary. We also include results test set introduced [20] that focuses syntactic similarity between words 3 . data consists several LDC corpora described detail [18] (320M words, 82K vocabulary). We used these data provide comparison previously trained recurrent neural network language model that took about 8 weeks train single CPU. We trained feed- forward NNLM with number 640 hidden units DistBelief parallel [6], history 8 previous words (thus, NNLM has more parameters RNNLM, projection layer has size 640 8 ). In Table 3, it can be seen vectors from RNN (as used [20]) perform well mostly syntactic questions. NNLM vectors perform signicantly RNN - this not surprising, vectors RNNLM are directly connected non-linear hidden layer. CBOW architecture works NNLM syntactic tasks, about same semantic one. Finally, Skip-gram architecture works slightly worse syntactic task CBOW model (but still better NNLM), much better semantic part test than all other models. Next, we evaluated our models trained using one CPU only compared results against publicly available word vectors. The comparison is given Table 4. The CBOW model was trained subset