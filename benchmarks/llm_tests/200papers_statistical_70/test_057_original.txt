maximising the expected value of r + Q  ( s 0 ; a 0 ) , Q  ( s; a ) = E s 0 E h r +  max a 0 Q  ( s 0 ; a 0 )    s; a i (1) The basic idea behind many reinforcement learning algorithms is to estimate the action- value function, by using the Bellman equation as an iterative update, Q i +1 ( s; a ) = E [ r +  max a 0 Q i ( s 0 ; a 0 ) j s; a ] . Such value iteration algorithms converge to the optimal action- value function, Q i ! Q  as i ! 1 [23]. In practice, this basic approach is totally impractical, because the action-value function is estimated separately for each sequence, without any generali- sation. Instead, it is common to use a function approximator to estimate the action-value function, Q ( s; a ;  )  Q  ( s; a ) . In the reinforcement learning community this is typically a linear function approximator, but sometimes a non-linear function approximator is used instead, such as a neural network. We refer to a neural network function approximator with weights  as a Q-network. A Q-network can be trained by minimising a sequence of loss functions L i (  i ) that changes at each iteration i , L i (  i ) = E s;a   (  ) h ( y i  Q ( s; a ;  i )) 2 i ; (2) where y i = E s 0 E [ r +  max a 0 Q ( s 0 ; a 0 ;  i  1 ) j s; a ] is the target for iteration i and  ( s; a ) is a probability distribution over sequences s and actions a that we refer to as the behaviour distribution . The parameters from the previous iteration  i  1 are held xed when optimising the loss function L i (  i ) . Note that the targets depend on the network weights; this is in contrast with the targets used for supervised learning, which are xed before learning begins. Differentiating the loss function with respect to the weights we arrive at the following gradient, r  i L i (  i ) = E s;a   (  ); s 0 E h r +  max a 0 Q ( s 0 ; a 0 ;  i  1 )  Q ( s; a ;  i )  r  i Q ( s; a ;  i ) i : (3) Rather than computing the full expectations in the above gradient, it is often computationally expe- dient to optimise the loss function by stochastic gradient descent. If the weights are updated after every time-step, and the expectations are replaced by single samples from the behaviour distribution  and the emulator E respectively, then we arrive at the familiar Q-learning algorithm [26]. Note that this algorithm is model-free : it solves the reinforcement learning task directly using sam- ples from the emulator E , without explicitly constructing an estimate of E . It is also off-policy : it learns about the greedy strategy a = max a Q ( s; a ;  ) , while following a behaviour distribution that ensures adequate exploration of the state space. In practice, the behaviour distribution is often se- lected by an  -greedy strategy that follows the greedy strategy with probability 1   and selects a random action with probability  . 3 Related Work Perhaps the best-known success story of reinforcement learning is TD-gammon , a backgammon- playing program which learnt entirely by reinforcement learning and self-play, and achieved a super- human level of play [24]. TD-gammon used a model-free reinforcement learning algorithm similar to Q-learning, and approximated the value function using a multi-layer perceptron with one hidden layer 1 . However, early attempts to follow up on TD-gammon, including applications of the same method to chess, Go and checkers were less successful. This led to a widespread belief that the TD-gammon approach was a special case that only worked in backgammon, perhaps because the stochasticity in the dice rolls helps explore the state space and also makes the value function particularly smooth [19]. Furthermore, it was shown that combining model-free reinforcement learning algorithms such as Q- learning with non-linear function approximators [25], or indeed with off-policy learning [1] could cause the Q-network to diverge. Subsequently, the majority of work in reinforcement learning fo- cused on linear function approximators with better convergence guarantees [25].