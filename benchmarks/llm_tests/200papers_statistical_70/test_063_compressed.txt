Figure 3: leftmost plot shows predicted function 30 frame segment game Seaquest. three screenshots correspond frames labeled A, B, C respectively. 5.2 Visualizing Value Function Figure 3 shows visualization learned function game Seaquest. gure shows predicted jumps after an enemy appears left screen (point A). agent then res torpedo at enemy predicted peaks torpedo is about hit enemy (point B). Finally, falls roughly its original after enemy disappears (point C). Figure 3 demonstrates method is able learn how function evolves reasonably complex sequence events. 5.3 Main Evaluation We compare results with best performing methods from RL literature [3, 4]. method labeled Sarsa used Sarsa algorithm learn linear policies several different feature sets hand- engineered Atari task we score best performing feature set [3]. Con- tingency used same basic approach Sarsa but augmented feature sets with learned representation parts screen are under agentâ€™s control [4]. Note both these methods incorporate signicant prior knowledge about visual problem using background sub- traction treating 128 colors separate channel. Since many Atari games use one distinct color type object, treating color separate channel can be similar producing separate binary map encoding presence object type. In contrast, agents only receive raw RGB screenshots input must learn detect objects their own. In addition learned agents, we also scores an expert human game player policy selects actions uniformly at random. human performance is median reward achieved after around two hours playing game. Note reported human scores are much higher than ones in Bellemare et al. [3]. For learned methods, we follow evaluation strategy used in Bellemare et al. [3, 5] average score obtained running an -greedy policy with = 0 : 05 xed number steps. rst ve rows table 1 show per-game average scores all games. Our approach (labeled DQN) outperforms other learning methods substantial margin all seven games despite incorporating almost no prior knowledge about inputs. We also include comparison evolutionary policy search approach from [8] in last three rows table 1. We report two sets results this method. HNeat Best score reects results obtained using hand-engineered object detector algorithm outputs locations 7