In practice, adversarial nets represent a limited family of p g distributions via the function G ( z ;  g ) , and we optimize  g rather than p g itself. Using a multilayer perceptron to dene G introduces multiple critical points in parameter space. However, the excellent performance of multilayer per- ceptrons in practice suggests that they are a reasonable model to use despite their lack of theoretical guarantees. 5 Experiments We trained adversarial nets an a range of datasets including MNIST[23], the Toronto Face Database (TFD) [28], and CIFAR-10 [21]. The generator nets used a mixture of rectier linear activations [19, 9] and sigmoid activations, while the discriminator net used maxout [10] activations. Dropout [17] was applied in training the discriminator net. While our theoretical framework permits the use of dropout and other noise at intermediate layers of the generator, we used noise as the input to only the bottommost layer of the generator network. We estimate probability of the test set data under p g by tting a Gaussian Parzen window to the samples generated with G and reporting the log-likelihood under this distribution. The  parameter 5