Figure 1. Schematic of a recurrent neural network. The recurrent connections hidden layer allow information persist from one input another. exploding gradient problems described Bengio et al. (1994). 1.1. Training networks A generic neural network, with input u state time step given equation (1). In theoretical section this paper we will sometimes make use specic parametrization given equa- tion (11) 1 order provide more precise conditions intuitions about everyday use-case. F ( ; u ; ) (1) W rec ( ) + W u + b (2) The parameters model are weight matrix W rec , biases b input weight matrix W collected for general case. 0 provided user, set zero or learned, an element-wise function (usually tanh or sigmoid ). A cost E measures performance network on some given task it can be broken apart into individual costs for each step E = P 1 T E , where E = L ( ). One approach that can be used compute nec- essary gradients Backpropagation Through Time (BPTT), where recurrent model represented as