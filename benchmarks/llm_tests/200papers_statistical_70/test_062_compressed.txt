## 5.1 Training Stability In supervised learning, one can easily track performance model during training evaluating it on training validation sets. In reinforcement learning, however, accurately evaluating progress an agent during can be challenging. Since our evaluation metric, as suggested [3], total agent collects an episode or game averaged over number games, we periodically compute it training. The metric tends be very noisy because small changes weights lead large changes distribution states visits . The leftmost two plots gure 2 show how evolves on games Seaquest Breakout. Both averaged plots are indeed quite noisy, giving one impression that learning algorithm not making steady progress. Another, more stable, metric policyâ€™s estimated action-value function Q , which provides estimate how much discounted obtain following its from given state. We collect xed set states running random before starts track maximum 2 predicted Q for these states. The two rightmost plots gure 2 show that predicted Q increases much more smoothly than obtained plotting same metrics other ve games produces similarly smooth curves. In addition seeing relatively smooth improvement predicted Q during we did not experience any divergence issues any our experiments. This suggests that, despite lacking any theoretical convergence guarantees, our method is able train large neural networks using reinforcement learning signal stochastic gradient descent stable manner.