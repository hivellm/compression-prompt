N EURAL M ACHINE T RANSLATION BY J OINTLY L EARNING TO A LIGN AND T RANSLATE Dzmitry Bahdanau Jacobs University Bremen, Germany KyungHyun Cho Yoshua Bengio Universit · e de Montr · eal A BSTRACT Neural machine translation is recently proposed approach machine transla- tion. Unlike traditional statistical machine translation, aims at building single network can be jointly tuned maximize performance. The models proposed recently for neu- ral machine often belong family encoderdecoders encode source sentence into xed-length vector from which decoder generates translation. In this paper, we conjecture use xed-length vector bottleneck in improving performance this basic encoderdecoder architec- ture, propose extend this by allowing model automatically (soft-)search parts sentence are relevant predicting target word, without having form these parts as hard segment explicitly. With this new approach, we achieve performance comparable existing state-of-the-art phrase-based system on task English-to-French translation. Furthermore, qualitative analysis reveals (soft-)alignments found by model agree well our intuition. 1 I NTRODUCTION Neural newly emerging approach translation, recently proposed by Kalchbrenner Blunsom (2013), Sutskever (2014) Cho (2014b). Unlike traditional phrase-based system (see, e.g., Koehn et al. , 2003) which consists many small sub-components are tuned separately, attempts build train single, large network reads outputs correct translation. Most proposed models belong family encoder decoders (Sutskever et , 2014; Cho , 2014a), an encoder decoder each lan- guage, or involve language-specic encoder applied each sentence whose outputs are then com- pared (Hermann Blunsom, 2014). An encoder network reads encodes sen- tence into xed-length vector. A decoder then outputs from encoded vector. The whole encoderdecoder system, which consists encoder decoder language pair, jointly trained maximize probability correct given sentence. A potential issue this encoderdecoder approach network needs be able compress all necessary information into xed-length vector. This may make it difcult network cope long sentences, especially those are longer than sentences in training corpus. Cho et al. (2014b) showed indeed performance basic encoderdecoder deteriorates rapidly as length an input sentence increases. In order address this issue, we introduce an extension encoderdecoder model which learns align translate jointly. Each time proposed model generates word in translation, it (soft-)searches for set positions in source sentence where most relevant information is concentrated. The model then predicts target word based on context vectors associated with these source positions all previous generated target words.