Figure 2: The BLEU scores generated translations on test set with respect lengths sen- tences. The results are on full test set which in- cludes sentences having un- known words models. 2012 news-test-2013 make development (validation) set, evaluate models on test set (news-test-2014) from WMT â€™14, which consists 3003 sentences not present training data. After usual tokenization 6 , use shortlist 30,000 most frequent words language train our models. Any word not included shortlist mapped special token ( [ UNK ] ). We do not apply any other special preprocessing, such lowercasing or stemming, data. 4.2 M ODELS We two types models. rst one an RNN EncoderDecoder (RNNencdec, Cho 2014a), other proposed model, which refer RNNsearch. We model twice: rst sentences length up 30 words (RNNencdec-30, RNNsearch-30) then length up 50 word (RNNencdec-50, RNNsearch-50). encoder decoder RNNencdec have 1000 hidden units each. 7 encoder RNNsearch consists forward backward recurrent neural networks (RNN) having 1000 hidden units. Its decoder has 1000 hidden units. In both cases, use multilayer network single maxout (Goodfellow 2013) hidden layer compute conditional probability target word (Pascanu 2014). We use minibatch stochastic gradient descent (SGD) algorithm together Adadelta (Zeiler, 2012) model. Each SGD update direction computed using minibatch 80 sen- tences. We trained model for approximately 5 days. Once model trained, use beam search nd translation that approximately maximizes conditional probability (see, e.g., Graves, 2012; Boulanger-Lewandowski 2013). Sutskever (2014) used this approach generate translations from their neural machine translation model. For more details architectures models training procedure used experiments, see Appendices A B. 5 R ESULTS 5.1 Q UANTITATIVE R ESULTS In Table 1, list translation performances measured BLEU score. It clear from table that all cases, proposed RNNsearch outperforms conventional RNNencdec. More importantly, performance RNNsearch as high as that conventional phrase-based translation system (Moses), when only sentences consisting known words are considered. This signicant achievement, considering that Moses uses separate monolingual corpus (418M words) addition parallel corpora we used train RNNsearch RNNencdec.