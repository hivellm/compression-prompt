Figure 1: The graphical illus- tration of the proposed model trying to generate the t -th tar- get word y t given a source sentence ( x 1 ; x 2 ; : : : ; x T ) . In a new model architecture, we dene each conditional probability in Eq. (2) as: p ( y i j y 1 ; : : : ; y i  1 ; x ) = g ( y i  1 ; s i ; c i ) ; (4) where s i is an RNN hidden state for time i , computed by s i = f ( s i  1 ; y i  1 ; c i ) : It should be noted that unlike the existing encoderdecoder ap- proach (see Eq. (2)), here the probability is conditioned on a distinct context vector c i for each target word y i . The context vector c i depends on a sequence of annotations ( h 1 ;    ; h T x ) to which an encoder maps the input sentence. Each annotation h i contains information about the whole input sequence with a strong focus on the parts surrounding the i -th word of the input sequence. We explain in detail how the annotations are com- puted in the next section. The context vector c i is, then, computed as a weighted sum of these annotations h i : c i = T x X j =1  ij h j : (5) The weight  ij of each annotation h j is computed by  ij = exp ( e ij )