2 noise = 1. The likelihood only depends on dierence between f ) and f ). We therefore dene g ) = f ) f ), and do inference entirely in terms of g , which likelihood becomes same as probit classication: y j f Bernoulli (( g ))). We observe that GP prior induced on g because it formed by performing linear operation on , which we have GP prior already GP (0 ). We can derive induced covariance function as (derivation Supplementary material) as: pref (( i )) = + ). Note that this kernel pref respects anti-symmetry properties desired preference learning scenario, i.e. value u; perfectly anti-correlated with v; ), ensuring P [ ] = 1 P [ ] holds. Thus, we can conclude that GP preference learning framework [ Chu Ghahramani, 2005 ], equivalent GPC with particular class kernels, that we may call preference judgement kernels . Therefore, our active learning algorithm presented Section 3 GPC can readily be applied pairwise preference learning also. 4 Related Methodologies There are number closely related algorithms active classication which we now review. The Informative Vector Machine (IVM): Perhaps most closely re- lated approach IVM [ Lawrence et al., 2003 ]. This popular,and successful approach active learning was designed specically GPs; it uses an infor- mation theoretic approach so appears very similar BALD. The IVM algorithm was designed subsampling dataset training GP, so it privy to y values before including measurement; it cannot therefore work explic- itly in output space i.e. with Eqn. (2) . The IVM uses Eqn. (1) , but parameter entropies are calculated approximately in marginal subspace corresponding observed data points. The entropy decrease after inclusion of new data point can then be calculated eciently using GP covariance matrix. Although IVM and BALD are motivated by same objective, they work fundamentally dierently when approximate inference is carried out. At any time 7