The most important distinguishing feature this approach from basic encoderdecoder is that it does not attempt encode whole input sentence into single xed-length vector. Instead, it en- codes input sentence into sequence vectors chooses subset these vectors adaptively while decoding translation. This frees neural model from having squash all information source sentence, regardless its length, into xed-length vector. We show this allows model cope better with long sentences. In this paper, we show that proposed approach jointly learning align translate achieves signicantly improved performance over basic encoderdecoder approach. The im- provement more apparent with longer sentences, but can be observed with sentences any length. On task English-to-French translation, proposed approach achieves, single model, performance comparable, or close, conventional phrase-based system. Furthermore, qualitative analysis reveals proposed model nds linguistically plausible (soft-)alignment between source corresponding target sentence. 2 B ACKGROUND : N EURAL M ACHINE T RANSLATION From probabilistic perspective, equivalent nding target y max- imizes conditional probability y given source i.e., arg max y p ( y j ) . In machine translation, we t parameterized model maximize conditional probability pairs using parallel training corpus. Once conditional distribution learned by model, given corresponding can be generated by searching for maximizes conditional probability. Recently, number papers have proposed use networks directly learn this condi- tional distribution (see, e.g., Kalchbrenner Blunsom, 2013; Cho 2014a; Sutskever 2014; Cho 2014b; Forcada Neco, 1997). This machine approach typ- ically consists two components, rst which encodes source second decodes target y . For instance, two recurrent networks (RNN) were used by (Cho 2014a) (Sutskever 2014) encode variable-length source into xed-length vector decode vector into variable-length target sentence. Despite being quite new approach, machine has already shown promising results. Sutskever (2014) reported machine based on RNNs long short- term memory (LSTM) units achieves close state-of-the-art performance conventional phrase-based machine system on an English-to-French task. 1 Adding components existing systems, for instance, score phrase pairs in phrase table (Cho 2014a) or re-rank candidate translations (Sutskever 2014), has allowed surpass previous state-of-the-art performance level. 2.1 RNN E NCODER D ECODER Here, we describe briey underlying framework, called RNN EncoderDecoder proposed by Cho (2014a) Sutskever (2014) upon which we build novel architecture learns align translate simultaneously. In EncoderDecoder framework, an encoder reads input sentence, sequence vectors = ( 1 T ) into vector c . 2 The most common approach use an RNN such that h t = f ( t ; h t 1 ) (1) c = q ( f 1 T x g ) ; where h t 2 R n is hidden state at time t c is vector generated from sequence hidden states. f q are some nonlinear functions. Sutskever (2014) used an LSTM as f q ( f h 1 ; ; h T g ) = h T for instance.