Figure 1: Screen shots from ve Atari 2600 Games: ( Left-to-right ) Pong, Breakout, Space Invaders, Seaquest, Beam Rider experience replay mechanism [13] which randomly samples previous transitions, thereby smooths training distribution over many past behaviors. We apply our approach range Atari 2600 games implemented The Arcade Learning Envi- ronment (ALE) [3]. Atari 2600 challenging RL testbed that presents agents with high dimen- sional visual input ( 210 160 RGB video at 60Hz) diverse interesting set tasks that were designed be difcult for humans players. Our goal create single neural network agent that able successfully learn play many games possible. The network was not pro- vided with any game-specic information or hand-designed visual features, was not privy internal state emulator; it learned nothing but video input, reward terminal signals, set possible actionsjust human player would. Furthermore network ar- chitecture all hyperparameters used training were kept constant across games. So far network has outperformed all previous RL algorithms on six seven games we have attempted surpassed expert human player on three them. Figure 1 provides sample screenshots ve games used training. 2 Background We consider tasks which agent interacts with environment E this case Atari emulator, sequence actions, observations rewards. At each time-step agent selects action set legal actions, A f 1 : : : K g . action passed emulator modies its internal state score. In general E may be stochastic. emulatorâ€™s internal state not observed agent; instead it observes image x 2 R d emulator, which vector raw pixel values representing current screen. In addition it receives reward r representing change score. Note general score may depend on whole prior sequence actions observations; feedback about action may only be received after many thousands time-steps have elapsed. Since agent only observes images current screen, task partially observed many emulator states are perceptually aliased, i.e. it impossible fully understand current situation only current screen x . We therefore consider sequences actions observations, x 1 1 x 2 :::; 1 x learn strategies depend upon these sequences. All sequences emulator are assumed terminate nite number time-steps. This formalism gives rise large but nite Markov decision process (MDP) which each sequence distinct state. As result, we can apply standard reinforcement learning methods MDPs, simply using complete sequence state representation time . goal agent interact with emulator selecting actions way maximises future rewards. We make standard assumption future rewards are discounted factor per time-step, dene future discounted return time R P T r , where T time-step at which game terminates. We dene optimal action-value function Q ( s; ) maximum expected return achievable by following any strategy, after seeing some sequence s then taking some action , Q ( s; ) = max E [ R j s s; = a; ] , where policy mapping sequences actions (or distributions over actions). The optimal action-value function obeys important identity known as Bellman equation . This based on following intuition: if optimal value Q ( s 0 ; 0 ) sequence s 0 at next time-step was known for all possible actions 0 , then optimal strategy select action 0 2