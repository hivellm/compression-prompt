Z ( jD ) p ( y j f; x ). If posterior at + 1 approximated directly one gets ( jD ; x ; y ). BALD calculates entropy dierence between ^ p , without having compute each candidate x . In contrast, IVM calculates entropy change between . The IVM’s ap- proach cannot calculate entropy full innite dimensional posterior, requires O ( N x N y ) posterior updates. To do these updates eciently, ap- proximate inference performed using Assumed Density Filtering (ADF). Using ADF means that direct approximation ^ p indicating that IVM makes further approximation BALD. Since BALD only requires O (1) posterior updates it can aord use more accurate, iterative procedures, such as EP. Information Theoretic approaches: Maximum Entropy Sampling (MES) Sebastiani Wynn, 2000 explicitly works dataspace (Eqn. (2) ). MES was proposed regression models with input-independent noise. Al- though Eqn. (2) used, second term constant because input independent noise ignored. One cannot, however, use MES heteroscedastic re- gression or classication; it fails dierentiate uncertainty uncertainty (about which our model may be condent). Some toy demonstrations show this ‘information based’ active learning criterion performing pathologically classication by repeatedly querying points close decision boundary or regions high uncertainty e.g. Huang 2010 ]. This because MES inappropriate this domain; BALD distinguishes be- tween uncertainty eliminates problems as we will show. Mutual-information based objective functions are presented Ertin Fuhrmann, 2003 ]. They maximise mutual information variable being measured variable interest. Fuhrmann Fuhrmann, 2003 applies this linear Gaussian models acoustic arrays, Ertin al. Ertin communications channel. Although related, objectives do not work with parameters are not applied classication. Guestrin 2005 Krause 2006 also use mutual information. They specify interest advance maximise expected mutual information predictive distributions observed locations. Although this objective promising regression, it not tractable models with input-dependent observation noise, such as classication or preference learning. Decision theoretic: We briey mention decision theoretic approaches ac- tive learning. Two closely related algorithms, Kapoor 2007 Zhu 2003 ], seek minimize expected cost i.e. loss weighted misclassication probability on all seen future data. These methods observe locations test points their objective functions become monotonic predictive entropies at test points. Kapoor 2007 ] also includes an empirical error term 8