assumes exact match, results Table 8 would score only about 60%). We believe that trained on even larger data sets with larger dimensionality will perform signicantly better, will enable development new innovative applications. Another way improve accuracy provide more than one example relationship. By using ten examples instead one form relationship vector (we average individual together), we have observed improvement accuracy our best models by about 10% absolutely semantic-syntactic test. It possible apply operations solve different tasks. For example, we have observed good accuracy selecting out-of-the-list words, computing average list words, nding most distant vector. This popular type problems certain human intelligence tests. Clearly, there still lot discoveries made using these techniques. 6 Conclusion In this paper we studied quality representations words derived various collection syntactic semantic language tasks. We observed possible train high quality using very simple model architectures, compared popular neural network (both feedforward recurrent). Because much lower computational complexity, possible compute very accurate high dimensional much data set. Using DistBelief distributed framework, should possible train CBOW Skip-gram even corpora one trillion words, basically unlimited size vocabulary. That several orders magnitude than best previously published results similar models. An interesting task where have recently been shown signicantly outperform previous state art SemEval-2012 Task 2 [11]. The publicly available RNN were used together other techniques achieve over 50% increase Spearmanâ€™s rank correlation over previous best result [31]. The neural network based were previously applied many other NLP tasks, example sentiment analysis [12] paraphrase detection [28]. It can expected these applications can benet model architectures described this paper. Our ongoing work shows can successfully applied automatic extension facts Knowledge Bases, also verication correctness existing facts. Results from machine translation experiments also look very promising. In future, it would also interesting compare our techniques Latent Relational Analysis [30] others. We believe that our comprehensive test set will help research community improve existing techniques estimating vectors. We also expect that high quality will become an important building block future NLP applications. 10