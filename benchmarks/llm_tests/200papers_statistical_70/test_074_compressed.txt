In other words, D and play following two-player minimax game with value function V G; D ) : min max V D; ) = E x p data x ) [log x )] + E p ) [log(1 )))] : (1) In next section, we present theoretical analysis adversarial nets, essentially showing that training criterion allows one recover data generating distribution as are given enough capacity, i.e., non-parametric limit. See Figure 1 for less formal, more pedagogical explanation approach. practice, we must implement game using an iterative, numerical approach. Optimizing completion inner loop training computationally prohibitive, on nite datasets would result overtting. Instead, we alternate between k steps optimizing one step optimizing . This results being maintained near its optimal solution, so long as changes slowly enough. This strategy analogous way that SML/PCD [31, 29] training maintains samples from Markov chain from one learning step next order avoid burning Markov chain as part inner loop learning. The procedure formally presented Algorithm 1. practice, equation 1 may not provide sufcient gradient for learn well. Early learning, when poor, can reject samples with high condence because they are clearly different from training data. In this case, log(1 ))) saturates. Rather than training minimize log(1 ))) we can train maximize log z )) . This objective function results same xed point dynamics and D but provides much stronger gradients early learning.