alignment is not considered latent variable. Instead, alignment model directly com- putes soft alignment, which allows gradient cost function backpropagated through. This gradient can used train alignment model as well as whole translation model jointly. We can understand approach taking weighted sum all annotations as computing an expected annotation , where expectation is over possible alignments. Let ij probability that target word y i is aligned to, or translated from, source word Then, i -th context vector c i is expected annotation over all annotations with probabilities ij The probability ij , or its associated energy e ij , reects importance annotation with respect previous hidden state s deciding next state s generating y Intuitively, this implements mechanism attention decoder. The decoder decides parts source sentence pay attention to. By letting decoder have attention mechanism, we relieve encoder from burden having encode all information source sentence into xed- length vector. With this new approach information can spread throughout annotations, which can selectively retrieved decoder accordingly. 3.2 E NCODER : B IDIRECTIONAL RNN FOR A NNOTATING S EQUENCES The usual RNN, described Eq. (1), reads input sequence order starting from rst symbol last one T However, proposed scheme, we would like annotation each word summarize not only preceding words, but also following words. Hence, we propose use bidirectional RNN (BiRNN, Schuster Paliwal, 1997), which has been successfully used recently speech recognition (see, e.g., Graves et al. , 2013). A BiRNN consists forward backward RNN’s. The forward RNN ! f reads input sequence as it ordered (from T ) calculates sequence forward hidden states ( ! ; ; ! T ) The backward RNN f reads reverse order (from T ), resulting sequence backward hidden states ( ; ; T ) We obtain annotation for each word concatenating forward hidden state ! backward one , i.e., = ! > ; > > In this way, contains summaries both preceding words following words. Due tendency RNNs better represent recent inputs, will focused on words around This annotations used decoder alignment model later compute context vector (Eqs. (5)(6)). See Fig. for graphical illustration proposed model. 4 E XPERIMENT S ETTINGS We evaluate proposed approach on task English-to-French translation. We use bilin- gual, parallel corpora provided by ACL WMT ’14. 3 As comparison, we also report perfor- mance an RNN EncoderDecoder which was proposed recently by Cho et al. (2014a). We use same training procedures same dataset for both models. 4 4.1 D ATASET WMT ’14 contains following English-French parallel corpora: Europarl (61M words), news commentary (5.5M), UN (421M) two crawled corpora 90M 272.5M words respectively, totaling 850M words. Following procedure described Cho et al. (2014a), we reduce size combined corpus have 348M words using data selection method by Axelrod et al. (2011). 5 We do not use any monolingual data other than mentioned parallel corpora, although it may possible use much larger monolingual corpus pretrain an encoder. We concatenate news-test-