We now describe the exact architecture used for all seven Atari games. The input to the neural network consists is an 84  84  4 image produced by  . The rst hidden layer convolves 16 8  8 lters with stride 4 with the input image and applies a rectier nonlinearity [10, 18]. The second hidden layer convolves 32 4  4 lters with stride 2 , again followed by a rectier nonlinearity. The nal hidden layer is fully-connected and consists of 256 rectier units. The output layer is a fully- connected linear layer with a single output for each valid action. The number of valid actions varied between 4 and 18 on the games we considered. We refer to convolutional networks trained with our approach as Deep Q-Networks (DQN). 5 Experiments So far, we have performed experiments on seven popular ATARI games  Beam Rider, Breakout, Enduro, Pong, Q*bert, Seaquest, Space Invaders. We use the same network architecture, learning algorithm and hyperparameters settings across all seven games, showing that our approach is robust enough to work on a variety of games without incorporating game-specic information. While we evaluated our agents on the real and unmodied games, we made one change to the reward structure of the games during training only. Since the scale of scores varies greatly from game to game, we xed all positive rewards to be 1 and all negative rewards to be  1 , leaving 0 rewards unchanged. Clipping the rewards in this manner limits the scale of the error derivatives and makes it easier to use the same learning rate across multiple games. At the same time, it could affect the performance of our agent since it cannot differentiate between rewards of different magnitude. In these experiments, we used the RMSProp algorithm with minibatches of size 32. The behavior policy during training was  -greedy with  annealed linearly from 1 to 0 : 1 over the rst million frames, and xed at 0 : 1 thereafter. We trained for a total of 10 million frames and used a replay memory of one million most recent frames. Following previous approaches to playing Atari games, we also use a simple frame-skipping tech- nique [3]. More precisely, the agent sees and selects actions on every k th frame instead of every frame, and its last action is repeated on skipped frames. Since running the emulator forward for one step requires much less computation than having the agent select an action, this technique allows the agent to play roughly k times more games without signicantly increasing the runtime. We use k = 4 for all games except Space Invaders where we noticed that using k = 4 makes the lasers invisible because of the period at which they blink. We used k = 3 to make the lasers visible and this change was the only difference in hyperparameter values between any of the games.