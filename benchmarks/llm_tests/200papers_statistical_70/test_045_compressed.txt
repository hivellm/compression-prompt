For all following models, training complexity proportional O = E T Q; (1) where E number training epochs, T number words training set Q dened further for each model architecture. Common choice E = 3 50 T up one billion. All models trained using stochastic gradient descent backpropagation [26]. 2.1 Feedforward Neural Net Language Model (NNLM) The probabilistic feedforward neural network language model has been proposed [1]. It consists input, projection, hidden output layers. At input layer, N previous words encoded 1-of- V coding, V size vocabulary. The input then projected P has dimensionality D , shared matrix. As only inputs active at any given time, composition relatively cheap operation. The NNLM architecture becomes complex computation between layer, as values dense. For common choice = 10 , size ( P ) might 500 2000, while hidden size H typically 500 1000 units. Moreover, used compute probability distribution over all vocabulary, resulting an output with dimensionality V . Thus, computational complexity per each example Q D + D H + H V; (2) dominating term H V . However, several practical solutions were proposed avoiding it; either hierarchical versions softmax [25, 23, 18], or avoiding normalized completely by not normalized during [4, 9]. With binary tree representations vocabulary, units need evaluated can go down around log 2 ( V ) . Thus, most complexity caused by term D H . In our models, we use hierarchical softmax where vocabulary represented as Huffman binary tree. This follows previous observations that frequency words works well for obtaining classes neural net language models [16]. Huffman trees assign short binary codes frequent words, this further reduces number output units that need be evaluated: while balanced binary tree would require log 2 ( V ) outputs be evaluated, Huffman tree based hierarchical softmax requires only about log 2 ( U nigram