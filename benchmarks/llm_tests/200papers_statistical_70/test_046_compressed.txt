perplexity ( V )) . For example when vocabulary size one million words, this results about two times speedup evaluation. While this not crucial speedup for neural network LMs computational bottleneck N D term, we will later propose architectures that do not have layers thus depend heavily on efciency softmax normalization. 2.2 Recurrent Neural Net Language Model (RNNLM) Recurrent neural network based language has been proposed overcome certain limitations feedforward NNLM, such need specify context length (the order N ), because theoretically RNNs can efciently represent more complex patterns than shallow neural networks [15, 2]. The RNN does not have a projection layer; only input, output layer. What special for this type recurrent matrix that connects itself, using time-delayed connections. This allows recurrent form some kind short term memory, information from past be represented by state that gets updated based on current input state previous time step. The complexity per training example RNN Q = + V; (3) where word representations D have same dimensionality . Again, term V can be efciently reduced log 2 ( V ) by using hierarchical softmax. Most complexity then comes from . 3