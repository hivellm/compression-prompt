Figure 1: The graphical illus- tration of proposed model trying to generate t -th tar- get word y t given a source sentence ( x 1 x 2 x T ) . In a new model architecture, we dene each conditional probability in Eq. (2) as: p ( j x = g s (4) where s is an RNN hidden state for time , computed by s = f s It should be noted that unlike existing encoderdecoder ap- proach (see Eq. (2)), here probability is conditioned on distinct context vector c for each target word . The context vector depends on sequence annotations T to which an encoder maps input sentence. Each annotation h contains information about whole input sequence with strong focus on parts surrounding -th word input sequence. We explain in detail how annotations are com- puted in next section. The context vector c is, then, computed as a weighted sum of these annotations h c = T x X j =1 ij h j : (5) The weight ij of each annotation h j is computed by ij = exp ( e ij )