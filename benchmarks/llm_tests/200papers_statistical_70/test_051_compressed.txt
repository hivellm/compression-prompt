Google News about day, while training time for Skip-gram was about three days. For experiments reported further, used just one training epoch (again, decrease learning linearly so that it approaches zero at end training). Training twice much using gives comparable or better results than iterating over same epochs, shown Table 5, provides additional small speedup. 4.4 Large Scale Parallel Training Models As mentioned earlier, have implemented various distributed framework Dis- tBelief. Below report several trained Google News 6B set, with mini-batch asynchronous gradient descent adaptive learning rate procedure called Ada- grad [7]. We used 50 to 100 replicas during training. The number CPU cores is an 8