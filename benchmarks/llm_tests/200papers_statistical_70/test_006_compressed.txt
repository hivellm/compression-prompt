h (( )) exp( t 2 log(2) 5 10 3 dierence Figure 1: Analytic approximation 1 binary entropy error function by a squared exponential ). The absolute error remains under 3 10 3 For most practically relevant kernels, objective (5) is smooth dierentiable function x so gradient-based optimisation procedures can be used nd maximally informative query. 3.1 Extension: Learning Hyperparameters many applications parameter set naturally divides into parameters interest, nuisance parameters i.e. = f g such settings, active may want query points that are maximally informative about while not caring about By integrating Eqn. (1) over nuisance parameters, BALDâ€™s objective re-derived as: H E p jD y j x E p jD H E p j D [ y j x ] (6) context GP models, hyperparameters typically control smooth- ness or spatial length-scale functions. If we maintain posterior distribution over these hyperparameters, which we do e. g. via Hamiltonian Monte Carlo, we choose either treat them nuisance parameters use Eq. 6, or include them in perform active over them well. certain cases, such automatic relevance determination [ Rasmussen Williams, 2005 ], it may even make sense treat hyperparameters variables primary interest, function f itself nuisance parameter 3.2 Preference Learning Our active framework for GPC be extended important problem preference [ Furnkranz Hullermeier, 2003 Chu Ghahramani, 2005 ]. preference dataset consists for pairs items u v 2 X 2 with binary labels, y 2 f 0 1 g y = 1 means instance u is preferred v denoted u v . The task is predict preference relation between any u v ). We can view this a special case building classier on pairs inputs 6