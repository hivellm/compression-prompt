A M ODEL A RCHITECTURE A.1 A RCHITECTURAL C HOICES The proposed scheme in Section 3 general framework where one can freely dene, for instance, activation functions f recurrent neural networks (RNN) and alignment model . Here, we describe choices we made for experiments in this paper. A.1.1 R ECURRENT N EURAL N ETWORK For activation function f an RNN, we use gated hidden unit recently proposed by Cho et al. (2014a). The gated hidden unit an alternative conventional simple units such as an element-wise tanh . This gated unit similar long short-term memory (LSTM) unit proposed earlier by Hochreiter Schmidhuber (1997), sharing with it ability better model learn long-term dependencies. This made possible having computation paths in unfolded RNN for which product derivatives close 1. These paths allow gradients ow backward easily without suffering too much from vanishing effect (Hochreiter, 1991; Bengio et al. , 1994; Pascanu et al. , 2013a). It therefore possible use LSTM units instead gated hidden unit described here, as was done in similar context Sutskever et al. (2014). The new state RNN employing n gated hidden units 8 computed = f c = (1 z ~ where an element-wise multiplication, z output update gates (see below). The proposed updated state ~ computed ~ = tanh W e + U [ r ] + Cc where e 2 R m an m -dimensional embedding word , r output reset gates (see below). When represented as -of- K vector, e simply column an embedding matrix E 2 R m K . Whenever possible, we omit bias terms make equations less cluttered. The update gates z allow each hidden unit maintain its previous activation, reset gates r control how much what information from previous state should be reset. We compute them z = W e + U + C z c r = W r e + U r + C r c where logistic sigmoid function. At each step decoder, we compute output probability (Eq. (4)) as multi-layered func- tion (Pascanu et al. , 2014). We use single hidden layer maxout units (Goodfellow et al. , 2013) normalize output probabilities (one for each word) with softmax function (see Eq. (6)). A.1.2 A LIGNMENT M ODEL The alignment model should be designed considering that model needs be evaluated T x T y times for each sentence pair lengths T x T y . In order reduce computation, we use single- layer multilayer perceptron such that ; h j = v > tanh ( W s + U h j ) ; where W 2 R n n ; U 2 R n 2 n v 2 R n are weight matrices. Since U h j does not depend on , we can pre-compute it in advance minimize computational cost.