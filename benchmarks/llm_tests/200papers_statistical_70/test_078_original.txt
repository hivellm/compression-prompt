4.2 Convergence of Algorithm 1 Proposition 2. If G and D have enough capacity, and at each step of Algorithm 1, the discriminator is allowed to reach its optimum given G , and p g is updated so as to improve the criterion E x  p data [log D  G ( x )] + E x  p g [log(1  D  G ( x ))] then p g converges to p data Proof. Consider V ( G; D ) = U ( p g ; D ) as a function of p g as done in the above criterion. Note that U ( p g ; D ) is convex in p g . The subderivatives of a supremum of convex functions include the derivative of the function at the point where the maximum is attained. In other words, if f ( x ) = sup  2A f  ( x ) and f  ( x ) is convex in x for every  , then @f  ( x ) 2 @f if  = arg sup  2A f  ( x ) . This is equivalent to computing a gradient descent update for p g at the optimal D given the cor- responding G . sup D U ( p g ; D ) is convex in p g with a unique global optima as proven in Thm 1, therefore with sufciently small updates of p g , p g converges to p x , concluding the proof.