Z q t ( jD ) p ( y t +1 j f; x t +1 ). If the posterior at t + 1 is approximated directly one gets q t +1 ( jD ; x t +1 ; y t +1 ). BALD calculates the entropy dierence between q t and ^ p t +1 , without having to compute q t +1 for each candidate x . In contrast, the IVM calculates the entropy change between q t and q t +1 . The IVM’s ap- proach cannot calculate the entropy of the full innite dimensional posterior, and requires O ( N x N y ) posterior updates. To do these updates eciently, ap- proximate inference is performed using Assumed Density Filtering (ADF). Using ADF means that q t +1 is a direct approximation to ^ p t +1 , indicating that the IVM makes a further approximation to BALD. Since BALD only requires O (1) posterior updates it can aord to use more accurate, iterative procedures, such as EP. Information Theoretic approaches: Maximum Entropy Sampling (MES) [ Sebastiani and Wynn, 2000 ] explicitly works in dataspace (Eqn. (2) ). MES was proposed for regression models with input-independent observation noise. Al- though Eqn. (2) is used, the second term is constant because of input independent noise and is ignored. One cannot, however, use MES for heteroscedastic re- gression or classication; it fails to dierentiate between model uncertainty and observation uncertainty (about which our model may be condent). Some toy demonstrations show this ‘information based’ active learning criterion performing pathologically in classication by repeatedly querying points close the decision boundary or in regions of high observation uncertainty e.g. [ Huang et al., 2010 ]. This is because MES is inappropriate in this domain; BALD distinguishes be- tween observation and model uncertainty and eliminates these problems as we will show. Mutual-information based objective functions are presented in [ Ertin et al., , Fuhrmann, 2003 ]. They maximise the mutual information between the variable being measured and the variable of interest. Fuhrmann [ Fuhrmann, 2003 ] applies this to linear Gaussian models and acoustic arrays, Ertin et al. [ Ertin et al., ] to a communications channel. Although related, these objectives do not work with the model parameters and are not applied to classication. [ Guestrin et al., 2005 , Krause et al., 2006 ] also use mutual information. They specify interest points in advance and maximise the expected mutual information between the predictive distributions at these points and at the observed locations. Although this is a objective is promising for regression, it is not tractable for models with input-dependent observation noise, such as classication or preference learning. Decision theoretic: We briey mention decision theoretic approaches to ac- tive learning. Two closely related algorithms, [ Kapoor et al., 2007 , Zhu et al., 2003 ], seek to minimize the expected cost i.e. loss weighted misclassication probability on all seen and future data. These methods observe the locations of the test points and their objective functions become monotonic in the predictive entropies at the test points. [ Kapoor et al., 2007 ] also includes an empirical error term 8