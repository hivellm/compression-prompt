rameters inferred, ). The central goal information theoretic ac- tive learning reduce number possible hypotheses maximally fast, i.e. minimize uncertainty parameters using Shannonâ€™s entropy Cover 1991 Data points selected satisfy arg min H[ = R log )d . Solving problem general NP-hard; however, common sequential decision making tasks a myopic (greedy) approxi- mation made Heckerman 1995 It has been shown myopic policy perform near-optimally Golovin Krause, 2010 Dasgupta, 2005 Therefore, data point maximises decrease expected entropy: max E y; Note expectation over unseen required. Many works e.g. MacKay, 1992 Krishnapuram Lawrence 2003 propose using directly. However, posteriors often high dimen- sional computing their usually intractable. Furthermore, nonparametric processes space innite dimensional becomes poorly dened. To avoid gridding space (exponentially hard dimensionality), sampling (from it notoriously hard estimate without introducing bias Panzeri Petersen, 2007 ]), these papers make Gaussian low dimensional approximations calculate approximate posterior. A second computational diculty arises; if data points consideration, responses may seen, then O ), potentially expensive, updates required calculate (1). An important insight arises if note equivalent conditional mutual information between unknown parameters, I[ Using insight it simple show rearranged compute space: E overcomes challenges described . Entropies now calculated in, usually low dimensional, space. For binary classication, these just Bernoulli variables. Also now conditioned only on O updates required. also provides us an interesting intuition objective; seek model marginally most uncertain (high H[ ]), but individual settings parameters condent (low E ). This interpreted seeking parameters under disagree outcome most, so refer Bayesian Active Learning by Disagreement (BALD). We present a method apply directly GPC preference learning. We no longer need build our entropy calculation around type approximation (as 3