than few hundred millions words, modest dimensionality between 50 - 100. We use recently techniques measuring quality resulting representa- tions, expectation not only will similar tend close each other, but have multiple degrees similarity [20]. This been observed earlier context inectional languages example, nouns multiple endings, if search similar subspace original space, is possible nd similar endings [13, 14]. Somewhat surprisingly, found similarity representations goes beyond syntactic regularities. Using offset technique algebraic operations per- formed vectors, example vector(King) vector(Man) + vec- tor(Woman) results closest representation Queen [20]. paper, try maximize accuracy these operations developing new preserve among words. We design new comprehensive test set measuring both syntactic semantic 1 , show such high accuracy. Moreover, discuss how time accuracy depends dimensionality amount data. 1.2 Previous Work Representation continuous long history [10, 26, 8]. A very popular architecture estimating language (NNLM) [1], feedforward projection layer non-linear hidden layer learn jointly representation statistical language This work been followed others. Another interesting architecture NNLM presented [13, 14], single hidden layer. The then train NNLM. Thus, even without constructing full NNLM. work, directly extend architecture, focus just step It later improve simplify NLP applications [4, 5, 29]. Estimation itself performed trained various corpora [4, 29, 23, 19, 9], some resulting made available future research comparison 2 . However, far know, these more computationally expensive one [13], exception certain version log-bilinear diagonal weight matrices [23]. 2 Model Architectures Many different types models estimating continuous representations words, including well-known Latent Semantic Analysis (LSA) Latent Dirichlet Allocation (LDA). paper, focus distributed representations networks, previously they perform signicantly better LSA preserving among [20, 31]; LDA moreover becomes computationally very expensive large data sets. Similar [18], compare different dene rst computational complex- ity number parameters need accessed fully train model. Next, will try maximize accuracy, while minimizing computational complexity.