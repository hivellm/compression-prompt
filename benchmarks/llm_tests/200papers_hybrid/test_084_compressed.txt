[16] E., Osindero, Teh, Y. (2006). fast algorithm belief nets. Neural Computation 18 15271554. [17] E., Srivastava, N., Sutskever, I., Salakhutdinov, R. (2012b). Improving networks preventing co-adaptation feature detectors. arXiv:1207.0580. [18] Hyv ¤ arinen, (2005). Estimation non-normalized statistical models score matching. J. Machine Res. 6 [19] Jarrett, K., Kavukcuoglu, K., Ranzato, M., What best multi-stage architecture object recognition? Proc. International Computer Vision (ICCV’09) 21462153. IEEE. [20] Kingma, Welling, M. Auto-encoding variational bayes. Interna- tional Representations (ICLR) multiple images. University ImageNet classication convolutional networks. NIPS’2012 Bottou, L., Haffner, (1998). Gradient-based applied document recognition. IEEE 22782324. Rezende, Mohamed, Wierstra, backpropagation approximate inference models. arXiv:1401.4082. Rifai, Dauphin, process sampling contractive auto-encoders. ICML’12 Deep machines. AISTATS’2009 Smolensky, (1986). Information processing dynamical systems: Foundations harmony theory. Rumelhart McClelland, Parallel Distributed Processing chapter 194281. MIT Press, Cambridge. Susskind, Anderson, (2010). Toronto dataset. Report UTML TR 2010-001, U. Tieleman, Training restricted Boltzmann machines approximations likelihood gradient. W. W. Cohen, McCallum, S. T. Roweis, ICML 10641071. ACM. [30] Vincent, P., Larochelle, H., Manzagol, P.-A. Extracting composing robust denoising autoencoders. ICML [31] Younes, L. (1999). On convergence Markovian stochastic algorithms rapidly decreasing ergodicity rates. Stochastics Stochastic Reports 65 (3), 177228. 9