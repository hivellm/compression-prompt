@ x i  1 to preserve norm only in relevant directions. In practice we show that these so- lutions improve performance on both the pathological synthetic datasets considered as well as on polyphonic music prediction and language modelling. Acknowledgements We would like to thank the Theano development team as well (particularly to Frederic Bastien, Pascal Lam- blin and James Bergstra) for their help. We acknowledge NSERC, FQRNT, CIFAR, RQCHP and Compute Canada for the resources they provided. References Bastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I., Bergeron, A., Bouchard, N., and Bengio, Y. (2012). Theano: new features and speed improvements. Submited to Deep Learning and Un- supervised Feature Learning NIPS 2012 Workshop. Bengio, Y., Frasconi, P., and Simard, P. (1993). The problem of learning long-term dependencies in re- current networks. pages 1183{1195, San Francisco. IEEE Press. (invited paper). Bengio, Y., Simard, P., and Frasconi, P. (1994). Learn- ing long-term dependencies with gradient descent is dicult. IEEE Transactions on Neural Networks , 5 (2), 157{166. Bengio, Y., Boulanger-Lewandowski, N., and Pascanu, R. (2012). Advances in optimizing recurrent net- works. Technical Report arXiv:1212.0901, U. Mon- treal. Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde- Farley, D., and Bengio, Y. (2010). Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientic Computing Conference (SciPy) . Oral Presentation. Boulanger-Lewandowski, N., Bengio, Y., and Vincent, P. (2012). Modeling temporal dependencies in high- dimensional sequences: Application to polyphonic music generation and transcription. In Proceed- ings of the Twenty-nine International Conference on Machine Learning (ICML’12) . ACM. Doya, K. (1993). Bifurcations of recurrent neural net- works in gradient descent learning. IEEE Transac- tions on Neural Networks , 1 , 75{80. Doya, K. and Yoshizawa, S. (1991). Adaptive synchro- nization of neural and physical oscillators. In J. E. Moody, S. J. Hanson, and R. Lippmann, editors, NIPS , pages 109{116. Morgan Kaufmann. Duchi, J. C., Hazan, E., and Singer, Y. (2011). Adap- tive subgradient methods for online learning and stochastic optimization. Journal of Machine Learn- ing Research , 12 , 2121{2159. Elman, J. (1990). Finding structure in time. Cognitive Science , 14 (2), 179{211. Graves, A., Liwicki, M., Fernandez, S., Bertolami, R., Bunke, H., and Schmidhuber, J. (2009). A Novel Connectionist System for Unconstrained Handwrit- ing Recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence , 31 (5), 855{868. Hochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural Computation , 9 (8), 1735{1780. Jaeger, H. (2012). Long short-term memory in echo state networks: Details of a simulation study. Tech- nical report, Jacobs University Bremen. Jaeger, H., Lukosevicius, M., Popovici, D., and Siew- ert, U. (2007). Optimization and applications of echo state networks with leaky- integrator neurons. Neural Networks , 20 (3), 335{352. Lukosevicius, M. and Jaeger, H. (2009). Reservoir computing approaches to recurrent neural network training. Computer Science Review , 3 (3), 127{149. Martens, J. and Sutskever, I. (2011). Learning recur- rent neural networks with Hessian-free optimization. In Proc. ICML’2011 . ACM. Mikolov, T. (2012). Statistical Language Models based on Neural Networks . Ph.D. thesis, Brno University of Technology.