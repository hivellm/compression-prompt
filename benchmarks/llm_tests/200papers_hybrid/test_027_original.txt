Figure 6. We plot the error surface of a single hidden unit recurrent network, highlighting the existence of high cur- vature walls. The solid lines depicts standard trajectories that gradient descent might follow. Using dashed arrow the diagram shows what would happen if the gradients is rescaled to a xed size when its norm is above a threshold. explode so does the curvature along v , leading to a wall in the error surface , like the one seen in Fig. 6. If this holds, then it gives us a simple solution to the exploding gradients problem depicted in Fig. 6. If both the gradient and the leading eigenvector of the curvature are aligned with the exploding direction v , it follows that the error surface has a steep wall perpen- dicular to v (and consequently to the gradient). This means that when stochastic gradient descent (SGD) reaches the wall and does a gradient descent step, it will be forced to jump across the valley moving perpen- dicular to the steep walls, possibly leaving the valley and disrupting the learning process. The dashed arrows in Fig. 6 correspond to ignoring the norm of this large step , ensuring that the model stays close to the wall. The key insight is that all the steps taken when the gradient explodes are aligned with v and ignore other descent direction (i.e. the model moves perpendicular to the wall). At the wall, a small-norm step in the direction of the gradient there- fore merely pushes us back inside the smoother low- curvature region besides the wall, whereas a regular gradient step would bring us very far, thus slowing or preventing further training. Instead, with a bounded step, we get back in that smooth region near the wall where SGD is free to explore other descent directions. The important addition in this scenario to the classical high curvature valley, is that we assume that the val- ley is wide, as we have a large region around the wall where if we land we can rely on rst order methods to move towards the local minima. This is why just clipping the gradient might be sucient, not requiring the use a second order method. Note that this algo- rithm should work even when the rate of growth of the gradient is not the same as the one of the curvature (a case for which a second order method would fail as the ratio between the gradient and curvature could still explode). Our hypothesis could also help to understand the re- cent success of the Hessian-Free approach compared to other second order methods. There are two key dif- ferences between Hessian-Free and most other second- order algorithms. First, it uses the full Hessian matrix and hence can deal with exploding directions that are not necessarily axis-aligned. Second, it computes a new estimate of the Hessian matrix before each up- date step and can take into account abrupt changes in curvature (such as the ones suggested by our hypothe- sis) while most other approaches use a smoothness as- sumption, i.e., averaging 2nd order signals over many steps. 3. Dealing with the exploding and vanishing gradient 3.1. Previous solutions Using an L1 or L2 penalty on the recurrent weights can help with exploding gradients. Given that the parame- ters initialized with small values, the spectral radius of W rec is probably smaller than 1, from which it follows that the gradient can not explode (see necessary condi- tion found in section 2.1). The regularization term can ensure that during training the spectral radius never exceeds 1. This approach limits the model to a sim- ple regime (with a single point attractor at the origin), where any information inserted in the model has to die out exponentially fast in time. In such a regime we can not train a generator network, nor can we exhibit long term memory traces. Doya (1993) proposes to pre-program the model (to initialize the model in the right regime) or to use teacher forcing . The rst proposal assumes that if the model exhibits from the beginning the same kind of asymptotic behaviour as the one required by the target, then there is no need to cross a bifurcation boundary. The downside is that one can not always know the required asymptotic behaviour, and, even if such information is known, it is not trivial to initial- ize a model in this specic regime. We should also note that such initialization does not prevent cross- ing the boundary between basins of attraction, which, as shown, could happen even though no bifurcation boundary is crossed. Teacher forcing is a more interesting, yet a not very well understood solution. It can be seen as a way of initializing the model in the right regime and the right