Figure 1: The graphical illus- tration proposed model trying to generate t -th tar- get word t given source sentence 2 T . In new model architecture, we dene each conditional probability Eq. (2) as: p g (4) where an RNN hidden state for time , computed by f It should be noted that unlike existing encoderdecoder ap- proach (see Eq. (2)), here probability conditioned distinct context for target . context depends sequence annotations T to which an encoder maps sentence. Each annotation contains information about whole sequence with strong focus parts surrounding -th sequence. We explain detail how are com- puted next section. context vector is, then, computed as weighted sum these annotations T X j =1 ij j (5) weight ij each annotation j is computed by ij = exp e ij