More recently, there has been a revival of interest in combining deep learning with reinforcement learning. Deep neural networks have been used to estimate the environment E ; restricted Boltzmann machines have been used to estimate the value function [21]; or the policy [9]. In addition, the divergence issues with Q-learning have been partially addressed by gradient temporal-difference methods. These methods are proven to converge when evaluating a xed policy with a nonlinear function approximator [14]; or when learning a control policy with linear function approximation using a restricted variant of Q-learning [15]. However, these methods have not yet been extended to nonlinear control. Perhaps the most similar prior work to our own approach is neural tted Q-learning (NFQ) [20]. NFQ optimises the sequence of loss functions in Equation 2, using the RPROP algorithm to update the parameters of the Q-network. However, it uses a batch update that has a computational cost per iteration that is proportional to the size of the data set, whereas we consider stochastic gradient updates that have a low constant cost per iteration and scale to large data-sets. NFQ has also been successfully applied to simple real-world control tasks using purely visual input, by rst using deep autoencoders to learn a low dimensional representation of the task, and then applying NFQ to this representation [12]. In contrast our approach applies reinforcement learning end-to-end, directly from the visual inputs; as a result it may learn features that are directly relevant to discriminating action-values. Q-learning has also previously been combined with experience replay and a simple neural network [13], but again starting with a low-dimensional state rather than raw visual inputs. The use of the Atari 2600 emulator as a reinforcement learning platform was introduced by [3], who applied standard reinforcement learning algorithms with linear function approximation and generic visual features. Subsequently, results were improved by using a larger number of features, and using tug-of-war hashing to randomly project the features into a lower-dimensional space [2]. The HyperNEAT evolutionary architecture [8] has also been applied to the Atari platform, where it was used to evolve (separately, for each distinct game) a neural network representing a strategy for that game. When trained repeatedly against deterministic sequences using the emulator’s reset facility, these strategies were able to exploit design aws in several Atari games. 4 Deep Reinforcement Learning Recent breakthroughs in computer vision and speech recognition have relied on efciently training deep neural networks on very large training sets. The most successful approaches are trained directly from the raw inputs, using lightweight updates based on stochastic gradient descent. By feeding sufcient data into deep neural networks, it is often possible to learn better representations than handcrafted features [11]. These successes motivate our approach to reinforcement learning. Our goal is to connect a reinforcement learning algorithm to a deep neural network which operates directly on RGB images and efciently process training data by using stochastic gradient updates. Tesauro’s TD-Gammon architecture provides a starting point for such an approach. This architec- ture updates the parameters of a network that estimates the value function, directly from on-policy samples of experience, s t ; a t ; r t ; s t +1 ; a t +1 , drawn from the algorithm’s interactions with the envi- ronment (or by self-play, in the case of backgammon). Since this approach was able to outperform the best human backgammon players 20 years ago, it is natural to wonder whether two decades of hardware improvements, coupled with modern deep neural network architectures and scalable RL algorithms might produce signicant progress. In contrast to TD-Gammon and similar online approaches, we utilize a technique known as expe- rience replay [13] where we store the agent’s experiences at each time-step, e t = ( s t ; a t ; r t ; s t +1 ) in a data-set D = e 1 ; :::; e N , pooled over many episodes into a replay memory . During the inner loop of the algorithm, we apply Q-learning updates, or minibatch updates, to samples of experience, e  D , drawn at random from the pool of stored samples. After performing experience replay, the agent selects and executes an action according to an  -greedy policy. Since using histories of arbitrary length as inputs to a neural network can be difcult, our Q-function instead works on xed length representation of histories produced by a function  . The full algorithm, which we call deep Q-learning , is presented in Algorithm 1. This approach has several advantages over standard online Q-learning [23]. First, each step of experience is potentially used in many weight updates, which allows for greater data efciency. 4