Figure 1: Screen shots from ve Atari 2600 Games: ( Left-to-right ) Pong, Breakout, Space Invaders, Seaquest, Beam Rider an experience replay mechanism [13] which randomly samples previous transitions, and thereby smooths the training distribution over many past behaviors. We apply our approach to a range of Atari 2600 games implemented in The Arcade Learning Envi- ronment (ALE) [3]. Atari 2600 is a challenging RL testbed that presents agents with a high dimen- sional visual input ( 210  160 RGB video at 60Hz) and a diverse and interesting set of tasks that were designed to be difcult for humans players. Our goal is to create a single neural network agent that is able to successfully learn to play as many of the games as possible. The network was not pro- vided with any game-specic information or hand-designed visual features, and was not privy to the internal state of the emulator; it learned from nothing but the video input, the reward and terminal signals, and the set of possible actionsjust as a human player would. Furthermore the network ar- chitecture and all hyperparameters used for training were kept constant across the games. So far the network has outperformed all previous RL algorithms on six of the seven games we have attempted and surpassed an expert human player on three of them. Figure 1 provides sample screenshots from ve of the games used for training. 2 Background We consider tasks in which an agent interacts with an environment E , in this case the Atari emulator, in a sequence of actions, observations and rewards. At each time-step the agent selects an action a t from the set of legal game actions, A = f 1 ; : : : ; K g . The action is passed to the emulator and modies its internal state and the game score. In general E may be stochastic. The emulatorâ€™s internal state is not observed by the agent; instead it observes an image x t 2 R d from the emulator, which is a vector of raw pixel values representing the current screen. In addition it receives a reward r t representing the change in game score. Note that in general the game score may depend on the whole prior sequence of actions and observations; feedback about an action may only be received after many thousands of time-steps have elapsed. Since the agent only observes images of the current screen, the task is partially observed and many emulator states are perceptually aliased, i.e. it is impossible to fully understand the current situation from only the current screen x t . We therefore consider sequences of actions and observations, s t = x 1 ; a 1 ; x 2 ; :::; a t  1 ; x t , and learn game strategies that depend upon these sequences. All sequences in the emulator are assumed to terminate in a nite number of time-steps. This formalism gives rise to a large but nite Markov decision process (MDP) in which each sequence is a distinct state. As a result, we can apply standard reinforcement learning methods for MDPs, simply by using the complete sequence s t as the state representation at time t . The goal of the agent is to interact with the emulator by selecting actions in a way that maximises future rewards. We make the standard assumption that future rewards are discounted by a factor of  per time-step, and dene the future discounted return at time t as R t = P T t 0 = t  t 0  t r t 0 , where T is the time-step at which the game terminates. We dene the optimal action-value function Q  ( s; a ) as the maximum expected return achievable by following any strategy, after seeing some sequence s and then taking some action a , Q  ( s; a ) = max  E [ R t j s t = s; a t = a;  ] , where  is a policy mapping sequences to actions (or distributions over actions). The optimal action-value function obeys an important identity known as the Bellman equation . This is based on the following intuition: if the optimal value Q  ( s 0 ; a 0 ) of the sequence s 0 at the next time-step was known for all possible actions a 0 , then the optimal strategy is to select the action a 0 2