(such 50 - 100). Given Equation 4, increasing amount twice results increase computational complexity increasing vector size twice. For experiments reported Tables 2 4, three epochs stochastic gradi- ent descent backpropagation. We chose starting learning rate 0.025 decreased linearly, so approaches zero at end last epoch. 4.3 Comparison Model Architectures First compare different architectures for deriving dimensionality vectors. further experiments, use full questions new Semantic-Syntactic Word Relationship set, i.e. unrestricted 30k vocabulary. also include introduced [20] focuses similarity between 3 . consists several LDC corpora described detail [18] (320M words, 82K vocabulary). these provide previously recurrent neural network language took weeks train single CPU. feed- forward NNLM number units DistBelief parallel [6], history previous (thus, NNLM more parameters RNNLM, projection layer ). Table 3, can be seen from RNN (as [20]) perform well mostly questions. NNLM perform signicantly RNN this not surprising, RNNLM are directly connected non-linear layer. CBOW architecture NNLM tasks, semantic one. Finally, Skip-gram architecture works slightly worse task CBOW (but still NNLM), much semantic part all other models. Next, evaluated our models one CPU only compared results against publicly available vectors. comparison given Table 4. CBOW was subset