estimate since data center machines shared production tasks, usage can uctuate quite bit. Note due overhead distributed framework, CPU usage CBOW Skip-gram much closer than their single-machine implementations. reported Table 6. 4.5 Microsoft Research Sentence Completion Challenge Microsoft Sentence Completion Challenge has been recently introduced as advancing language modeling NLP techniques This consists 1040 sentences, where one missing goal select coherent rest sentence, given list ve reasonable choices. Performance several techniques already set, including N-gram models, LSA-based [32], log-bilinear [24] recurrent neural networks currently holds 55.4% benchmark [19]. We have explored architecture task. First, train 640- dimensional 50M provided Then, compute unknown at input, predict all surrounding nal then sum these individual predictions. Using scores, choose likely A short summary some previous together presented 7. While itself does not perform better LSA similarity, from complementary obtained RNNLMs, weighted leads 58.9% (59.2% development 58.7% set). 5 Examples Learned Relationships Table 8 shows follow various relationships. We follow approach described above: relationship dened subtracting two vectors, added another word. Thus example, Paris - France + Italy = Rome . As it can be seen, quite good, although there clearly lot room further improvements (note using our metric 9