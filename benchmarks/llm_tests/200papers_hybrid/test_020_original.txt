@ x k (equation (5)) transport the error \in time\ from step t back to step k . We would further loosely distin- guish between long term and short term contributions, where long term refers to components for which k  t and short term to everything else. 2. Exploding and Vanishing Gradients As introduced in Bengio et al. (1994), the exploding gradients problem refers to the large increase in the norm of the gradient during training. Such events are caused by the explosion of the long term components, which can grow exponentially more then short term ones. The vanishing gradients problem refers to the opposite behaviour, when long term components go exponentially fast to norm 0, making it impossible for the model to learn correlation between temporally dis- tant events. 2.1. The mechanics To understand this phenomenon we need to look at the form of each temporal component, and in particular at the matrix factors @ x t