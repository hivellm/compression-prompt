to affect the class score the most. One can expect that such pixels correspond to the object location in the image. We note that a similar technique has been previously applied by [ 1 ] in the context of Bayesian classication. 3.1  Class Saliency Extraction Given an image I 0 (with m rows and n columns) and a class c , the class saliency map M 2 R m  n is computed as follows. First, the derivative w ( 4 ) is found by back-propagation. After that, the saliency map is obtained by rearranging the elements of the vector w . In the case of a grey-scale image, the number of elements in w is equal to the number of pixels in I 0 , so the map can be computed as M ij = j w h ( i;j ) j , where h ( i; j ) is the index of the element of w , corresponding to the image pixel in the i -th row and j -th column. In the case of the multi-channel ( e.g. RGB) image, let us assume that the colour channel c of the pixel ( i; j ) of image I corresponds to the element of w with the index h ( i; j; c ) . To derive a single class saliency value for each pixel ( i; j ) , we took the maximum magnitude of w across all colour channels: M ij = max c j w h ( i;j;c ) j . It is important to note that the saliency maps are extracted using a classication ConvNet trained on the image labels, so no additional annotation is required (such as object bounding boxes or segmentation masks). The computation of the image-specic saliency map for a single class is extremely quick, since it only requires a single back-propagation pass. We visualise the saliency maps for the highest-scoring class (top-1 class prediction) on randomly se- lected ILSVRC-2013 test set images in Fig.  2 . Similarly to the ConvNet classication procedure [ 8 ], where the class predictions are computed on 10 cropped and reected sub-images, we computed 10 saliency maps on the 10 sub-images, and then averaged them. 3.2  Weakly Supervised Object Localisation The weakly supervised class saliency maps (Sect.  3.1 ) encode the location of the object of the given class in the given image, and thus can be used for object localisation (in spite of being trained on image labels only). Here we briey describe a simple object localisation procedure, which we used for the localisation task of the ILSVRC-2013 challenge [ 12 ]. Given an image and the corresponding class saliency map, we compute the object segmentation mask using the GraphCut colour segmentation [ 3 ]. The use of the colour segmentation is motivated by the fact that the saliency map might capture only the most discriminative part of an object, so saliency thresholding might not be able to highlight the whole object. Therefore, it is important to be able to propagate the thresholded map to other parts of the object, which we aim to achieve here using the colour continuity cues. Foreground and background colour models were set to be the Gaussian Mixture Models. The foreground model was estimated from the pixels with the saliency higher than a threshold, set to the 95% quantile of the saliency distribution in the image; the background model was estimated from the pixels with the saliency smaller than the 30% quantile (Fig.  3 , right-middle). The GraphCut segmentation [ 3 ] was then performed using the publicly available implementation 2 . Once the image pixel labelling into foreground and background is computed, the object segmentation mask is set to the largest connected component of the foreground pixels (Fig.  3 , right). We entered our object localisation method into the ILSVRC-2013 localisation challenge. Consid- ering that the challenge requires the object bounding boxes to be reported, we computed them as the bounding boxes of the object segmentation masks. The procedure was repeated for each of the top-5 predicted classes. The method achieved 46 : 4% top-5 error on the test set of ILSVRC-2013. It should be noted that the method is weakly supervised (unlike the challenge winner with 29 : 9% error), and the object localisation task was not taken into account during training. In spite of its simplicity, the method still outperformed our submission to ILSVRC-2012 challenge (which used the same dataset), which achieved 50 : 0% localisation error using a fully-supervised algorithm based on the part-based models [ 6 ] and Fisher vector feature encoding [ 11 ]. 4  Relation to Deconvolutional Networks In this section we establish the connection between the gradient-based visualisation and the DeconvNet architecture of [ 13 ]. As we show below, DeconvNet-based reconstruction of the n -th layer input X n is either equivalent or similar to computing the gradient of the visualised neuron ac-