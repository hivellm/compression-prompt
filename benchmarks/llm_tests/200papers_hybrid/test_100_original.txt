A M ODEL A RCHITECTURE A.1 A RCHITECTURAL C HOICES The proposed scheme in Section 3 is a general framework where one can freely dene, for instance, the activation functions f of recurrent neural networks (RNN) and the alignment model a . Here, we describe the choices we made for the experiments in this paper. A.1.1 R ECURRENT N EURAL N ETWORK For the activation function f of an RNN, we use the gated hidden unit recently proposed by Cho et al. (2014a). The gated hidden unit is an alternative to the conventional simple units such as an element-wise tanh . This gated unit is similar to a long short-term memory (LSTM) unit proposed earlier by Hochreiter and Schmidhuber (1997), sharing with it the ability to better model and learn long-term dependencies. This is made possible by having computation paths in the unfolded RNN for which the product of derivatives is close to 1. These paths allow gradients to ow backward easily without suffering too much from the vanishing effect (Hochreiter, 1991; Bengio et al. , 1994; Pascanu et al. , 2013a). It is therefore possible to use LSTM units instead of the gated hidden unit described here, as was done in a similar context by Sutskever et al. (2014). The new state s i of the RNN employing n gated hidden units 8 is computed by s i = f ( s i  1 ; y i  1 ; c i ) = (1  z i )  s i  1 + z i  ~ s i ; where  is an element-wise multiplication, and z i is the output of the update gates (see below). The proposed updated state ~ s i is computed by ~ s i = tanh ( W e ( y i  1 ) + U [ r i  s i  1 ] + Cc i ) ; where e ( y i  1 ) 2 R m is an m -dimensional embedding of a word y i  1 , and r i is the output of the reset gates (see below). When y i is represented as a 1 -of- K vector, e ( y i ) is simply a column of an embedding matrix E 2 R m  K . Whenever possible, we omit bias terms to make the equations less cluttered. The update gates z i allow each hidden unit to maintain its previous activation, and the reset gates r i control how much and what information from the previous state should be reset. We compute them by z i =  ( W z e ( y i  1 ) + U z s i  1 + C z c i ) ; r i =  ( W r e ( y i  1 ) + U r s i  1 + C r c i ) ; where  (  ) is a logistic sigmoid function. At each step of the decoder, we compute the output probability (Eq. (4)) as a multi-layered func- tion (Pascanu et al. , 2014). We use a single hidden layer of maxout units (Goodfellow et al. , 2013) and normalize the output probabilities (one for each word) with a softmax function (see Eq. (6)). A.1.2 A LIGNMENT M ODEL The alignment model should be designed considering that the model needs to be evaluated T x  T y times for each sentence pair of lengths T x and T y . In order to reduce computation, we use a single- layer multilayer perceptron such that a ( s i  1 ; h j ) = v > a tanh ( W a s i  1 + U a h j ) ; where W a 2 R n  n ; U a 2 R n  2 n and v a 2 R n are the weight matrices. Since U a h j does not depend on i , we can pre-compute it in advance to minimize the computational cost.