Any differentiable function is theoretically permitted Table 2: Challenges in generative modeling: a summary of the difculties encountered by different approaches to deep generative modeling for each of the major operations involving a model. 6 Advantages and disadvantages This new framework comes with advantages and disadvantages relative to previous modeling frame- works. The disadvantages are primarily that there is no explicit representation of p g ( x ) , and that D must be synchronized well with G during training (in particular, G must not be trained too much without updating D , in order to avoid the Helvetica scenario in which G collapses too many values of z to the same value of x to have enough diversity to model p data ), much as the negative chains of a Boltzmann machine must be kept up to date between learning steps. The advantages are that Markov chains are never needed, only backprop is used to obtain gradients, no inference is needed during learning, and a wide variety of functions can be incorporated into the model. Table 2 summarizes the comparison of generative adversarial nets with other generative modeling approaches. The aforementioned advantages are primarily computational. Adversarial models may also gain some statistical advantage from the generator network not being updated directly with data exam- ples, but only with gradients owing through the discriminator. This means that components of the input are not copied directly into the generatorâ€™s parameters. Another advantage of adversarial net- works is that they can represent very sharp, even degenerate distributions, while methods based on Markov chains require that the distribution be somewhat blurry in order for the chains to be able to mix between modes. 7 Conclusions and future work This framework admits many straightforward extensions: 1.  A conditional generative model p ( x j c ) can be obtained by adding c as input to both G and D . 2. Learned approximate inference can be performed by training an auxiliary network to predict z given x . This is similar to the inference net trained by the wake-sleep algorithm [15] but with the advantage that the inference net may be trained for a xed generator net after the generator net has nished training. 7