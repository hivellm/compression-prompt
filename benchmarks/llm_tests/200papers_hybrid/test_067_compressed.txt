cuda-convnet toolbox 1 although our net less wide, we used additional jittering, based on zeroing-out random parts image. Our weight conguration is: conv64-conv256- conv256-conv256-conv256-full4096-full4096-full1000, where convN denotes convolutional N lters, fullM fully-connected M outputs. On ILSVRC-2013 validation set, network achieves top-1/top-5 error 39 = 17 which slightly better than 40 / 18 2% reported [ 8 ] single ConvNet. Class Model Visualisation In this section describe technique visualising models, clas- sication ConvNets. Given interest, visualisation method consists numerically generating [ 5 ], which representative terms scoring model. More formally, let ( ) computed . We would like nd L -regularised such high: arg max ; (1) regularisation parameter. A locally-optimal can back-propagation method. procedure related procedure, back-propagation optimise weights. difference case optimisation performed respect input while weights xed those during stage. We initialised optimisation zero (in case, was trained zero-centred data), then added set mean result. model visualisations several classes shown Fig. 1 . It should noted that we used (unnormalised) scores S rather than posteriors, returned soft-max layer: P = exp S