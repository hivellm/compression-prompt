perplexity ( V )) . For example when the vocabulary size is one million words, this results in about two times speedup in evaluation. While this is not crucial speedup for neural network LMs as the computational bottleneck is in the N  D  H term, we will later propose architectures that do not have hidden layers and thus depend heavily on the efciency of the softmax normalization. 2.2 Recurrent Neural Net Language Model (RNNLM) Recurrent neural network based language model has been proposed to overcome certain limitations of the feedforward NNLM, such as the need to specify the context length (the order of the model N ), and because theoretically RNNs can efciently represent more complex patterns than the shallow neural networks [15, 2]. The RNN model does not have a projection layer; only input, hidden and output layer. What is special for this type of model is the recurrent matrix that connects hidden layer to itself, using time-delayed connections. This allows the recurrent model to form some kind of short term memory, as information from the past can be represented by the hidden layer state that gets updated based on the current input and the state of the hidden layer in the previous time step. The complexity per training example of the RNN model is Q = H  H + H  V; (3) where the word representations D have the same dimensionality as the hidden layer H . Again, the term H  V can be efciently reduced to H  log 2 ( V ) by using hierarchical softmax. Most of the complexity then comes from H  H . 3