Figure 1: New model architectures. The CBOW architecture predicts current based on context, Skip-gram predicts surrounding given current word. R from future current correct labels. This will require us do R 2 classications, input, each R + R output. In following experiments, C = 10 4 Results To compare quality different versions vectors, previous papers typically table showing example their most understand them intuitively. Although easy show France Italy perhaps some other countries, much more challenging when subjecting those more complex similarity task, follows. We follow previous observation there many different types similarities between example, bigger smaller Example another type relationship pairs - - smallest [20]. We further denote two pairs relationship question, ask: What ? Somewhat surprisingly, these questions answered performing simple algebraic operations representation words. To nd , simply compute X = + Then, search space closest X measured cosine distance, (we discard input during this search). When are well trained, possible nd correct (word smallest ) using this method. Finally, found when train high dimensional on large amount data, resulting used very subtle semantic relationships between city country belongs to, e.g. France Paris Germany Berlin. Word such semantic relationships could used improve many existing NLP applications, such machine translation, information retrieval question answering systems, may enable other future applications yet invented. 5