EP 41 57 4 02 16 05 43 45 3 67 Figure 2: Percentage error s.d.) dierent columns evaluating Eqn. (4) rows results indicate very accurate approximation; EP causes some loss signicantly more, which line comparison presented Kuss Rasmussen, 2005 ]. For our experiments use EP. yield pathological behaviour (we investigate experimentally). These approaches computationally expensive, requiring O y updates. Also, must know locations (and thus transductive approaches); designing inductive, decision-theoretic algorithm open, hard it would require expensive integration over distributions. Non-probabilistic Some non-probabilistic have close analogues information theoretic learning. Perhaps ubiquitous learning SVMs Tong Koller, , Seung ], where Version Space (VS) proxy entropy. If uniform (improper) prior classication likelihood, log fact equivalent. Just posteriors intractable after observing many points, complicated. Tong Koller, proposes approximat- ing simple shapes, such hyperspheres (their simplest reduces margin sampling). This closely resembles approximating using distribution via approximations. sidesteps working predictions. al- gorithm, Query Committee (QBC), samples parameters (committee members), outcome each . balanced selected; termed ‘principle maximal disagreement’. BALD sampled posterior, query committee implemented but probabilistic measure disagreement. QBC’s criterion discards condence predictions so exhibit same pathologies MES. 5 Experiments Quantifying Approximation Losses: To obtain (5) made two approx- imations: perform approximate inference ), approximated binary entropy Gaussian CDF squared exponential Both these be substituted Monte Carlo sampling, enabling us compute 9