A M ODEL A RCHITECTURE A.1 A RCHITECTURAL C HOICES proposed scheme Section 3 general framework where one can freely dene, instance, activation functions f recurrent neural networks (RNN) alignment model . Here, describe choices made experiments this paper. A.1.1 R ECURRENT N EURAL N ETWORK For activation function f RNN, use gated recently proposed Cho (2014a). gated alternative conventional simple units such element-wise tanh . This gated similar long short-term memory (LSTM) proposed earlier Hochreiter Schmidhuber (1997), sharing with it ability better model learn long-term dependencies. This made possible having computation paths unfolded RNN which product derivatives close 1. These paths allow gradients ow backward easily without suffering too much from vanishing effect (Hochreiter, 1991; Bengio 1994; Pascanu 2013a). It therefore possible use LSTM units instead described here, was done similar context Sutskever (2014). new state RNN employing units 8 computed f c (1 ~ element-wise multiplication, update (see below). proposed updated state ~ computed ~ tanh W U [ ] Cc R m m -dimensional embedding word reset (see below). When represented -of- K vector, simply column embedding matrix E R m K Whenever possible, omit bias terms make equations less cluttered. update allow maintain its previous activation, reset control how much what information from previous state should be reset. We compute them W U C c W U C c logistic sigmoid function. At step decoder, compute probability (Eq. (4)) multi-layered func- tion (Pascanu 2014). We single layer maxout (Goodfellow 2013) normalize probabilities (one word) with softmax function (see Eq. (6)). A.1.2 A LIGNMENT M ODEL alignment model should be designed considering that model needs be evaluated T x T times each sentence pair lengths T x T . In order reduce computation, use single- layer multilayer perceptron such that h j = v > tanh W U h j where W R U R v R n are weight matrices. Since U h j does not depend on , can pre-compute it advance minimize computational cost.