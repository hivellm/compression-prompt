The most important distinguishing feature this from basic encoderdecoder it does not attempt encode whole input single xed-length vector. Instead, it en- codes input sequence vectors chooses subset these vectors adaptively while decoding translation. This frees model from having squash all information sentence, regardless its length, xed-length vector. We show allows cope better long sentences. In paper, show jointly learning align translate achieves signicantly improved over basic encoderdecoder approach. The im- provement more apparent longer sentences, but can be observed sentences any length. On task English-to-French translation, achieves, single model, comparable, close, conventional phrase-based system. Furthermore, qualitative analysis reveals nds linguistically plausible (soft-)alignment between corresponding sentence. B ACKGROUND : N EURAL M ACHINE RANSLATION From probabilistic perspective, equivalent nding max- imizes probability given i.e., arg max p j translation, parameterized maximize probability pairs using parallel training corpus. Once distribution learned model, given corresponding can generated searching maximizes probability. Recently, number papers have use networks directly learn condi- tional distribution (see, e.g., Kalchbrenner Blunsom, 2013; Cho 2014a; 2014; Cho 2014b; Forcada Neco, 1997). This typ- ically consists two components, rst which encodes second decodes For instance, two recurrent networks (RNN) were used (Cho 2014a) (Sutskever 2014) encode variable-length decode variable-length sentence. Despite being quite new approach, has already shown promising results. Sutskever reported based RNNs long short- term memory (LSTM) units achieves close state-of-the-art conventional phrase-based system English-to-French task. Adding components existing systems, instance, score phrase pairs in phrase table (Cho 2014a) re-rank candidate translations (Sutskever 2014), has allowed surpass previous state-of-the-art level. 2.1 RNN E NCODER D ECODER Here, describe briey underlying framework, called RNN EncoderDecoder Cho (2014a) Sutskever upon which build novel architecture learns align translate simultaneously. In EncoderDecoder framework, encoder reads input sentence, sequence vectors T c . 2 The most common use RNN such (1) c q T g where 2 R n hidden state at time c vector generated from sequence hidden states. q are some nonlinear functions. Sutskever (2014) used LSTM as q T g T for instance.