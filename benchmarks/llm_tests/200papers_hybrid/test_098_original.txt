A CKNOWLEDGMENTS The authors would like to thank the developers of Theano (Bergstra et al. , 2010; Bastien et al. , 2012). We acknowledge the support of the following agencies for research funding and computing support: NSERC, Calcul Qu · ebec, Compute Canada, the Canada Research Chairs and CIFAR. Bah- danau thanks the support from Planet Intelligent Systems GmbH. We also thank Felix Hill, Bart van Merri · enboer, Jean Pouget-Abadie, Coline Devin and Tae-Ho Kim. R EFERENCES Axelrod, A., He, X., and Gao, J. (2011). Domain adaptation via pseudo in-domain data selection. In Proceedings of the ACL Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 355362. Association for Computational Linguistics. Bastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I. J., Bergeron, A., Bouchard, N., and Bengio, Y. (2012). Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop. Bengio, Y., Simard, P., and Frasconi, P. (1994). Learning long-term dependencies with gradient descent is difcult. IEEE Transactions on Neural Networks , 5 (2), 157166. Bengio, Y., Ducharme, R., Vincent, P., and Janvin, C. (2003). A neural probabilistic language model. J. Mach. Learn. Res. , 3 , 11371155. Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde- Farley, D., and Bengio, Y. (2010). Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientic Computing Conference (SciPy) . Oral Presentation. Boulanger-Lewandowski, N., Bengio, Y., and Vincent, P. (2013). Audio chord recognition with recurrent neural networks. In ISMIR . Cho, K., van Merrienboer, B., Gulcehre, C., Bougares, F., Schwenk, H., and Bengio, Y. (2014a). Learning phrase representations using RNN encoder-decoder for statistical machine translation. In Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014) . to appear. Cho, K., van Merri ¤ enboer, B., Bahdanau, D., and Bengio, Y. (2014b). On the properties of neural machine translation: EncoderDecoder approaches. In Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation . to appear. Devlin, J., Zbib, R., Huang, Z., Lamar, T., Schwartz, R., and Makhoul, J. (2014). Fast and robust neural network joint models for statistical machine translation. In Association for Computational Linguistics . Forcada, M. L. and  Neco, R. P. (1997). Recursive hetero-associative memories for translation. In J. Mira, R. Moreno-D · az, and J. Cabestany, editors, Biological and Articial Computation: From Neuroscience to Technology , volume 1240 of Lecture Notes in Computer Science , pages 453462. Springer Berlin Heidelberg. Goodfellow, I., Warde-Farley, D., Mirza, M., Courville, A., and Bengio, Y. (2013). Maxout net- works. In Proceedings of The 30th International Conference on Machine Learning , pages 1319 1327. Graves, A. (2012). Sequence transduction with recurrent neural networks. In Proceedings of the 29th International Conference on Machine Learning (ICML 2012) . Graves, A. (2013). Generating sequences with recurrent neural networks. arXiv: 1308.0850 [cs.NE] . Graves, A., Jaitly, N., and Mohamed, A.-R. (2013). Hybrid speech recognition with deep bidirec- tional LSTM. In Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Work- shop on , pages 273278. 10