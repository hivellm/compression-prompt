[RULES] - Always expand ⟦n⟧ using DICT[n] before reasoning. - Do not print DICT result, only use it understand context. [DICT] 1: s 2: [BODY] maximising expected value + ⟦1⟧ , s h + max ⟦1⟧ (1) The basic idea behind many algorithms estimate action- function, using Bellman equation an iterative update, +1 [ + max ⟦1⟧ j ] Such iteration algorithms converge optimal action- function, ! ! [23]. In practice, this basic approach totally impractical, because action-value estimated separately each sequence, without any generali- sation. Instead, common use approximator estimate action-value function, In community typically linear approximator, but sometimes non-linear approximator used instead, such neural network. We refer neural network approximator weights Q-network. A Q-network can be trained minimising sequence functions L changes each iteration L s;a h y )) 2 (2) where y [ j ] target iteration probability over sequences actions we refer behaviour The parameters previous iteration held xed when optimising L Note targets depend network weights; contrast targets used supervised learning, which xed before begins. Differentiating respect weights we arrive following gradient, L s;a ); h : (3) Rather than computing full expectations above gradient, often computationally expe- dient optimise stochastic gradient descent. If weights updated after every time-step, expectations replaced single samples behaviour emulator respectively, then we arrive familiar Q-learning algorithm [26]. Note algorithm model-free : solves task directly sam- ples emulator without explicitly constructing estimate It also off-policy : learns about greedy strategy while following behaviour ensures adequate exploration state space. In practice, behaviour often se- lected -greedy strategy follows greedy strategy probability selects random action probability 3 Related Work Perhaps best-known success story TD-gammon backgammon- playing program which learnt entirely self-play, achieved super- human level play [24]. TD-gammon used model-free algorithm similar Q-learning, approximated using multi-layer perceptron one hidden layer However, early attempts follow up on TD-gammon, including applications same method chess, Go checkers were less successful. This led widespread belief TD-gammon approach was special case only worked backgammon, perhaps because stochasticity dice rolls helps explore state space also makes value particularly smooth [19]. Furthermore, it was shown combining model-free algorithms such Q- non-linear approximators [25], or indeed off-policy [1] could cause Q-network diverge. Subsequently, majority work fo- cused on linear approximators better convergence guarantees [25].