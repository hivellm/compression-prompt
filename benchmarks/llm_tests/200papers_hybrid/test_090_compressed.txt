alignment not considered latent variable. Instead, alignment model directly com- putes soft alignment, which allows gradient cost function backpropagated through. This gradient can used train alignment model well whole translation model jointly. We can understand approach taking weighted sum all annotations computing expected where expectation over possible alignments. Let probability that target y aligned to, or translated from, source Then, -th context vector c expected over all annotations with probabilities probability or its associated energy e reects importance respect previous s deciding next s generating y Intuitively, implements mechanism attention decoder. decides parts sentence pay to. By letting have mechanism, relieve encoder burden having encode information sentence into xed- length vector. With new approach information spread throughout annotations, selectively retrieved accordingly. 3.2 E NCODER : B IDIRECTIONAL RNN FOR A NNOTATING S EQUENCES usual RNN, described Eq. (1), input order starting rst symbol last one However, scheme, would like each summarize only preceding words, but also following Hence, propose bidirectional RNN (BiRNN, Schuster Paliwal, 1997), has been successfully recently speech recognition (see, e.g., Graves 2013). A BiRNN consists RNN’s. RNN f input it ordered (from calculates states ( RNN f reverse order (from ), resulting states ( obtain each concatenating one i.e., = In way, contains summaries both preceding Due tendency RNNs better represent recent inputs, will focused on around This annotations later compute context vector (Eqs. (5)(6)). See Fig. graphical illustration model. 4 E XPERIMENT S ETTINGS evaluate task English-to-French translation. bilin- gual, parallel corpora provided ACL WMT ’14. 3 As comparison, also report perfor- mance RNN EncoderDecoder was proposed recently Cho (2014a). same training procedures same dataset both models. 4 4.1 D ATASET WMT ’14 contains following English-French parallel corpora: Europarl (61M words), news commentary (5.5M), UN (421M) two crawled corpora 90M 272.5M words respectively, totaling 850M words. Following procedure described Cho et al. (2014a), reduce size combined corpus have 348M words using data selection method Axelrod et al. (2011). 5 We do not any monolingual data other than mentioned parallel corpora, although it may possible much larger monolingual corpus pretrain encoder. We concatenate news-test-