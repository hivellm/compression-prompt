losses encountered after making decisions based collected i.e. min- imize Bayes risk Roy McCallum, 2001 ]. Maximising perfor- mance under ultimate objective most learners, however, evaluat- ing objective can very hard. For example, methods proposed Kapoor 2007 Zhu 2003 ] classication general expensive compute. Furthermore, one not know loss function advance, want model perform well variety loss functions. extreme scenarios, exploratory analysis, visualisation, hard quantify. This motivates learning, agnostic decision task at hand data, known inductive approach. They seek reduce number feasible quickly possible, either heuristics (e.g. margin sampling Tong Koller, ]) by formalising uncertainty studied quantities, Shannons entropy KL-divergence Cover 1991 Although latter was several decades ago Lindley, 1956 Bernardo, 1979 ], always straightforward criteria complicated nonparametric processes spaces. As result many algorithms exist compute approximate entropies, sampling, work related quantities non-probabilistic models. return problem, presenting full criterion demonstrate Gaussian Processes Classication (GPC), yielding novel algorithm makes minimal approximations. GPC powerful, non-parametric kernel-based model, poses interesting problem information-theoretic because space dimensional analytically intractable. 2. 3 GPC, show extended method preference learning. 4 review other they compare algorithm. take care contrast Informative Vector Machine, addresses point selection GPs directly. results wide datasets 5 conclude 6. Bayesian Information Theoretic Active Learn- consider fully discriminative where goal discover variable Y input variable X . The key idea learner chooses queries X observes systemâ€™s response rather than passively receiving pairs. Within Bayesian framework assume existence some latent param- eters, control dependence between inputs outputs, p j ; ). Having observed D = f ; ) g n =1 over pa-