region space. It has been shown practice it can reduce chance gradients explode, even allow training generator models or models work unbounded amounts memory(Pascanu Jaeger, 2011; Doya Yoshizawa, 1991). One important downside it requires target dened at every time step. In Hochreiter Schmidhuber (1997); Graves et al. (2009) solution proposed vanishing gra- dients problem, where structure model changed. Specically introduces special set units called LSTM units which linear have recurrent connection itself which xed 1. The ow information into unit from unit guarded input output gates (their behaviour learned). There several variations basic structure. This solution does not address explicitly exploding gradients problem. Sutskever et al. (2011) use Hessian-Free opti- mizer conjunction structural , spe- cic strategy Hessian. This approach seems deal very well vanishing gradient, though more detailed analysis still missing. Pre- sumably method works because high dimen- sional spaces there high probability long term components orthogonal short term ones. This would allow Hessian rescale these components independently. In practice, one can not guarantee property holds. As discussed section 2.3, method able deal exploding gradient as well. Structural damping an enhancement forces change state small, when pa- rameter changes by some small value . This asks Jacobian matrices @ x t