This algorithm is very similar to the one proposed by Tomas Mikolov and we only diverged from the original proposal in an attempt to provide a better theoretical foundation (ensuring that we always move in a de- scent direction with respect to the current mini-batch), though in practice both variants behave similarly. The proposed clipping is simple to implement and computationally ecient, but it does however in- troduce an additional hyper-parameter, namely the threshold. One good heuristic for setting this thresh- old is to look at statistics on the average norm over a suciently large number of updates. In our ex- periments we have noticed that for a given task and model size, training is not very sensitive to this hyper- parameter and the algorithm behaves well even for rather small thresholds. The algorithm can also be thought of as adapting the learning rate based on the norm of the gradient. Compared to other learning rate adaptation strate- gies, which focus on improving convergence by col- lecting statistics on the gradient (as for example in