than few hundred millions words, with modest dimensionality vectors between 50 - 100. We use recently proposed techniques measuring quality resulting vector representa- tions, with expectation not only will similar words tend be close each other, but words can have multiple degrees similarity [20]. This has been observed earlier in context inectional languages - example, nouns can have multiple endings, if we search similar words in subspace original vector space, it is possible nd words have similar endings [13, 14]. Somewhat surprisingly, it found similarity representations goes beyond simple syntactic regularities. Using offset technique where simple algebraic operations per- formed vectors, it shown example vector(King) - vector(Man) + vec- tor(Woman) results vector is closest vector representation Queen [20]. In this paper, try maximize accuracy these vector operations by developing new architectures preserve linear regularities among words. We design new comprehensive test set measuring both syntactic semantic regularities 1 , show many such regularities can be learned high accuracy. Moreover, discuss how training time accuracy depends dimensionality amount training data. 1.2 Previous Work Representation as continuous has long history [10, 26, 8]. A very popular architecture estimating neural network language (NNLM) proposed [1], where feedforward neural network linear projection layer non-linear hidden layer used learn jointly vector representation statistical language model. This work has been followed by many others. Another interesting architecture NNLM presented [13, 14], where rst learned using neural network single hidden layer. The then used train NNLM. Thus, learned even without constructing full NNLM. In this work, directly extend this architecture, focus just rst step where learned using simple model. It later shown can be used signicantly improve simplify many NLP applications [4, 5, 29]. Estimation itself performed using different architectures trained various corpora [4, 29, 23, 19, 9], some resulting were made available future research comparison 2 . However, far know, these architectures were signicantly more computationally expensive training than one proposed [13], exception certain version log-bilinear where diagonal weight matrices used [23]. 2 Model Architectures Many different types models were proposed estimating continuous representations words, including well-known Latent Semantic Analysis (LSA) Latent Dirichlet Allocation (LDA). In this paper, we focus distributed representations words learned by neural networks, as it previously shown they perform signicantly better than LSA preserving linear regularities among words [20, 31]; LDA moreover becomes computationally very expensive on large data sets. Similar [18], compare different model architectures we dene rst computational complex- ity model as number parameters need be accessed fully train model. Next, we will try maximize accuracy, while minimizing computational complexity.