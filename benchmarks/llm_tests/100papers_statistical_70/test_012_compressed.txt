QBC100 Kapoor Zhu et al. Empirical Figure 5: Summary results for all classication experiments. y -axis denotes number additional data points, relative BALD, required achieve at least 97 : 5% predictive performance entire pool. ‘box’ denotes 25th 75th percentile, red line denotes median over datasets, ‘whiskers’ depict range. crosses denote outliers ( > 2 : 7 mean). Positive values mean that algorithm required more data than BALD achieve same performance. expected empirical error (the last not widely used method, but included analysis [Kapoor al., 2007]). We consider three articial, but challenging, datasets. rst which, middle , has noisy decision boundary, second corner , has uninformative far decision boundary: strong active algorithm should avoid these uninformative regions. third similar checkerboard dataset [ Zhu al., 2003 ], designed test algorithm’s capabilities nd multiple disjoint islands one class. three results using each depicted Fig. 3. Results also presented on eight UCI australia, crabs, vehicle, isolet, cancer, wine, wdbc letter . Letter multiclass dataset which we select hard-to-distinguish letters E vs. F D vs. P. For we use cpu, cart kinematics regression 1 processed yield preference task as described [ Chu Ghahramani, 2005 ]. Results plotted Fig. 4, Fig. 5 depicts an aggregation results. Discussion: Figs. 3 4 show that by using BALDwe make signicant gains over naive random sampling both classication preference learning domains. Relative other active learning algorithms BALDis consistently 1 http://www.liacc.up.pt/ ltorgo/Regression/DataSets.html 12