losses encountered after making decisions based data collected i.e. min- imize Bayes posterior risk Roy McCallum, 2001 ]. Maximising perfor- mance under test ultimate objective most learners, however, evaluat- ing this objective can be very hard. For example, methods proposed Kapoor et al., 2007 Zhu et al., 2003 ] for classication are general expensive compute. Furthermore, one may not know loss function test distribution advance, may want model perform well variety loss functions. In extreme scenarios, such exploratory data analysis, visualisation, losses may be very hard quantify. This motivates information theoretic approaches learning, which are agnostic decision task at hand particular test data, this known inductive approach. They seek reduce number feasible models quickly possible, using either heuristics (e.g. margin sampling Tong Koller, 2001 ]) by formalising uncertainty using well studied quantities, such Shannons entropy KL-divergence Cover et al., 1991 ]. Although latter approach was proposed several decades ago Lindley, 1956 Bernardo, 1979 ], it not always straightforward apply criteria complicated models such nonparametric processes with innite parameter spaces. As result many algorithms exist which compute approximate entropies, perform sampling, work with related quantities non-probabilistic models. We return this problem, presenting full information criterion demonstrate how apply it Gaussian Processes Classication (GPC), yielding novel algorithm makes minimal approximations. GPC powerful, non-parametric kernel-based model, poses interesting problem information-theoretic because parameter space innite dimensional distribution analytically intractable. We present information theoretic approach Section 2. Section 3 we apply it GPC, show how extended our method preference learning. Section 4 we review other approaches how they compare our algorithm. We take particular care contrast our approach Informative Vector Machine, addresses point selection GPs directly. We present results wide variety datasets Section 5 conclude Section 6. Bayesian Information Theoretic Active Learn- ing We consider fully discriminative model where goal discover dependence some variable Y input variable X . The key idea learner chooses input queries X observes systemâ€™s response rather than passively receiving ( ) pairs. Within Bayesian framework we assume existence some latent param- eters, that control dependence between inputs outputs, p ( j ; ). Having observed data D = f ( ; ) g n =1 posterior distribution over pa-