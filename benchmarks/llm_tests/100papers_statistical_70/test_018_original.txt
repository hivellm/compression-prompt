Figure 1. Schematic of a recurrent neural network. The recurrent connections in the hidden layer allow information to persist from one input to another. and exploding gradient problems described in Bengio et al. (1994). 1.1. Training recurrent networks A generic recurrent neural network, with input u t and state x t for time step t , is given by equation (1). In the theoretical section of this paper we will sometimes make use of the specic parametrization given by equa- tion (11) 1 in order to provide more precise conditions and intuitions about the everyday use-case. x t = F ( x t  1 ; u t ;  ) (1) x t = W rec  ( x t  1 ) + W in u t + b (2) The parameters of the model are given by the recurrent weight matrix W rec , the biases b and input weight matrix W in , collected in  for the general case. x 0 is provided by the user, set to zero or learned, and  is an element-wise function (usually the tanh or sigmoid ). A cost E measures the performance of the network on some given task and it can be broken apart into individual costs for each step E = P 1  t  T E t , where E t = L ( x t ). One approach that can be used to compute the nec- essary gradients is Backpropagation Through Time (BPTT), where the recurrent model is represented as