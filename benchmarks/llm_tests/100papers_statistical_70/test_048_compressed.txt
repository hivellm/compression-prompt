Figure 1: New model architectures. The CBOW architecture predicts current based on context, Skip-gram predicts surrounding words given current word. R words from future current correct labels. This will require us do R 2 classications, with current input, each R + R words output. In following experiments, we use C = 10 . 4 Results To compare quality different versions vectors, previous papers typically use table showing example words their most words, understand them intuitively. Although it easy show France Italy perhaps some other countries, it much more challenging when subjecting those vectors more complex similarity task, follows. We follow previous observation that there can many different types similarities between words, for example, big bigger same sense that small smaller . Example another type relationship pairs big - biggest small - smallest [20]. We further denote two pairs words with same relationship question, ask: What small same sense biggest big ? Somewhat surprisingly, these questions can answered by performing simple algebraic operations with vector representation words. To nd that small same sense biggest big , we can simply compute X = (" biggest ") (" big ") + (" small ") . Then, we search space for closest X measured by cosine distance, use it answer question (we discard input question words during this search). When vectors are well trained, it possible nd correct answer (word smallest ) using this method. Finally, we found that when we train high dimensional vectors on a large amount data, resulting vectors can be used answer very subtle semantic relationships between words, such a city country it belongs to, e.g. France Paris Germany Berlin. Word vectors with such semantic relationships could be used improve many existing NLP applications, such machine translation, information retrieval question answering systems, may enable other future applications yet be invented. 5