4.1 Task Description To measure quality vectors, we dene comprehensive test set that contains ve types semantic questions, nine types syntactic questions. Two examples from each category shown Table 1. Overall, there 8869 semantic 10675 syntactic questions. The questions each category were created two steps: rst, list similar pairs was created manually. Then, large list questions formed by connecting two pairs. For example, made list 68 large American cities states they belong to, formed 2.5K questions by picking two pairs at random. We included our test set only single token words, thus multi-word entities not present (such New York ). We evaluate overall accuracy all question types, each question type separately (se- mantic, syntactic). Question assumed correctly answered only if closest vector computed using above method exactly same correct question; synonyms thus counted mistakes. This also means reaching 100% accuracy likely impossible, current models do not any input information morphology. However, believe usefulness vectors certain applications should positively correlated this accuracy metric. Further progress can achieved incorporating information structure words, especially syntactic questions. 4.2 Maximization Accuracy We used Google News corpus vectors. This corpus contains 6B tokens. We restricted vocabulary size 1 million most frequent words. Clearly, facing time constrained optimization problem, it can expected both using more higher dimensional vectors will improve accuracy. To estimate best choice model architecture obtaining good possible results quickly, rst evaluated models trained on subsets data, vocabulary restricted most frequent 30k words. The results using CBOW architecture different choice vector dimensionality increasing amount training data shown Table 2. It can seen after some point, adding more dimensions or adding more training data provides diminishing improvements. So, increase both vector dimensionality amount training data together. While this observation might seem trivial, it must noted that it currently popular train vectors on relatively large amounts data, but with insufcient size 6