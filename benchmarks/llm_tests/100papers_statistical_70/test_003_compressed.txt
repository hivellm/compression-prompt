rameters inferred, p ). The central goal of information theoretic ac- tive learning reduce number possible hypotheses maximally fast, i.e. minimize uncertainty about parameters using Shannonâ€™s entropy [ Cover et al., 1991 ]. Data points D 0 selected that satisfy arg min D 0 H[ 0 ] = R 0 ) log 0 )d . Solving this problem general NP-hard; however, as common sequential decision making tasks a myopic (greedy) approxi- mation made [ Heckerman et al., 1995 ]. It has been shown myopic policy can perform near-optimally Golovin Krause, 2010 , Dasgupta, 2005 ]. Therefore, seek data point maximises decrease expected posterior entropy: arg max H[ ] E ) [H[ y; ]] Note expectation over unseen output required. Many works e.g. MacKay, 1992 , Krishnapuram et Lawrence et al., 2003 ] propose using this directly. However, parameter posteriors often high dimen- sional computing their entropies usually intractable. Furthermore, nonparametric processes parameter space innite dimensional so Eqn. becomes poorly dened. To avoid gridding parameter space (exponentially hard with dimensionality), or sampling (from which it notoriously hard estimate entropies without introducing bias Panzeri Petersen, 2007 ]), these papers make Gaussian or low dimensional approximations calculate entropy approximate posterior. A second computational diculty arises; if N data points under consideration, N responses may be seen, then O N N ), potentially expensive, updates required calculate Eqn. (1). An important insight arises if we note Eqn. equivalent conditional mutual information between unknown output parameters, I[ ]. Using this insight it simple show can be rearranged compute entropies space: arg max H[ ] E ) [H[ ]] (2) Eqn. (2) overcomes challenges we described Eqn. . Entropies now calculated in, usually low dimensional, output space. For binary classication, these just entropies Bernoulli variables. Also now conditioned only on , so only O updates required. Eqn. (2) also provides us with an interesting intuition about objective; we seek which model marginally most uncertain about (high H[ D ]), but which individual settings parameters condent (low E ) [H[ ]] ). This can be interpreted as seeking which parameters under posterior disagree about outcome most, so we refer this objective as Bayesian Active Learning by Disagreement (BALD). We present a method apply Eqn. (2) directly GPC preference learning. We no longer need build our entropy calculation around type of posterior approximation (as 3