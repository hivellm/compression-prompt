t . We make observation that natural tasks it seems useful use schedule that decreases reg- ularization term. We assume that regularization forces model focus on long correla- tions at expense short ones, so it may be useful have this decaying factor in order allow make better short infor- mation. Language modelling For language modelling task used 500 sig- moidal hidden units no biases (Mikolov et al. , 2012). The is trained over sequences 200 steps, where hidden state is carried over from one step next one. We cut-o threshold 45 (though take sum cost over sequence length) all ex- periments. For next character prediction have learning rate 0.01 when using clipping no reg- ularization term, 0.05 when add 0.001 when do not clipping. When predicting 5th character in future learning rate 0.05 regularization and 0.1 without it. The regularization factor next character predic- tion was set .01 and kept constant, while modied task used an initial value 0.05 1