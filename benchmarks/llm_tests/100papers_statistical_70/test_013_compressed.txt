best, or amongst best performing algorithms all datasets. On any individ- ual dataset BALD’s performance often matched because we compare many methods, more approximate algorithms can have good performance under dierent conditions. Fig. 5 reveals BALD has best overall perfor- mance; average, all other methods require more data achieve same classication accuracy. Zhu et al. ’s decision theoretic approach closest, median increase number data required 1 : 4 zero (i.e. equivalent BALD) within inter-quartile range. This algorithm, however, requires much more computational time has access full set test inputs, which BALD does have. MES QBC appear close performance BALD, zero line falls outside both their inter-quartile ranges. As expected, MES poorly noisy dataset (Fig. 3(a)) because discards knowledge observation noise. When there zero observation noise equivalent BALD e.g. 3(c). On many real-world datasets MES well BALD e.g. 4(b, e), indicating these datasets mostly noise-free. IVM well 3(c), pathologically 3(a); this due fact biases selection towards from only one class noisy cluster, reducing posterior entropy rapidly articially. However, also signicantly worse than BALD noise-free (indicated MES’s strong performance) datasets e.g. 4(b). This implies IVM’s posterior approximation or ADF update detrimental algorithm’s performance. QBC often yields only small decrement performance, sampling approximation often too detrimental. However, poorly noisy articial dataset (Fig. 3(a)) vote criterion maintaining notion inherent uncertainty, like MES. SVM-based approach exhibits variable (it does well 4(d), very poorly 4(f)). greatly eected approximation used, for consistency we present here one yielded most consistent good performance. Decision theoretic approaches sometimes perform well, 3(c) they choose rst 16 from centre each cluster they inuenced surrounding unlabelled points. BALDdoes observe unlabelled so may pick from centres. 5 reveals BALD performing well method [ Zhu al., 2003 ], outperforms approach [ Kapoor al., 2007 ], despite having access locations test having signicantly lower computational cost. objective [ Kapoor al., 2007 ] can fail, this because one term their objective function empirical error. The weight given this term determined by relative sizes training test set (and associated losses). Directly minimizing empirical error usually performs very pathologically, picking only ‘safe’ points. When method [ Kapoor et al., 2007 ] assigns too much weight this term, it can fail also. Finally we note BALD may occasionally perform poorly rst few data (e.g. Fig. 4(l)). This may be because hyperparameters are xed throughout experiments provide a fair comparison algorithms 13