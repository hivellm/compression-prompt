2.3 Parallel Training Neural Networks To train models huge data sets, have implemented several models top large-scale distributed framework called DistBelief [6], including feedforward NNLM new models proposed this paper. The framework allows us run multiple replicas same model parallel, each replica synchronizes its gradient updates through centralized server keeps all parameters. For parallel training, use mini-batch asynchronous gradient descent with an adaptive learning rate procedure called Adagrad [7]. Under framework, it common use one hundred or more replicas, each using many CPU cores at different machines data center. 3 New Log-linear Models In section, propose two new architectures for learning distributed representations try minimize computational complexity. main observation previous section was most complexity caused by non-linear hidden layer model. While what makes neural networks so attractive, decided explore simpler models might not able represent data precisely neural networks, but can possibly trained much more data efciently. new architectures directly follow those proposed our earlier work [13, 14], where it was found neural network language can successfully trained two steps: rst, continuous vectors are learned using simple model, then N-gram NNLM trained top these distributed representations words. While there has been later substantial amount work focuses learning vectors, consider approach proposed [13] simplest one. Note related models have been proposed also much earlier [26, 8]. 3.1 Continuous Bag-of-Words Model rst proposed architecture similar feedforward NNLM, where non-linear hidden layer removed projection layer shared all (not just projection matrix); thus, all get projected into same position (their vectors are averaged). We call archi- tecture bag-of-words order history does not inuence projection. Furthermore, also use future; have obtained best performance task introduced next section by building log-linear classier with four future four history at input, where training criterion correctly classify current (middle) word. Training complexity then Q = N D + D log 2 ( V ) : (4) We denote further CBOW, unlike standard bag-of-words model, it uses continuous distributed representation context. architecture shown at Figure 1. Note weight matrix between input projection layer shared all positions same way NNLM. 3.2 Continuous Skip-gram Model second architecture similar CBOW, but instead predicting current based context, it tries maximize classication based another same sentence. More precisely, use each current an input log-linear classier with continuous projection layer, predict within certain range before after current word. We found increasing range improves quality resulting vectors, but it also increases computational complexity. Since more distant are usually less related current than those close it, give less weight distant by sampling less from those our training examples. The training complexity this architecture proportional Q = C ( D + D log 2 ( V )) ; (5) where C maximum distance words. Thus, if choose C = 5 , for each training will select randomly number R range < 1; C > , then use R from history 4