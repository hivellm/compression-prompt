2 noise 1. likelihood only depends dierence between ). We therefore dene ), do inference entirely terms , likelihood becomes same as probit classication: y Bernoulli (( ))). We observe GP prior induced because formed by performing linear operation have prior already (0 We derive induced covariance function (derivation Supplementary material) as: pref (( )) + Note this kernel pref respects anti-symmetry properties desired scenario, i.e. value u; perfectly anti-correlated v; ), ensuring P ] 1 P ] holds. Thus, conclude GP framework Chu Ghahramani, 2005 ], equivalent GPC particular class kernels, may call judgement kernels . Therefore, our algorithm presented Section 3 GPC readily applied pairwise also. 4 Related Methodologies There number closely related algorithms classication now review. Informative Vector Machine (IVM): Perhaps most closely re- lated IVM Lawrence et al., 2003 ]. This popular,and successful designed specically GPs; uses an infor- mation theoretic so appears very similar BALD. IVM algorithm designed subsampling dataset training GP, so privy y values before including measurement; cannot therefore work explic- itly output space i.e. Eqn. (2) . IVM uses Eqn. (1) but parameter entropies calculated approximately marginal subspace corresponding observed data points. entropy decrease after inclusion new data point then be calculated eciently using GP covariance matrix. Although IVM BALD motivated by same objective, they work fundamentally dierently when approximate inference carried out. At any time 7