t . We make observation that natural tasks it seems useful schedule decreases reg- ularization term. We assume forces focus on long correla- tions at expense short ones, so may be this decaying order allow better infor- mation. Language modelling language modelling 500 sig- moidal units biases (Mikolov et al. , 2012). trained sequences 200 steps, where state carried from one step one. We cut-o threshold 45 (though take sum cost sequence length) all ex- periments. prediction learning 0.01 using clipping ularization term, add 0.001 do not clipping. When predicting 5th future learning rate 0.1 without it. The factor predic- tion was set .01 kept constant, while modied task used an initial value 1