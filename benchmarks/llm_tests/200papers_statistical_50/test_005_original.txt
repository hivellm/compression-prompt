two entropy quantities. The rst term in Eqn. (2) , H[ y j x ; D ] can be handled analytically for the probit case: H[ y j x ; D ] 1 h Z ( f x ) N ( f x j x ; D ; 2 x ; D ) df x = h 0 @ 0 @ x ; D 2 x ; D + 1 1 A 1 A (3) The second term, E f p ( f jD ) [H[ y j x ; f ]] can be computed approximately as follows: E f p ( f jD ) [H[ y j x ; f ]] 1 Z h(( f x )) N ( f x j x ; D ; 2 x ; D ) df x (4) 2 Z exp f 2 x ln 2 N ( f x j x ; D ; 2 x ; D ) df x = C 2 x ; D + C 2 exp 0 @ 2 x ; D 2 2 x ; D + C 2 1 A where C = q
2 . The rst approximation, 1 , reects the Gaussian ap- proximation to the posterior. The integral in the left hand side of Eqn. (4) is intractable. By performing a Taylor expansion on ln h(( f x )) (see supplementary material) we can see that it can be approximated up to O ( f 4 x ) by a squared exponential curve, exp ( f 2 x = ln 2). We will refer to this approximation as 2 . Now we can apply the standard convolution formula for Gaussians to nally get a closed form expression for both terms of Eqn. (2). Fig. 1 depicts the striking accuracy of this simple approximation. The max- imum possible error that will be incurred when using this approximation is if N ( f x j x ; D ; 2 x ; D ) is centred at x ; D = 2 : 05 with 2 x ; D tending to zero (see Fig. 1, absolute error ), yielding only a 0.27% error in the integral in Eqn. (4) . The authors are unaware of previous use of this simple and useful approximation in this context. In Section 5 we investigate experimentally the information lost from approximations 1 and 2 as compared to the golden standard of extensive Monte Carlo simulation. To summarise, the BALD algorithm for Gaussian process classication con- sists of two steps. First it applies any standard approximate inference algorithm for GPCs (such as EP) to obtain the posterior predictive mean x ; D and x ; D for each point of interest x . Then, it selects a query x that maximises the following objective function: h 0 @ 0 @ x ; D 2 x ; D + 1 1 A 1 A C exp 2 x ; D 2 ( 2 x ; D + C 2 ) 2 x ; D + C 2 (5) 5