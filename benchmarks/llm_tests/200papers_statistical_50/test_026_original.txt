@ x k go to zero (for t  k large), it means that x t does not depend on x k (if we change x k by some , x t stays the same). This translates into the model at x t being close to conver- gence towards some attractor (which it would reach from anywhere in the neighbourhood of x k ). 2.3. The geometrical interpretation Let us consider a simple one hidden unit model (equa- tion (8)) where we provide an initial state x 0 and train the model to have a specic target value after 50 steps. Note that for simplicity we assume no input. x t = w ( x t  1 ) + b (8) Fig. 6 shows the error surface E 50 = (  ( x 50 )  0 : 7) 2 , where x 0 = : 5 and  to be the sigmoid function. We can more easily analyze the behavior of this model by further simplifying it to be linear (  then being the identity function), with b = 0. x t = x 0 w t from which it follows that @x t