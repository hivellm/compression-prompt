@ to have small norm, hence further helping with the exploding gradients problem. The fact that it helps when training recurrent neural models on long sequences suggests that while the cur- vature might explode at the same time with the gradi- ent, it might not grow at the same rate and hence not be sucient to deal with the exploding gradient. Echo State Networks (Lukosevicius and Jaeger, 2009) avoid the exploding and vanishing gradients problem by not learning the recurrent and input weights. They are sampled from hand crafted distributions. Because usually the largest eigenvalue of the recurrent weight is, by construction, smaller than 1, information fed in to the model has to die out exponentially fast. This means that these models can not easily deal with long term dependencies, even though the reason is slightly dierent from the vanishing gradients problem. An extension to the classical model is represented by leaky integration units (Jaeger et al. , 2007), where x k =  x k  1 + (1   )  ( W rec x k  1 + W in u k + b ). While these units can be used to solve the standard benchmark proposed by Hochreiter and Schmidhu- ber (1997) for learning long term dependencies (see (Jaeger, 2012)), they are more suitable to deal with low frequency information as they act as a low pass lter. Because most of the weights are randomly sam- pled, is not clear what size of models one would need to solve complex real world tasks. We would make a nal note about the approach pro- posed by Tomas Mikolov in his PhD thesis (Mikolov, 2012)(and implicitly used in the state of the art re- sults on language modelling (Mikolov et al. , 2011)). It involves clipping the gradientâ€™s temporal compo- nents element-wise (clipping an entry when it exceeds in absolute value a xed threshold). Clipping has been shown to do well in practice and it forms the backbone of our approach. 3.2. Scaling down the gradients As suggested in section 2.3, one simple mechanism to deal with a sudden increase in the norm of the gradi- ents is to rescale them whenever they go over a thresh- old (see algorithm 1).