In other words, D and G play the following two-player minimax game with value function V ( G; D ) : min G max D V ( D; G ) = E x  p data ( x ) [log D ( x )] + E z  p z ( z ) [log(1  D ( G ( z )))] : (1) In the next section, we present a theoretical analysis of adversarial nets, essentially showing that the training criterion allows one to recover the data generating distribution as G and D are given enough capacity, i.e., in the non-parametric limit. See Figure 1 for a less formal, more pedagogical explanation of the approach. In practice, we must implement the game using an iterative, numerical approach. Optimizing D to completion in the inner loop of training is computationally prohibitive, and on nite datasets would result in overtting. Instead, we alternate between k steps of optimizing D and one step of optimizing G . This results in D being maintained near its optimal solution, so long as G changes slowly enough. This strategy is analogous to the way that SML/PCD [31, 29] training maintains samples from a Markov chain from one learning step to the next in order to avoid burning in a Markov chain as part of the inner loop of learning. The procedure is formally presented in Algorithm 1. In practice, equation 1 may not provide sufcient gradient for G to learn well. Early in learning, when G is poor, D can reject samples with high condence because they are clearly different from the training data. In this case, log(1  D ( G ( z ))) saturates. Rather than training G to minimize log(1  D ( G ( z ))) we can train G to maximize log D ( G ( z )) . This objective function results in the same xed point of the dynamics of G and D but provides much stronger gradients early in learning.