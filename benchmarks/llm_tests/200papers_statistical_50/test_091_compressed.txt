Figure 2: BLEU scores generated translations respect lengths sen- tences. results are full test set which in- cludes having un- known models. 2012 news-test-2013 make development (validation) set, evaluate models (news-test-2014) WMT â€™14, consists 3003 present training data. After usual tokenization 6 shortlist 30,000 most frequent language our Any included shortlist mapped special token ( [ UNK ] ). do apply any special preprocessing, such lowercasing or stemming, 4.2 M ODELS two types one an RNN EncoderDecoder (RNNencdec, Cho 2014a), proposed model, refer RNNsearch. twice: 30 (RNNencdec-30, RNNsearch-30) then 50 (RNNencdec-50, RNNsearch-50). encoder decoder RNNencdec have units each. 7 encoder consists forward backward recurrent networks (RNN) Its has both multilayer network single maxout (Goodfellow 2013) layer compute conditional probability target (Pascanu 2014). minibatch stochastic gradient descent (SGD) algorithm together Adadelta (Zeiler, 2012) Each SGD update direction computed using minibatch 80 tences. trained for approximately days. Once trained, beam search nd approximately maximizes conditional probability (see, e.g., Graves, 2012; Boulanger-Lewandowski 2013). Sutskever (2014) this approach generate translations their machine For more details architectures training procedure experiments, see Appendices A B. R ESULTS 5.1 Q UANTITATIVE R ESULTS In Table 1, list performances measured BLEU score. It clear table all cases, proposed outperforms conventional RNNencdec. More importantly, performance high conventional phrase-based system (Moses), when only consisting known are considered. This signicant achievement, considering Moses uses separate monolingual corpus (418M words) addition parallel corpora RNNsearch RNNencdec.