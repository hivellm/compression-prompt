35.63 Table 1: BLEU scores trained models com- puted test set. second third columns show respectively scores all and, without any unknown them- selves reference translations. Note RNNsearch-50 ? trained much longer until performance development set stopped improv- ing. ( ) disallowed models generate [UNK] tokens when only having no unknown evaluated (last column). 5.2 Q UALITATIVE A NALYSIS 5.2.1 A LIGNMENT proposed approach provides intuitive way inspect (soft-)alignment generated translation those sentence. done visualizing annotation ij Eq. (6), Each row matrix plot indicates associated annotations. positions considered more important generating can alignments alignment English French largely monotonic. strong along diagonal matrix. However, also observe number non-trivial, non-monotonic alignments. Adjectives nouns are typically ordered differently French English, example (a). gure, translates [European Economic Area] een]. RNNsearch align [zone] [Area], jumping over two ([European] [Economic]), then looked back time complete whole eenne]. strength soft-alignment, opposed hard-alignment, evident, instance, (d). Consider [the man] translated [l’ homme]. Any hard alignment will map [l’] [man] [homme]. helpful translation, must consider following determine whether should be translated [le], [la], [les] [l’]. Our soft-alignment solves issue naturally letting look both [man], translate [l’]. similar behaviors presented cases additional benet soft align- ment deals phrases different lengths, requiring counter-intuitive mapping some nowhere ([NULL]) (see, e.g., Chapters 4 5 Koehn, 2010). 5.2.2 L ONG S ENTENCES As clearly visible 2 proposed (RNNsearch) much better than conventional (RNNencdec) translating long sentences. This likely due fact RNNsearch does not require encoding long xed-length vector perfectly, but only accurately encoding parts input surround particular word. As example, consider test set: An admitting privilege right doctor admit patient hospital medical centre