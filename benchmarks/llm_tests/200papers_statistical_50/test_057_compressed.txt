maximising expected value + , = E E h + max (1) The basic idea behind many algorithms estimate action- value function, using Bellman equation an iterative update, +1 = [ + max j ] Such iteration algorithms converge optimal action- function, ! ! [23]. In practice, this basic approach totally impractical, because action-value estimated separately for each sequence, without any generali- sation. Instead, it common use approximator estimate action-value function, In community this typically linear approximator, but sometimes non-linear approximator used instead, such neural network. We refer neural network approximator weights Q-network. A Q-network can be trained minimising sequence loss functions L changes at each iteration L s;a h y )) 2 (2) where y [ j ] target for iteration probability over sequences actions we refer behaviour distribution The parameters from previous iteration held xed when optimising loss L Note targets depend on network weights; this contrast targets used for supervised learning, which xed before begins. Differentiating loss respect weights we arrive at following gradient, L s;a ); h : (3) Rather than computing full expectations above gradient, often computationally expe- dient optimise loss stochastic gradient descent. If weights updated after every time-step, expectations replaced single samples from behaviour emulator respectively, then we arrive at familiar Q-learning algorithm [26]. Note this algorithm model-free : solves task directly using sam- ples from emulator without explicitly constructing an estimate It also off-policy : learns about greedy strategy while following behaviour distribution ensures adequate exploration state space. In practice, behaviour often se- lected an -greedy strategy follows greedy strategy probability selects random action probability 3 Related Work Perhaps best-known success story TD-gammon backgammon- playing program which learnt entirely self-play, achieved super- human level play [24]. TD-gammon used model-free algorithm similar Q-learning, approximated using multi-layer perceptron one hidden layer However, early attempts follow up on TD-gammon, including applications same method chess, Go checkers were less successful. This led widespread belief TD-gammon approach was special case only worked in backgammon, perhaps because stochasticity in dice rolls helps explore state space also makes value particularly smooth [19]. Furthermore, it was shown combining model-free algorithms such Q- non-linear approximators [25], or indeed off-policy [1] could cause Q-network diverge. Subsequently, majority work in fo- cused on linear approximators better convergence guarantees [25].