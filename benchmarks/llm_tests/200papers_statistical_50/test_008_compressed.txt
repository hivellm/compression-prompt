Z jD ) j f; If posterior + 1 approximated directly one gets jD ; ; BALD calculates entropy dierence ^ without having compute each candidate . In contrast, IVM calculates change . The IVM’s ap- proach cannot calculate full innite dimensional posterior, requires O N N updates. To updates eciently, proximate inference performed using Assumed Density Filtering (ADF). Using ADF means direct indicating IVM makes further BALD. Since BALD only O (1) can aord more accurate, iterative procedures, EP. Information Theoretic approaches: Maximum Entropy Sampling (MES) Sebastiani Wynn, 2000 explicitly works dataspace (Eqn. MES was proposed regression input-independent noise. Al- though Eqn. used, second constant input independent noise ignored. One cannot, however, MES heteroscedastic re- gression classication; fails dierentiate (about which our may be condent). Some toy demonstrations show ‘information based’ active learning criterion performing pathologically by repeatedly querying close boundary regions high e.g. Huang 2010 This MES inappropriate domain; BALD distinguishes be- tween eliminates problems we will show. Mutual-information based presented Fuhrmann, They being measured interest. Fuhrmann Fuhrmann, applies linear Gaussian acoustic arrays, al. communications channel. Although related, objectives work parameters applied classication. Guestrin 2005 Krause 2006 information. They specify interest advance distributions observed locations. Although promising regression, tractable input-dependent noise, such classication preference learning. Decision theoretic: We briey mention decision theoretic approaches ac- tive learning. Two closely related algorithms, Kapoor 2007 Zhu ], seek minimize expected cost i.e. loss weighted misclassication probability on all seen future data. These methods observe locations test their objective functions become monotonic predictive entropies test points. Kapoor 2007 also includes an empirical error term 8