Figure 1: Screen shots ve Atari 2600 Games: Left-to-right ) Pong, Breakout, Space Invaders, Seaquest, Beam Rider experience replay mechanism [13] randomly samples previous transitions, thereby smooths training distribution over many past behaviors. We apply our approach range Atari 2600 games implemented Arcade Learning Envi- ronment (ALE) [3]. Atari 2600 challenging RL testbed presents agents high dimen- sional visual input 210 160 RGB video 60Hz) diverse interesting set tasks were designed difcult humans players. Our goal create single neural able successfully learn play possible. not pro- vided any game-specic information or hand-designed visual features, not privy internal emulator; learned nothing video input, reward terminal signals, possible actionsjust human player would. Furthermore ar- chitecture hyperparameters used training were kept constant across games. So far has outperformed previous RL algorithms six seven we have attempted surpassed expert human player three them. Figure provides sample screenshots ve used training. Background consider tasks interacts environment E this case Atari emulator, actions, observations rewards. At each time-step selects legal actions, A f K g passed modies its score. In general E stochastic. emulatorâ€™s internal observed agent; instead observes image R d emulator, vector raw pixel values representing screen. In addition receives reward r representing change score. Note general score depend whole prior observations; feedback about received after thousands time-steps have elapsed. Since observes images screen, task partially observed states perceptually aliased, i.e. impossible fully understand situation screen therefore consider observations, :::; learn strategies depend upon these sequences. All assumed terminate nite number time-steps. This formalism gives rise large but nite Markov decision process (MDP) each distinct state. As result, we can apply standard reinforcement learning methods MDPs, simply using complete representation time goal interact selecting way maximises rewards. make standard assumption rewards discounted factor per time-step, dene discounted return time R P T r where T time-step terminates. We dene action-value function Q s; maximum expected return achievable following any strategy, after seeing some then taking some Q s; max E [ R j s; a; ] where policy mapping sequences (or distributions over actions). optimal action-value function obeys important identity known Bellman equation . This based following intuition: if optimal value Q next time-step was known all possible then optimal strategy select 2