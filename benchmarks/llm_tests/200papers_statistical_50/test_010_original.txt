Dim. 1 Dim. 2 (a) block in the middle Dim. 1 Dim. 2 (b) block in the corner Dim. 1 Dim. 2 (c) checkerboard 0 : 5 0 : 9 No. queried points Accuracy 0 : 5 No. queried points Accuracy 0 : 5 No. queried points Accuracy Figure 3: Top: Evaluation on articial datasets. Exemplars of the two classes are shown with black squares ( ) and red circles ( ). Bottom: Results of active learning with nine methods: random query ( ), BALD( ), MES ( ), QBC with the vote criterion with 2 ( ) and 100 ( ) committee members, active SVM ( ), IVM ( ), decision theoretic: [ Kapoor et al., 2007 ] ( ), [Zhu et al., 2003] ( ) and empirical error ( ). an asymptotically unbiased estimate of the expected information gain. Using extensive Monte Carlo as the ‘gold standard’, we can evaluate how much we loose by applying these approximations. We quantify approximation error as: max x 2P I ( x ) I (arg max x 2P ^ I ( x )) max x 2P I ( x ) 100% (8) where I is the objective computed using Monte Carlo, ^ I is the approximate objective. The cancer UCI dataset was used, results and discussion are in Fig. 2. Pool based active learning: We test BALDfor GPC and preference learning in the pool-based setting i.e. selecting x values from a xed set of data-points. Although BALD can generalise to selecting continuous x , this enables us to compare to algorithms that cannot. We compare to eight other algorithms: random sampling, MES, QBC (with 2 and 100 committee members), SVM with version space approximation [ Tong and Koller, 2001 ], decision theoretic approaches in [ Kapoor et al., 2007 , Zhu et al., 2003 ] and directly minimizing 10