The most important distinguishing feature of this approach from the basic encoderdecoder is that it does not attempt to encode a whole input sentence into a single xed-length vector. Instead, it en- codes the input sentence into a sequence of vectors and chooses a subset of these vectors adaptively while decoding the translation. This frees a neural translation model from having to squash all the information of a source sentence, regardless of its length, into a xed-length vector. We show this allows a model to cope better with long sentences. In this paper, we show that the proposed approach of jointly learning to align and translate achieves signicantly improved translation performance over the basic encoderdecoder approach. The im- provement is more apparent with longer sentences, but can be observed with sentences of any length. On the task of English-to-French translation, the proposed approach achieves, with a single model, a translation performance comparable, or close, to the conventional phrase-based system. Furthermore, qualitative analysis reveals that the proposed model nds a linguistically plausible (soft-)alignment between a source sentence and the corresponding target sentence. 2 B ACKGROUND : N EURAL M ACHINE T RANSLATION From a probabilistic perspective, translation is equivalent to nding a target sentence y that max- imizes the conditional probability of y given a source sentence x , i.e., arg max y p ( y j x ) . In neural machine translation, we t a parameterized model to maximize the conditional probability of sentence pairs using a parallel training corpus. Once the conditional distribution is learned by a translation model, given a source sentence a corresponding translation can be generated by searching for the sentence that maximizes the conditional probability. Recently, a number of papers have proposed the use of neural networks to directly learn this condi- tional distribution (see, e.g., Kalchbrenner and Blunsom, 2013; Cho et al. , 2014a; Sutskever et al. , 2014; Cho et al. , 2014b; Forcada and  Neco, 1997). This neural machine translation approach typ- ically consists of two components, the rst of which encodes a source sentence x and the second decodes to a target sentence y . For instance, two recurrent neural networks (RNN) were used by (Cho et al. , 2014a) and (Sutskever et al. , 2014) to encode a variable-length source sentence into a xed-length vector and to decode the vector into a variable-length target sentence. Despite being a quite new approach, neural machine translation has already shown promising results. Sutskever et al. (2014) reported that the neural machine translation based on RNNs with long short- term memory (LSTM) units achieves close to the state-of-the-art performance of the conventional phrase-based machine translation system on an English-to-French translation task. 1 Adding neural components to existing translation systems, for instance, to score the phrase pairs in the phrase table (Cho et al. , 2014a) or to re-rank candidate translations (Sutskever et al. , 2014), has allowed to surpass the previous state-of-the-art performance level. 2.1 RNN E NCODER D ECODER Here, we describe briey the underlying framework, called RNN EncoderDecoder , proposed by Cho et al. (2014a) and Sutskever et al. (2014) upon which we build a novel architecture that learns to align and translate simultaneously. In the EncoderDecoder framework, an encoder reads the input sentence, a sequence of vectors x = ( x 1 ;    ; x T x ) , into a vector c . 2 The most common approach is to use an RNN such that h t = f ( x t ; h t  1 ) (1) and c = q ( f h 1 ;    ; h T x g ) ; where h t 2 R n is a hidden state at time t , and c is a vector generated from the sequence of the hidden states. f and q are some nonlinear functions. Sutskever et al. (2014) used an LSTM as f and q ( f h 1 ;    ; h T g ) = h T , for instance.