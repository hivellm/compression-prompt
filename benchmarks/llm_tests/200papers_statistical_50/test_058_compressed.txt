More recently, there revival interest combining learning. Deep networks estimate environment E restricted Boltzmann machines estimate value [21]; policy [9]. addition, divergence issues partially addressed temporal-difference methods. These methods proven converge when evaluating xed policy nonlinear approximator [14]; when control linear approximation restricted variant [15]. However, these methods not yet extended nonlinear control. Perhaps most similar prior work own tted (NFQ) [20]. NFQ optimises sequence loss functions Equation 2, RPROP algorithm update parameters Q-network. However, uses batch update computational cost per iteration proportional size set, whereas consider low constant cost per iteration scale large data-sets. NFQ successfully simple real-world control tasks purely input, rst autoencoders low dimensional task, then applying NFQ this [12]. contrast applies end-to-end, inputs; result may relevant discriminating action-values. previously combined simple [13], but again starting low-dimensional state rather than raw inputs. use Atari 2600 emulator platform introduced [3], who standard algorithms linear approximation generic features. Subsequently, results improved larger number features, tug-of-war hashing randomly project lower-dimensional space [2]. HyperNEAT evolutionary architecture [8] Atari platform, evolve (separately, distinct game) representing strategy game. When trained repeatedly against deterministic sequences emulator’s reset facility, these strategies able exploit design aws several Atari games. 4 Deep Reinforcement Learning Recent breakthroughs computer vision speech recognition relied efciently networks very sets. successful approaches trained inputs, lightweight based descent. By feeding sufcient networks, often possible better representations handcrafted [11]. These successes motivate learning. Our goal connect algorithm operates RGB images efciently process updates. Tesauro’s TD-Gammon architecture provides starting point such approach. This architec- ture parameters estimates function, on-policy samples experience, algorithm’s interactions envi- ronment (or self-play, case backgammon). Since outperform best human backgammon players 20 years ago, natural wonder whether two decades hardware improvements, coupled modern architectures scalable RL algorithms might produce signicant progress. contrast TD-Gammon similar approaches, utilize technique known expe- rience [13] store agent’s experiences time-step, = ( r ) data-set D = 1 :::; N pooled over many episodes memory . During inner loop algorithm, apply minibatch samples experience, D drawn random pool stored samples. After performing replay, agent selects executes action according -greedy policy. Since histories arbitrary length inputs can be difcult, Q-function instead works xed length representation histories produced . full algorithm, which call presented Algorithm 1. This several advantages over standard online [23]. First, each step experience potentially many weight updates, which allows greater efciency. 4