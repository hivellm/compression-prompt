The decoder is often trained to predict the next word y t 0 given the context vector c and all the previously predicted words f y 1 ;    ; y t 0  1 g . In other words, the decoder denes a probability over the translation y by decomposing the joint probability into the ordered conditionals: p ( y ) = T Y t =1 p ( y t j f y 1 ;    ; y t  1 g ; c ) ; (2) where y =  y 1 ;    ; y T y  . With an RNN, each conditional probability is modeled as p ( y t j f y 1 ;    ; y t  1 g ; c ) = g ( y t  1 ; s t ; c ) ; (3) where g is a nonlinear, potentially multi-layered, function that outputs the probability of y t , and s t is the hidden state of the RNN. It should be noted that other architectures such as a hybrid of an RNN and a de-convolutional neural network can be used (Kalchbrenner and Blunsom, 2013). 3 L EARNING TO A LIGN AND T RANSLATE In this section, we propose a novel architecture for neural machine translation. The new architecture consists of a bidirectional RNN as an encoder (Sec. 3.2) and a decoder that emulates searching through a source sentence during decoding a translation (Sec. 3.1). 3.1 D ECODER : G ENERAL D ESCRIPTION