the alignment is not considered to be a latent variable. Instead, the alignment model directly com- putes a soft alignment, which allows the gradient of the cost function to be backpropagated through. This gradient can be used to train the alignment model as well as the whole translation model jointly. We can understand the approach of taking a weighted sum of all the annotations as computing an expected annotation , where the expectation is over possible alignments. Let  ij be a probability that the target word y i is aligned to, or translated from, a source word x j . Then, the i -th context vector c i is the expected annotation over all the annotations with probabilities  ij . The probability  ij , or its associated energy e ij , reects the importance of the annotation h j with respect to the previous hidden state s i  1 in deciding the next state s i and generating y i . Intuitively, this implements a mechanism of attention in the decoder. The decoder decides parts of the source sentence to pay attention to. By letting the decoder have an attention mechanism, we relieve the encoder from the burden of having to encode all information in the source sentence into a xed- length vector. With this new approach the information can be spread throughout the sequence of annotations, which can be selectively retrieved by the decoder accordingly. 3.2 E NCODER : B IDIRECTIONAL RNN FOR A NNOTATING S EQUENCES The usual RNN, described in Eq. (1), reads an input sequence x in order starting from the rst symbol x 1 to the last one x T x . However, in the proposed scheme, we would like the annotation of each word to summarize not only the preceding words, but also the following words. Hence, we propose to use a bidirectional RNN (BiRNN, Schuster and Paliwal, 1997), which has been successfully used recently in speech recognition (see, e.g., Graves et al. , 2013). A BiRNN consists of forward and backward RNN’s. The forward RNN ! f reads the input sequence as it is ordered (from x 1 to x T x ) and calculates a sequence of forward hidden states ( ! h 1 ;    ; ! h T x ) . The backward RNN   f reads the sequence in the reverse order (from x T x to x 1 ), resulting in a sequence of backward hidden states (   h 1 ;    ;   h T x ) . We obtain an annotation for each word x j by concatenating the forward hidden state ! h j and the backward one   h j , i.e., h j = h ! h > j ;   h > j i > . In this way, the annotation h j contains the summaries of both the preceding words and the following words. Due to the tendency of RNNs to better represent recent inputs, the annotation h j will be focused on the words around x j . This sequence of annotations is used by the decoder and the alignment model later to compute the context vector (Eqs. (5)(6)). See Fig. 1 for the graphical illustration of the proposed model. 4 E XPERIMENT S ETTINGS We evaluate the proposed approach on the task of English-to-French translation. We use the bilin- gual, parallel corpora provided by ACL WMT ’14. 3 As a comparison, we also report the perfor- mance of an RNN EncoderDecoder which was proposed recently by Cho et al. (2014a). We use the same training procedures and the same dataset for both models. 4 4.1 D ATASET WMT ’14 contains the following English-French parallel corpora: Europarl (61M words), news commentary (5.5M), UN (421M) and two crawled corpora of 90M and 272.5M words respectively, totaling 850M words. Following the procedure described in Cho et al. (2014a), we reduce the size of the combined corpus to have 348M words using the data selection method by Axelrod et al. (2011). 5 We do not use any monolingual data other than the mentioned parallel corpora, although it may be possible to use a much larger monolingual corpus to pretrain an encoder. We concatenate news-test-