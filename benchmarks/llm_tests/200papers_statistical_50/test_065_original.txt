[6]  George E. Dahl, Dong Yu, Li Deng, and Alex Acero. Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. Audio, Speech, and Language Pro- cessing, IEEE Transactions on , 20(1):30 42, January 2012. [7]  Alex Graves, Abdel-rahman Mohamed, and Geoffrey E. Hinton. Speech recognition with deep recurrent neural networks. In Proc. ICASSP , 2013. [8]  Matthew Hausknecht, Risto Miikkulainen, and Peter Stone. A neuro-evolution approach to general atari game playing. 2013. [9]  Nicolas Heess, David Silver, and Yee Whye Teh. Actor-critic reinforcement learning with energy-based policies. In European Workshop on Reinforcement Learning , page 43, 2012. [10]  Kevin Jarrett, Koray Kavukcuoglu, MarcAurelio Ranzato, and Yann LeCun. What is the best multi-stage architecture for object recognition? In Proc. International Conference on Com- puter Vision and Pattern Recognition (CVPR 2009) , pages 21462153. IEEE, 2009. [11]  Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton. Imagenet classication with deep con- volutional neural networks. In Advances in Neural Information Processing Systems 25 , pages 11061114, 2012. [12]  Sascha Lange and Martin Riedmiller. Deep auto-encoder neural networks in reinforcement learning. In Neural Networks (IJCNN), The 2010 International Joint Conference on , pages 18. IEEE, 2010. [13]  Long-Ji Lin. Reinforcement learning for robots using neural networks. Technical report, DTIC Document, 1993. [14]  Hamid Maei, Csaba Szepesvari, Shalabh Bhatnagar, Doina Precup, David Silver, and Rich Sutton. Convergent Temporal-Difference Learning with Arbitrary Smooth Function Approxi- mation. In Advances in Neural Information Processing Systems 22 , pages 12041212, 2009. [15]  Hamid Maei, Csaba Szepesv Â· ari, Shalabh Bhatnagar, and Richard S. Sutton. Toward off-policy learning control with function approximation. In Proceedings of the 27th International Con- ference on Machine Learning (ICML 2010) , pages 719726, 2010. [16]  Volodymyr Mnih. Machine Learning for Aerial Image Labeling . PhD thesis, University of Toronto, 2013. [17]  Andrew Moore and Chris Atkeson. Prioritized sweeping: Reinforcement learning with less data and less real time. Machine Learning , 13:103130, 1993. [18]  Vinod Nair and Geoffrey E Hinton. Rectied linear units improve restricted boltzmann ma- chines. In Proceedings of the 27th International Conference on Machine Learning (ICML 2010) , pages 807814, 2010. [19]  Jordan B. Pollack and Alan D. Blair. Why did td-gammon work. In Advances in Neural Information Processing Systems 9 , pages 1016, 1996. [20]  Martin Riedmiller. Neural tted q iterationrst experiences with a data efcient neural re- inforcement learning method. In Machine Learning: ECML 2005 , pages 317328. Springer, 2005. [21]  Brian Sallans and Geoffrey E. Hinton. Reinforcement learning with factored states and actions. Journal of Machine Learning Research , 5:10631088, 2004. [22]  Pierre Sermanet, Koray Kavukcuoglu, Soumith Chintala, and Yann LeCun. Pedestrian de- tection with unsupervised multi-stage feature learning. In Proc. International Conference on Computer Vision and Pattern Recognition (CVPR 2013) . IEEE, 2013. [23]  Richard Sutton and Andrew Barto. Reinforcement Learning: An Introduction . MIT Press, 1998. [24]  Gerald Tesauro. Temporal difference learning and td-gammon. Communications of the ACM , 38(3):5868, 1995. [25]  John N Tsitsiklis and Benjamin Van Roy. An analysis of temporal-difference learning with function approximation. Automatic Control, IEEE Transactions on , 42(5):674690, 1997. [26]  Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning , 8(3-4):279292, 1992. 9