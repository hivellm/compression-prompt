Second, directly consecutive inefcient, due strong correlations between samples; randomizing breaks these correlations therefore reduces variance updates. Third, on-policy current determine next data sample trained on. For example, if maximizing move left training will dominated left-hand side; if maximizing switches right training distribution will also switch. It easy see how unwanted feedback loops may arise could get stuck poor local minimum, even diverge catastrophically [25]. By behavior distribution averaged over many its previous states, smoothing out avoiding oscillations divergence parameters. Note replay, necessary off-policy (because different those generate sample), motivates choice Q-learning. In practice, algorithm stores N tuples memory, uniformly random D performing updates. This approach respects limited since buffer does not differentiate important always overwrites recent nite size N . Similarly, uniform sampling gives equal importance memory. A more sophisticated sampling strategy might emphasize most, similar prioritized sweeping [17]. 4.1 Preprocessing Model Architecture Working directly Atari frames, 210 160 pixel images 128 color palette, computationally demanding, so apply basic preprocessing step aimed reducing dimensionality. preprocessed rst converting RGB gray-scale down-sampling 110 image. obtained region image roughly captures playing area. cropping stage required because GPU implementation 2D convolutions [11], expects square inputs. experiments paper, function 1 applies preprocessing 4 history stacks them produce -function. There several ways parameterizing Since maps history- pairs scalar estimates Q-value, have been as inputs network approaches [20, 12]. main drawback type separate forward pass required compute Q-value each action, resulting cost scales linearly number actions. We instead use there separate output unit each action, state representation network. outputs correspond predicted Q-values individual state. main advantage type architecture ability compute Q-values all possible actions given state single forward pass through network. 5