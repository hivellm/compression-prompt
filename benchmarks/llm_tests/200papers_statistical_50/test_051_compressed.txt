Google News day, while time Skip-gram days. For experiments reported further, just (again, decrease linearly so that it approaches zero end training). twice much using gives comparable better than iterating over same epochs, shown Table 5, provides additional small speedup. Large Scale Parallel Models mentioned earlier, implemented various distributed framework Dis- tBelief. Below several trained mini-batch asynchronous gradient descent adaptive procedure Ada- grad [7]. We 50 to 100 replicas during training. The number CPU cores an 8