N EURAL M ACHINE T RANSLATION BY J OINTLY L EARNING TO LIGN AND T RANSLATE Dzmitry Bahdanau Jacobs University Bremen, Germany KyungHyun Cho Yoshua Bengio Universit · e de Montr · eal BSTRACT Neural recently approach transla- tion. Unlike traditional statistical translation, aims at building single can be jointly tuned maximize performance. The models recently neu- ral often belong family encoderdecoders encode into vector from generates translation. In paper, conjecture use vector bottleneck improving performance basic architec- ture, propose extend allowing automatically (soft-)search parts relevant predicting word, without having form these parts as hard segment explicitly. With new approach, achieve performance comparable existing state-of-the-art phrase-based system task English-to-French Furthermore, qualitative analysis reveals (soft-)alignments found agree well our intuition. 1 I NTRODUCTION Neural newly emerging Kalchbrenner Blunsom (2013), Sutskever (2014) Cho (2014b). Unlike traditional phrase-based system (see, e.g., Koehn 2003) consists many small sub-components tuned separately, attempts build train single, large reads correct Most models belong family decoders (Sutskever 2014; Cho 2014a), each lan- guage, or involve language-specic applied each whose com- pared (Hermann Blunsom, 2014). An reads encodes sen- tence from encoded whole system, consists language pair, jointly trained maximize probability correct given sentence. potential issue needs able compress all necessary information This may make it difcult cope long sentences, especially those longer than sentences training corpus. Cho (2014b) showed indeed performance basic deteriorates rapidly as length input increases. In order address issue, we introduce extension learns align translate jointly. Each time generates word translation, it (soft-)searches set positions where most relevant information concentrated. The then predicts target word based on context vectors associated these positions all previous generated target words.