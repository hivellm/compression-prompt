Figure 3: leftmost plot shows predicted function 30 frame segment game Seaquest. three screenshots correspond frames labeled A, B, C respectively. 5.2 Visualizing Value Function Figure 3 visualization function Seaquest. gure jumps appears left screen A). agent then res torpedo peaks torpedo hit B). Finally, falls roughly its original disappears C). Figure demonstrates able how evolves reasonably complex sequence events. 5.3 Main Evaluation We compare performing RL literature 4]. labeled Sarsa Sarsa algorithm linear policies several different hand- engineered Atari task performing set Con- tingency same basic Sarsa but augmented representation parts under agentâ€™s control [4]. Note both these incorporate signicant knowledge visual problem background sub- traction treating 128 colors channel. Since many Atari use one distinct type object, treating channel can be similar producing binary map encoding presence type. contrast, agents only receive raw RGB screenshots input must detect objects their own. addition agents, expert player selects actions uniformly random. performance median reward achieved around hours playing game. Note reported much higher than ones Bellemare For methods, follow evaluation strategy Bellemare 5] average obtained running -greedy = 0 : 05 xed number steps. rst ve rows table 1 show per-game average all games. Our (labeled DQN) outperforms other learning substantial margin all seven games despite incorporating almost no prior knowledge inputs. We also include comparison evolutionary policy search approach from [8] last three rows table 1. We two sets results this method. HNeat Best score reects results obtained using hand-engineered object detector algorithm outputs locations 7