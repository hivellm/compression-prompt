assumes exact match, results Table 8 would score only about 60%). We believe trained even data sets dimensionality perform signicantly better, enable development new innovative applications. Another way improve accuracy provide more than example relationship. By ten examples instead form relationship (we average individual together), improvement about 10% absolutely semantic-syntactic test. It apply operations solve different For example, good selecting out-of-the-list computing average list nding most distant vector. This popular type problems certain human intelligence tests. Clearly, there still lot discoveries made techniques. 6 Conclusion paper studied representations words derived various collection syntactic semantic language simple architectures, compared popular network (both feedforward recurrent). Because lower computational complexity, compute accurate dimensional set. Using DistBelief distributed framework, should CBOW Skip-gram corpora trillion basically unlimited size vocabulary. That several orders magnitude previously published results similar models. An interesting task where recently been shown signicantly outperform state art SemEval-2012 Task 2 [11]. publicly available RNN used together achieve 50% increase Spearmanâ€™s rank correlation previous result [31]. network based many NLP tasks, sentiment analysis [12] paraphrase detection [28]. It expected applications benet architectures described paper. Our ongoing work shows successfully applied automatic extension facts Knowledge Bases, verication correctness existing facts. Results machine translation experiments look promising. In future, would interesting compare Latent Relational Analysis [30] others. We believe comprehensive test set help research community improve existing techniques estimating vectors. We expect quality become an important building block future NLP applications. 10