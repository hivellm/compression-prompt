Figure 1. Schematic a neural network. The connections hidden layer allow information persist from one input another. exploding gradient problems described Bengio et al. (1994). 1.1. Training networks A generic neural network, with state time equation (1). In theoretical section this paper we will sometimes make use specic parametrization equa- tion (11) order provide more precise conditions intuitions about everyday use-case. F (1) ) (2) parameters are biases collected general case. 0 provided user, set zero learned, an element-wise function (usually tanh sigmoid cost measures performance network on some task it broken apart into individual costs each step E P T E where E L ). One approach that can be used compute nec- essary gradients Backpropagation Through Time (BPTT), where model represented as