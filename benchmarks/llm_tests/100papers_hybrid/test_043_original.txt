t . We make the observation that for natural tasks it seems useful to use a schedule that decreases the reg- ularization term. We assume that the regularization term forces the model to focus on long term correla- tions at the expense of short term ones, so it may be useful to have this decaying factor in order to allow the model to make better use of the short term infor- mation. Language modelling For the language modelling task we used a 500 sig- moidal hidden units model with no biases (Mikolov et al. , 2012). The model is trained over sequences of 200 steps, where the hidden state is carried over from one step to the next one. We use a cut-o threshold of 45 (though we take the sum of the cost over the sequence length) for all ex- periments. For next character prediction we have a learning rate of 0.01 when using clipping with no reg- ularization term, 0.05 when we add the regularization term and 0.001 when we do not use clipping. When predicting the 5th character in the future we use a learning rate of 0.05 with the regularization term and 0.1 without it. The regularization factor  for next character predic- tion was set to .01 and kept constant, while for the modied task we used an initial value of 0.05 with a 1