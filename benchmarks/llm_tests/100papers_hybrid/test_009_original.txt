EP ( 1 ) Laplace ( 1 ) 7 : 51 2 : 51 41 : 57 4 : 02 0 : 16 0 : 05 7 : 43 2 : 40 40 : 45 3 : 67 Figure 2: Percentage approximation error ( 1 s.d.) for dierent methods of approximate inference ( columns ) and approximation methods for evaluating Eqn. (4) ( rows ). The results indicate that 2 is a very accurate approximation; EP causes some loss and Laplace signicantly more, which is in line with the comparison presented in [ Kuss and Rasmussen, 2005 ]. For our experiments we use EP. that can yield pathological behaviour (we investigate this experimentally). These approaches are computationally expensive, requiring O ( N x N y ) posterior updates. Also, they must know the locations of the test data (and thus are transductive approaches); designing an inductive, decision-theoretic algorithm is an open, hard problem as it would require expensive integration over possible test data distributions. Non-probabilistic Some non-probabilistic methods have close analogues to information theoretic active learning. Perhaps the most ubiquitous is active learning for SVMs [ Tong and Koller, 2001 , Seung et al., 1992 ], where the volume of Version Space (VS) is used as a proxy for the posterior entropy. If a uniform (improper) prior is used with a deterministic classication likelihood, the log volume of VS and Bayesian posterior entropy are in fact equivalent. Just as Bayesian posteriors become intractable after observing many data points, VS can become complicated. [ Tong and Koller, 2001 ] proposes methods for approximat- ing VS with a simple shapes, such as hyperspheres (their simplest approximation reduces to margin sampling). This closely resembles approximating a Bayesian posterior using a Gaussian distribution via the Laplace or EP approximations. [ Seung et al., 1992 ] sidesteps the problem by working with predictions. The al- gorithm, Query by Committee (QBC), samples parameters from VS (committee members), they vote on the outcome of each possible x . The x with the most balanced vote is selected; this is termed the ‘principle of maximal disagreement’. If BALD is used with a sampled posterior, query by committee is implemented but with a probabilistic measure of disagreement. QBC’s deterministic vote criterion discards condence in the predictions and so can exhibit the same pathologies as MES. 5 Experiments Quantifying Approximation Losses: To obtain (5) we made two approx- imations: we perform approximate inference ( 1 ), and we approximated the binary entropy of the Gaussian CDF by a squared exponential ( 2 ). Both of these can be substituted with Monte Carlo sampling, enabling us to compute 9