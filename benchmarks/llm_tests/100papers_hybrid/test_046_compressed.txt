perplexity V )) For example when vocabulary size one million words, results about two times speedup evaluation. While crucial LMs computational bottleneck N D term, we will later propose architectures do layers thus depend heavily efciency softmax normalization. 2.2 Recurrent Neural Net Language Model (RNNLM) language has been proposed overcome certain limitations feedforward NNLM, such need specify context length (the order ), because theoretically RNNs represent more complex patterns than shallow networks [15, 2]. RNN does projection layer; only input, output layer. What special type matrix connects itself, time-delayed connections. This allows form some kind short memory, information past represented gets updated current input previous time step. per training RNN Q = + V; (3) where word representations D same dimensionality Again, efciently reduced log 2 V ) hierarchical softmax. Most complexity then comes 3