Figure 6. We plot error surface single hidden unit recurrent network, highlighting existence high cur- vature walls. solid lines depicts standard trajectories might follow. Using dashed arrow diagram shows what would happen gradients rescaled xed size its norm above threshold. explode so does along leading error surface like seen Fig. If holds, then gives simple solution gradients problem depicted Fig. If both leading eigenvector aligned follows has steep perpen- dicular (and consequently gradient). This means stochastic (SGD) reaches step, will forced jump across moving perpen- dicular walls, possibly leaving disrupting learning process. dashed arrows Fig. 6 correspond ignoring norm large ensuring stays close wall. key insight all steps taken explodes aligned ignore (i.e. moves perpendicular wall). At small-norm there- fore merely pushes back inside smoother low- besides wall, whereas regular bring very far, thus slowing preventing further training. Instead, bounded step, get back smooth near SGD free explore directions. important addition scenario classical high valley, assume val- ley wide, have large around land rely methods move towards local minima. This why just clipping sucient, requiring method. Note algo- rithm work rate growth (a case for method fail ratio still explode). Our hypothesis understand re- cent success Hessian-Free approach compared methods. There two dif- ferences Hessian-Free second- algorithms. First, uses full Hessian hence deal directions necessarily axis-aligned. Second, computes new estimate Hessian before each up- date take into account abrupt changes (such ones suggested our hypothe- sis) while approaches smoothness as- sumption, i.e., averaging 2nd signals over many steps. 3. Dealing vanishing 3.1. Previous solutions Using an L1 L2 penalty recurrent weights gradients. Given parame- ters initialized small values, spectral W rec probably smaller than 1, follows explode (see necessary condi- tion found section 2.1). regularization ensure during training spectral never exceeds 1. This approach limits sim- ple (with point attractor at origin), any information inserted die out exponentially fast time. In train generator network, nor exhibit long term memory traces. Doya (1993) proposes pre-program (to initialize regime) teacher forcing . rst proposal assumes exhibits beginning same kind asymptotic behaviour required target, then there no need cross bifurcation boundary. downside always know required asymptotic behaviour, and, such information known, trivial initial- ize specic regime. We should also note such initialization does prevent cross- ing boundary between basins attraction, which, shown, could happen even though no bifurcation boundary crossed. Teacher forcing more interesting, yet very well understood solution. It seen way initializing