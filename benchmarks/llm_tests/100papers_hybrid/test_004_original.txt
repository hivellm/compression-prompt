in [ MacKay, 1992 , Krishnapuram et al., , Lawrence et al., 2003 ]) but are free to choose from many of the available algorithms. Minimal additional approximations are introduced, and so, to our knowledge our algorithm represents the most exact and fastest way to perform full information-theoretic active learning in non-parametric discriminative models. 3 Gaussian Processes for Classication and Pref- erence Learning In this section we derive the BALD algorithm for Gaussian Process classication (GPC). GPs are a powerful and popular non-parametric tool for regression and classication. GPC appears to be an especially challenging problem for information-theoretic active learning because the parameter space is innite, however, by using (2) we are able to calculate fully the relevant information quantities without having to work out entropies of innite dimensional objects. The probabilistic model underlying GPC is as follows: f GP( ( ) ; k ( ; )) y j x ; f Bernoulli(( f ( x ))) The latent parameter, now called f is a function X ! R , and is assigned a Gaussian process prior with mean ( ) and covariance function or kernel k ( ; ). We consider the probit case where given the value of f , y takes a Bernoulli distribution with probability ( f ( x )), and is the Gaussian CDF. For further details on GPs see [Rasmussen and Williams, 2005]. Inference in the GPC model is intractable; given some observations D , the posterior over f becomes non-Gaussian and complicated. The most commonly used approximate inference methods { EP, Laplace approximation, Assumed Density Filtering and sparse methods { all approximate the posterior by a Gaussian [ Rasmussen and Williams, 2005 ]. Throughout this section we will assume that we are provided with such a Gaussian approximation from one of these methods, though the active learning algorithm does not care which one. In our derivation we will use 1 to indicate where such an approximation is exploited. The informativeness of a query x is computed using Eqn. (2) . The entropy of the binary output variable y given a xed f can be expressed in terms of the binary entropy function h: H[ y j x ; f ] = h (( f ( x )) h( p ) = p log p (1 p ) log(1 p ) Expectations over the posterior need to be computed. Using a Gaussian approxi- mation to the posterior, for each x , f x = f ( x ) will follow a Gaussian distribution with mean x ; D and variance 2 x ; D . To compute Eqn. (2) we have to compute 4