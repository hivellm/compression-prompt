For all the following models, the training complexity is proportional to O = E  T  Q; (1) where E is number of the training epochs, T is the number of the words in the training set and Q is dened further for each model architecture. Common choice is E = 3  50 and T up to one billion. All models are trained using stochastic gradient descent and backpropagation [26]. 2.1 Feedforward Neural Net Language Model (NNLM) The probabilistic feedforward neural network language model has been proposed in [1]. It consists of input, projection, hidden and output layers. At the input layer, N previous words are encoded using 1-of- V coding, where V is size of the vocabulary. The input layer is then projected to a projection layer P that has dimensionality N  D , using a shared projection matrix. As only N inputs are active at any given time, composition of the projection layer is a relatively cheap operation. The NNLM architecture becomes complex for computation between the projection and the hidden layer, as values in the projection layer are dense. For a common choice of N = 10 , the size of the projection layer ( P ) might be 500 to 2000, while the hidden layer size H is typically 500 to 1000 units. Moreover, the hidden layer is used to compute probability distribution over all the words in the vocabulary, resulting in an output layer with dimensionality V . Thus, the computational complexity per each training example is Q = N  D + N  D  H + H  V; (2) where the dominating term is H  V . However, several practical solutions were proposed for avoiding it; either using hierarchical versions of the softmax [25, 23, 18], or avoiding normalized models completely by using models that are not normalized during training [4, 9]. With binary tree representations of the vocabulary, the number of output units that need to be evaluated can go down to around log 2 ( V ) . Thus, most of the complexity is caused by the term N  D  H . In our models, we use hierarchical softmax where the vocabulary is represented as a Huffman binary tree. This follows previous observations that the frequency of words works well for obtaining classes in neural net language models [16]. Huffman trees assign short binary codes to frequent words, and this further reduces the number of output units that need to be evaluated: while balanced binary tree would require log 2 ( V ) outputs to be evaluated, the Huffman tree based hierarchical softmax requires only about log 2 ( U nigram