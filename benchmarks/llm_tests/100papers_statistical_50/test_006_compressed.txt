h (( )) exp( t log(2) 5 10 3 dierence Figure 1: Analytic approximation 1 binary entropy error function by squared exponential ). The absolute error remains under 10 For most practically relevant kernels, objective (5) smooth dierentiable so gradient-based optimisation procedures used nd maximally informative query. 3.1 Extension: Learning Hyperparameters many applications parameter set naturally divides into interest, i.e. = g such settings, may want query points that are maximally informative about while not caring about By integrating Eqn. (1) parameters, BALDâ€™s objective re-derived as: H E jD E H E D ] (6) context GP models, hyperparameters typically control smooth- ness spatial length-scale functions. If maintain posterior distribution these hyperparameters, which do e. g. via Hamiltonian Monte Carlo, choose either treat use Eq. 6, include in perform well. certain cases, automatic relevance determination Rasmussen Williams, ], it even make sense hyperparameters variables primary interest, itself parameter 3.2 Preference Learning Our framework GPC extended important problem Furnkranz Hullermeier, 2003 Chu Ghahramani, ]. dataset consists pairs items X with binary labels, 0 g = means instance preferred denoted The task predict preference relation between any ). We view this special case building classier on pairs inputs 6