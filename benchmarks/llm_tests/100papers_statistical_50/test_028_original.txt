region of space. It has been shown that in practice it can reduce the chance that gradients explode, and even allow training generator models or models that work with unbounded amounts of memory(Pascanu and Jaeger, 2011; Doya and Yoshizawa, 1991). One important downside is that it requires a target to be dened at every time step. In Hochreiter and Schmidhuber (1997); Graves et al. (2009) a solution is proposed for the vanishing gra- dients problem, where the structure of the model is changed. Specically it introduces a special set of units called LSTM units which are linear and have a recurrent connection to itself which is xed to 1. The ow of information into the unit and from the unit is guarded by an input and output gates (their behaviour is learned). There are several variations of this basic structure. This solution does not address explicitly the exploding gradients problem. Sutskever et al. (2011) use the Hessian-Free opti- mizer in conjunction with structural damping , a spe- cic damping strategy of the Hessian. This approach seems to deal very well with the vanishing gradient, though more detailed analysis is still missing. Pre- sumably this method works because in high dimen- sional spaces there is a high probability for long term components to be orthogonal to short term ones. This would allow the Hessian to rescale these components independently. In practice, one can not guarantee that this property holds. As discussed in section 2.3, this method is able to deal with the exploding gradient as well. Structural damping is an enhancement that forces the change in the state to be small, when the pa- rameter changes by some small value   . This asks for the Jacobian matrices @ x t