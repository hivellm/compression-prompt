2.3 Parallel Training Neural Networks To train huge sets, have implemented several top large-scale framework called DistBelief [6], including feedforward NNLM new paper. framework allows us run multiple replicas parallel, replica synchronizes its gradient updates through centralized server keeps parameters. For parallel training, mini-batch asynchronous gradient descent an adaptive learning rate procedure called Adagrad [7]. Under framework, common one hundred or replicas, using many CPU cores different machines center. 3 New Log-linear Models In section, propose two architectures learning representations try minimize computational complexity. main observation previous section most caused non-linear hidden model. While what makes networks so attractive, decided explore simpler might not able represent precisely networks, can possibly much efciently. architectures directly follow our earlier work [13, 14], found network language can successfully two steps: rst, vectors learned using simple model, N-gram NNLM top these representations words. While there has been later substantial amount work focuses vectors, consider approach [13] simplest one. Note related been much earlier [26, 8]. 3.1 Continuous Bag-of-Words Model rst similar feedforward NNLM, non-linear hidden removed shared (not just matrix); thus, get projected into position (their vectors averaged). We call archi- tecture bag-of-words order does not inuence projection. Furthermore, future; obtained best performance task introduced next section building log-linear classier four future four input, criterion correctly classify (middle) word. Training Q N + log V ) : (4) We denote further CBOW, unlike standard bag-of-words model, uses representation context. shown Figure 1. Note matrix between shared positions way NNLM. 3.2 Continuous Skip-gram Model second similar CBOW, instead predicting context, tries maximize classication another sentence. More precisely, input log-linear classier layer, predict within certain before after word. We found increasing improves quality resulting vectors, increases computational complexity. Since distant usually related than close it, give weight distant sampling our examples. complexity proportional Q = C ( D + D log 2 ( V )) ; (5) C maximum distance words. Thus, if choose C = 5 , will select randomly number R range < 1; C > , then R history 4