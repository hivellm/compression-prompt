@ x k +1 , not for any direction (i.e. we do not enforce that all eigenvalues are close to 1). The second observation is that we are using a soft con- straint, therefore we are not ensured the norm of the error signal is preserved. If it happens that these Jaco- bian matrices are such that the norm explodes (as t  k increases), then this could lead to the exploding gradi- ents problem and we need to deal with it for example as described in section 3.2. This can be seen from the dynamical systems perspective as well: preventing vanishing gradients implies that we are pushing the model such that it is further away from the attrac- tor (such that it does not converge to it, case in which the gradients vanish) and closer to boundaries between basins of attractions, making it more probable for the gradients to explode. 4. Experiments and Results 4.1. Pathological synthetic problems As done in Martens and Sutskever (2011), we address the pathological problems proposed by Hochreiter and Schmidhuber (1997) that require learning long term correlations. We refer the reader to this original pa- per for a detailed description of the tasks and to the supplementary materials for the complete description of the experimental setup. 4.1.1. The Temporal Order problem We consider the temporal order problem as the pro- totypical pathological problem, extending our results to the other proposed tasks afterwards. The input is a long stream of discrete symbols. At two points in time (in the beginning and middle of the sequence) a symbol within f A; B g is emitted. The task consists in classifying the order (either AA; AB; BA; BB ) at the end of the sequence. Fig. 7 shows the success rate of standard SGD, SGD-C (SGD enhanced with out clipping strategy) and SGD- CR (SGD with the clipping strategy and the regular- ization term). Note that for sequences longer than 20, the vanishing gradients problem ensures that neither SGD nor SGD-C algorithms can solve the task. The x -axis is on log scale. This task provides empirical evidence that exploding gradients are linked with tasks that require long mem- ory traces. We know that initially the model oper- ates in the one-attractor regime (i.e.  1 < 1), in which the amount of memory is controlled by  1 . More memory means larger spectral radius, and, when this value crosses a certain threshold the model enters rich regimes where gradients are likely to explode. We see in Fig. 7 that as long as the vanishing gradient prob-