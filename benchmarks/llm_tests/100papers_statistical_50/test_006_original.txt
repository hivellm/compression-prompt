h (( )) exp( t 2 log(2) ) 5 10 3 dierence Figure 1: Analytic approximation ( 1 ) to the binary entropy of the error function ( ) by a squared exponential ( ). The absolute error ( ) remains under 3 10 3 . For most practically relevant kernels, the objective (5) is a smooth and dierentiable function of x , so gradient-based optimisation procedures can be used to nd the maximally informative query. 3.1 Extension: Learning Hyperparameters In many applications the parameter set naturally divides into parameters of interest, + , and nuisance parameters , i.e. = f + ; g . In such settings, the active learning may want to query points that are maximally informative about + , while not caring about . By integrating Eqn. (1) over the nuisance parameters, , BALDâ€™s objective is re-derived as: H E p ( + ; jD ) y j x ; + ; E p ( + jD ) H E p ( j + ; D ) [ y j x ; + ; ] (6) In the context of GP models, hyperparameters typically control the smooth- ness or spatial length-scale of functions. If we maintain a posterior distribution over these hyperparameters, which we can do e. g. via Hamiltonian Monte Carlo, we can choose either to treat them as nuisance parameters and use Eq. 6, or to include them in + and perform active learning over them as well. In certain cases, such as automatic relevance determination [ Rasmussen and Williams, 2005 ], it may even make sense to treat hyperparameters as variables of primary interest, and the function f itself as nuisance parameter . 3.2 Preference Learning Our active learning framework for GPC can be extended to the important problem of preference learning [ Furnkranz and Hullermeier, 2003 , Chu and Ghahramani, 2005 ]. In preference learning the dataset consists for pairs of items ( u i ; v i ) 2 X 2 with binary labels, y i 2 f 0 ; 1 g . y i = 1 means instance u i is preferred to v i , denoted u i v i . The task is to predict the preference relation between any ( u ; v ). We can view this as a special case of building a classier on pairs of inputs 6