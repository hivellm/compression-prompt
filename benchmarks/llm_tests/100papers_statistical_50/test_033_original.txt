Figure 7. Rate of success for solving the temporal order problem versus log of sequence length. See text. lem does not become an issue, addressing the explod- ing gradients problem ensures a better success rate. When combining clipping as well as the regularization term proposed in section 3.3, we call this algorithm SGD-CR. SGD-CR solved the task with a success rate of 100% for sequences up to 200 steps (the maximal length used in Martens and Sutskever (2011)). Fur- thermore, we can train a single model to deal with any sequence of length 50 up to 200 (by providing se- quences of dierent lengths for dierent SGD steps). Interestingly enough, the trained model can gen- eralize to new sequences that can be twice as long as the ones seen during training . 4.1.2. Other pathological tasks SGD-CR was also able to solve (100% success on the lengths listed below, for all but one task) other patho- logical problems proposed in Hochreiter and Schmid- huber (1997), namely the addition problem, the mul- tiplication problem, the 3-bit temporal order prob- lem , the random permutation problem and the noise- less memorization problem in two variants (when the pattern needed to be memorized is 5 bits in length and when it contains over 20 bits of information; see Martens and Sutskever (2011)). For the rst 4 prob- lems we used a single model for lengths up to 200, while for the noiseless memorization we used a dif- ferent model for each sequence length (50, 100, 150 and 200). The hardest problems for which only one trail out of 8 succeeded was the random permutation problem. In all cases, we observe successful generaliza- tion to sequences longer than the training sequences. In most cases, these results outperforms Martens and Sutskever (2011) in terms of success rate, they deal with longer sequences than in Hochreiter and Schmid- huber (1997) and compared to (Jaeger, 2012) they gen- eralize to longer sequences. Table 1. Results on polyphonic music prediction in nega- tive log likelihood per time step. Lower is better.