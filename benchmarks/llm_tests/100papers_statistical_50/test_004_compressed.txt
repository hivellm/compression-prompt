[ MacKay, 1992 Krishnapuram et al., Lawrence et al., 2003 ]) but free choose from many available algorithms. Minimal additional approximations introduced, so, knowledge algorithm represents most exact fastest way perform full information-theoretic non-parametric discriminative models. 3 Processes Classication Pref- erence Learning In this section derive BALD Process classication (GPC). GPs powerful popular non-parametric tool regression classication. GPC appears especially challenging problem information-theoretic because parameter space innite, however, able calculate fully relevant information quantities without having work out entropies innite dimensional objects. probabilistic underlying GPC as follows: GP( Bernoulli(( ))) latent parameter, now called X ! R assigned process prior covariance or kernel ). We consider probit case value takes Bernoulli distribution probability )), CDF. For further details on GPs see [Rasmussen Williams, 2005]. Inference GPC intractable; some observations D becomes non-Gaussian complicated. commonly used approximate inference methods EP, Laplace approximation, Assumed Density Filtering sparse methods all approximate Rasmussen Williams, 2005 ]. Throughout section assume that provided approximation one these methods, though does not care which one. derivation use 1 indicate approximation exploited. informativeness query computed Eqn. entropy output variable xed can expressed terms binary entropy h: H[ j ] h (( )) h( log (1 log(1 Expectations over need computed. Using approxi- mation posterior, each follow distribution mean D variance 2 D . To compute Eqn. have compute 4