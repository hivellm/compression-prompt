Bayesian Active Learning for Classication and Preference Learning Neil Houlsby, Ferenc Huszar, Zoubin Ghahramani, Mate Lengyel Computational and Biological Learning Laboratory University of Cambridge November 27, 2024 Abstract Information theoretic active learning has been widely studied for prob- abilistic models. For simple regression an optimal myopic policy is easily tractable. However, for other tasks and with more complex models, such as classication with nonparametric models, the optimal solution is harder to compute. Current approaches make approximations to achieve tractabil- ity. We propose an approach that expresses information gain in terms of predictive entropies, and apply this method to the Gaussian Process Classier (GPC). Our approach makes minimal approximations to the full information theoretic objective. Our experimental performance compares favourably to many popular active learning algorithms, and has equal or lower computational complexity. We compare well to decision theoretic approaches also, which are privy to more information and require much more computational time. Secondly, by developing further a reformulation of binary preference learning to a classication problem, we extend our algorithm to Gaussian Process preference learning. 1 Introduction In most machine learning systems, the learner passively collects data with which it makes inferences about its environment. In active learning, however, the learner seeks the most useful measurements to be trained upon. The goal of active learning is to produce the best model with the least possible data; this is closely related to the statistical eld of optimal experimental design. With the advent of the internet and expansion of storage facilities, vast quantities of unlabelled data have become available, but it can be costly to obtain labels. Finding the most useful data in this vast space calls for ecient active learning algorithms. Two approaches to active learning are to use decision and information the- ory [ Kapoor et al., 2007 , Lindley, 1956 ]. The former minimizes the expected 1