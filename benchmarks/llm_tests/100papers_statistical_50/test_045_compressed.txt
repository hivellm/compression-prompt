For all following models, complexity proportional O E T Q; (1) E epochs, T set Q dened further each model architecture. Common choice E 3 50 T up one billion. All trained stochastic gradient descent backpropagation [26]. 2.1 Feedforward Neural Net Language Model (NNLM) probabilistic feedforward neural network language model has been proposed [1]. It consists input, projection, layers. At input layer, previous encoded 1-of- coding, vocabulary. input then projected P has dimensionality , shared matrix. As only inputs active at any given time, composition relatively cheap operation. NNLM architecture becomes complex computation between values dense. common choice 10 P might 500 2000, while typically 500 1000 units. Moreover, used compute probability distribution over vocabulary, resulting an with dimensionality Thus, computational per each example Q V; (2) dominating However, several practical solutions were proposed avoiding it; either versions [25, 23, 18], or avoiding normalized completely not normalized during [4, 9]. With representations vocabulary, evaluated can go down around Thus, most caused In our models, we use vocabulary represented Huffman tree. This follows previous observations frequency works well obtaining classes neural net language [16]. Huffman trees assign short codes frequent words, this further reduces units need evaluated: while balanced tree would require log 2 ) outputs evaluated, Huffman tree based hierarchical softmax requires only about log 2 U nigram