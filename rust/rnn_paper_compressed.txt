diculty Recurrent Networks Razvan Pascanu pascanur@iro.umontreal.ca Universite de Montreal Tomas Mikolov t.mikolov@gmail.com Figure 1. Schematic recurrent neural network. connections hidden layer allow information persist input another. exploding problems described Bengio (1994). 1.1. Training networks A generic neural network, u step given (1). theoretical section paper sometimes make use specic parametrization given equa- tion (11) order provide precise conditions intuitions about everyday use-case. = F ( x t 1 ; u t ; ) (1) W ( x t 1 ) u (2) parameters given weight matrix W biases input weight matrix W collected for general case. 0 provided user, zero learned, an element-wise function (usually the tanh or sigmoid ). A cost measures performance network some given task broken apart individual costs each step P T L ( x t ). One approach used compute nec- essary Backpropagation Through Time (BPTT), recurrent represented This formulation equivalent more widely known equation ( W rec x t 1 + W in u t + b ), was chosen convenience. arXiv:1211.5063v2 [cs.LG] 16 Feb 2013 diculty Recurrent Networks deep multi-layer (with an unbounded number of layers) backpropagation applied unrolled (see Fig. 2). Figure 2. Unrolling neural networks time creating copy model each step. denote hidden network at u network at time obtained output at time diverge classical BPTT equations at point re-write (see equations (3), (4) (5)) order better highlight exploding problem. These equations were obtained writing gradients sum-of-products form. E X T (3) E X + (4) = Y i>k i Y i>k W diag ( 0 ( x i 1 )) (5) + refers \immediate" partial derivative state respect i.e., where taken constant respect Specically, considering 2, value any row i matrix ( @ + x k @ W rec ) just ( x k 1 ). Equation (5) provides form Jacobian matrix i specic parametrization given (11), diag converts vector diagonal matrix, computes derivative an element-wise fash- ion. Note each equation (3) has form behaviour individual terms determine behaviour sum. Henceforth focus such generic term, calling simply gradient there no confusion. Any component also sum (see equa- tion (4)), whose terms refer temporal contribu- tions temporal components. One see each temporal contribution + measures how at aects cost at > . factors @ (equation (5)) transport \in time\ step back step further loosely distin- guish between short contributions, long refers components which short everything else. 2. Exploding Vanishing Gradients As introduced Bengio (1994), exploding refers large increase norm during training. Such events caused explosion long components, grow exponentially then short ones. vanishing refers opposite behaviour, long components go exponentially fast 0, making impossible model learn correlation temporally dis- tant events. 2.1. mechanics To understand phenomenon need look at form each temporal component, particular at matrix factors @ (see equation (5)) take form product k Jacobian matrices. same way product k real numbers shrink zero explode innity, so product matrices (along some direction v ). what follows try formalize intu- itions (extending a similar derivation done in Bengio et al. (1994) only single unit case considered). If consider linear version model (i.e. set to the identity function in equation (11)) use power iteration method formally analyze prod- uct Jacobian matrices obtain tight conditions when explode vanish (see the sup- plementary materials for a detailed derivation of these conditions). It sucient largest eigenvalue recurrent weight matrix smaller than components vanish (as t ! 1 ) necessary be larger than explode. generalize results nonlinear functions absolute values ( x ) bounded (say by a value 2 R ) therefore diag ( 0 ( x k )) rst prove it sucient < where absolute value largest eigenvalue recurrent weight matrix occur. Note assume parametrization given (11). Jacobian matrix diag ( 0 ( x k )). 2- norm Jacobian bounded product diculty Recurrent Networks norms two matrices (see equation (6)). Due assumption, implies it smaller 8 k; +1 diag ( 0 ( x k )) < < (6) Let R such 8 k; +1 < existence given (6). By induction over show E Y +1 ! E (7) As < 1, follows that, according (7), contributions (for which t k is large) go exponentially fast . By inverting proof get necessary condition exploding namely largest eigen- value larger --- (otherwise the long term com- ponents would vanish instead of exploding). tanh while sigmoid have 4 2.2. Drawing similarities Dynamical Systems improve understanding vanishing problems employ- ing dynamical systems perspective, was done before Doya (1993); Bengio (1993). recommend reading Strogatz (1994) formal detailed treatment dynamical systems theory. any parameter assignment depending ini- tial state autonomous dynamical system converges, under repeated application map F several possible attrac- tor states (e.g. point attractors, though other type of attractors exist). could nd itself chaotic regime, case some following observations may not hold, but not treated depth here. Attractors describe asymptotic be- haviour model. space divided basins attraction, each attractor. If model started basin attraction, will converge corresponding attractor grows. Dynamical systems theory tells us changes slowly, asymptotic behaviour changes smoothly almost everywhere except certain crucial points drastic changes occur (the new asymptotic be- haviour ceases to be topologically equivalent to the old one). These points called bifurcation boundaries caused attractors appear, disappear shape. (Doya, 1993) hypothesizes bifurcation cross- ings cause gradients explode. like extend observation sucient condi- tion explode, for reason re-use one-hidden unit (and plot) (Doya, 1993) (see Fig. 3). x-axis covers parameter b y-axis asymptotic state bold line follows move- ment nal point attractor, b changes. At b bifurcation boundary new attrac- tor emerges (when b decreases from 1 ), at 2 another results disappearance two attractors. interval ( b 1 ; b 2 ) are rich regime, there attractors change position boundary them, change traced out dashed line. vector eld (gray dashed arrows) describe evolution state if network initialized region. Figure 3. Bifurcation diagram single unit RNN (with xed recurrent weight of 5.0 and adjustable bias b ; example introduced in Doya (1993)). See text. show there types events lead change ! One cross- basins attraction (depicted with a unlled circles), other crossing bi- furcation (lled circles). large resulting even small changes (as the system is attracted towards dierent attractors) leads large gradient. It however neither necessary nor sucient cross bifurcation explode, bifurca- tions global events could no eect lo- cally. Learning traces path parameter- space. If at boundary, but basin attraction attractor (from many possible attrac- tors) not shape disappear bifurcation crossed, bifurcation not aect learning. Recurrent Crossing boundaries basins attraction local event, it sucient gradients ex- plode. assume crossing emerging attractor or disappearing (due to a bifur- cation) qualies crossing boundary attractors, can formulate sucient condition explode encapsulates obser- vations made Doya (1993), extending them normal crossing boundaries basins attractions. Note how gure, only values b bifurcation, whole range values boundary crossing. Another limitation previous analysis they only consider autonomous systems assume observations hold input-driven models. (Ben- gio et al. , 1994) dealt assuming bounded noise. downside approach it limits how reason about input. practice, input supposed drive dynamical system, being able leave some attrac- tor state, kick out basin attraction certain triggering patterns present themselves. propose extend analysis driven models folding into map. consider family maps F apply F at step. Intuitively, explode require behaviour before, (at least in some direction) maps F ::; agree direction. Fig. 4 describes behaviour. Figure 4. diagram illustrates how change small 0 blue vs red (left vs right) trajectories generated maps : : : dierent initial states. specic parametrization provided equa- (11) take analogy further decomposing maps xed map ~ F time-varying U F ( x ) W ( x ) corresponds input-less network, U ( x ) u describes eect input. depicted Fig. 5. Since U changes time, not analyzed using standard dynami- cal systems tools, ~ can. basins attractions crossed ~ F move towards attractor, could lead (unless the input maps U t are opposing this) large discrepancy . There- fore studying asymptotic behaviour ~ pro- vide useful information about events likely happen. Figure 5. Illustrates how break apart maps F ::F into constant map ~ F maps U ::; U . dotted vertical line represents basins attraction, straight dashed arrow direction map ~ F side boundary. diagram extension Fig. 4. One interesting observation dynamical sys- tems perspective respect vanishing following. factors @ go zero (for t k large), means not depend on (if we change x k by some , x t stays the same). translates into at being close conver- gence towards attractor (which it would reach from anywhere in the neighbourhood of x k ). 2.3. geometrical interpretation Let us consider simple unit (equa- tion (8)) provide initial train specic target after steps. Note simplicity assume no input. = w ( x t 1 ) (8) Fig. 6 shows surface 50 ( ( x 50 ) : 7) 2 : 5 be sigmoid function. more easily analyze behavior this further simplifying be linear ( then being the identity function), b 0. 0 w follows @x @w tx w 2 @w 2 ( t 1) w 2 implying rst derivative explodes, so second derivative. general case, do so along directions v says exists, situations, vector v C where C; R > 1. linear ( is the identity function), v eigenvector corresponding largest eigenvalue If bound tight, hypothesize general --- Recurrent Figure 6. plot surface single unit network, highlighting existence high cur- vature walls. solid lines depicts standard trajectories descent might follow. Using dashed arrow diagram shows what happen if rescaled xed size its above threshold. so curvature along leading wall error surface like seen Fig. 6. holds, gives us simple solution exploding depicted Fig. 6. both gradient leading eigenvector curvature aligned follows surface steep wall perpen- dicular (and consequently to the gradient). means stochastic (SGD) reaches wall descent step, will forced jump across valley moving perpen- dicular steep walls, possibly leaving valley disrupting process. dashed arrows Fig. 6 correspond ignoring large ensuring model stays close wall. key insight all steps taken gradient explodes aligned v ignore descent (i.e. the model moves perpendicular to the wall). At wall, small-norm direction there- fore merely pushes us back inside smoother low- region besides wall, whereas regular step bring us far, thus slowing preventing further training. Instead, bounded step, get back smooth region near wall SGD free explore directions. important addition scenario classical high valley, assume val- ley wide, large region around wall if land rely rst methods move towards local minima. why just clipping might sucient, not requiring second method. Note algo- rithm should work even growth gradient not curvature (a case for which a second order method would fail as the ratio between the gradient and curvature could still explode). Our hypothesis help understand re- cent success Hessian-Free approach compared second methods. There key dif- ferences Hessian-Free most second- algorithms. First, uses full Hessian hence deal directions not necessarily axis-aligned. Second, computes new estimate Hessian before up- date step take account abrupt curvature (such as the ones suggested by our hypothe- sis) most approaches smoothness as- sumption, i.e., averaging 2nd signals many steps. 3. Dealing vanishing 3.1. Previous solutions Using L1 L2 penalty recurrent weights help gradients. Given parame- ters initialized small values, spectral radius W probably smaller 1, follows not (see necessary condi- tion found in section 2.1). regularization ensure during spectral radius never exceeds approach limits model sim- ple regime (with a single point attractor at the origin), any information inserted has die exponentially fast time. such regime not generator network, nor exhibit memory traces. Doya (1993) proposes pre-program (to initialize the model in the right regime) or teacher forcing rst proposal assumes if exhibits beginning kind asymptotic behaviour one required target, there no need cross boundary. downside one not always know required asymptotic behaviour, and, if information known, not trivial initial- ize this specic regime. should note initialization not prevent cross- basins attraction, which, shown, happen no bifurcation crossed. Teacher forcing more interesting, yet not understood solution. It seen way initializing model right regime right Recurrent region space. It been shown practice reduce chance explode, allow generator models models work unbounded amounts memory(Pascanu and Jaeger, 2011; Doya and Yoshizawa, 1991). One important downside it requires target dened at every step. Hochreiter Schmidhuber (1997); Graves (2009) solution proposed gra- dients problem, structure model changed. Specically introduces special units called LSTM units linear recurrent connection itself xed ow information unit guarded input output gates (their behaviour is learned). There several variations basic structure. solution not address explicitly exploding problem. Sutskever (2011) Hessian-Free opti- mizer conjunction structural damping spe- cic damping strategy Hessian. approach seems well gradient, detailed analysis still missing. Pre- sumably method works because high dimen- sional spaces there high probability long components orthogonal short ones. allow Hessian rescale components independently. practice, not guarantee property holds. discussed section 2.3, method able deal well. Structural damping enhancement forces change small, pa- rameter changes small value asks Jacobian matrices @ --- have small norm, hence further helping exploding problem. fact helps neural models sequences suggests while cur- vature might at time gradi- ent, might not grow at same hence not sucient deal gradient. Echo State (Lukosevicius and Jaeger, 2009) avoid vanishing not learning weights. They sampled hand crafted distributions. Because usually largest eigenvalue recurrent weight is, construction, smaller 1, information fed has die exponentially fast. means models not easily deal dependencies, even reason slightly dierent problem. An extension classical represented leaky integration (Jaeger et al. , 2007), + (1 ) ( W rec x k 1 + W in u k + b ). While units solve standard benchmark Hochreiter Schmidhu- ber (1997) learning dependencies (see (Jaeger, 2012)), they suitable deal low frequency information act low pass lter. Because most weights randomly sam- pled, not clear what size models need solve complex real world tasks. make nal note about approach pro- posed Tomas Mikolov his PhD thesis (Mikolov, 2012)(and implicitly used in the state of the art re- sults on language modelling (Mikolov et al. , 2011)). It involves gradient’s compo- nents element-wise (clipping an entry when it exceeds in absolute value a xed threshold). Clipping been shown do practice forms backbone approach. 3.2. Scaling down As suggested section 2.3, simple mechanism deal sudden increase norm gradi- ents rescale them whenever go thresh- old (see algorithm 1). Algorithm Pseudo-code clipping gra- dients whenever ^ if ^ threshold ^ threshold ^ ^ end algorithm similar proposed Tomas Mikolov only diverged original proposal attempt provide better theoretical foundation (ensuring that we always move in a de- scent direction with respect to the current mini-batch), practice both variants behave similarly. proposed simple implement computationally ecient, but however in- troduce additional hyper-parameter, namely threshold. One good heuristic setting thresh- old look at statistics average over suciently number updates. ex- periments noticed given size, not sensitive hyper- parameter algorithm behaves even rather small thresholds. algorithm also thought adapting learning based norm gradient. Compared other adaptation strate- gies, focus improving convergence col- lecting statistics gradient (as for example in On the diculty of training Recurrent Neural Networks Duchi et al. (2011), Moreira Fiesler (1995) for overview), rely instantaneous gradient. means handle abrupt norm, other methods not able so. 3.3. Vanishing regularization opt address vanishing us- ing represents preference parameter values back-propagated gra- dients neither increase decrease too much mag- nitude. Our intuition increasing means at more sensitive inputs u ::; u @ x t @ x k is a factor in @ E t @ u k ). practice some inputs irrelevant predic- at time behave like noise net- work needs learn ignore. network not learn ignore irrelevant inputs unless there signal. These issues not solved parallel, seems natural expect need force network increase norm at expense larger errors (caused by the irrelevant input entries) and wait it learn ignore irrelevant entries. suggest moving to- wards increasing norm not always done following descent (which is, for e.g., what a second order method would try to do), therefore need enforce via term. regularizer propose below prefers solutions error signal preserves norm travels back time: X = X +1 A (9) In order computationally ecient, only \immediate" partial derivative respect W (we consider that x k and @ E @ x k +1 as being constant with respect to W rec when computing the derivative of k ), depicted (10). Note use parametrization (11). This done ef- ciently because get values E BPTT. Theano compute (Bergstra et al. , 2010; Bastien et al. , 2012). W P W P +1 diag 0 ( x k )) +1 A W (10) Note our only forces Ja- cobian matrices +1 preserve norm relevant direction --- +1 not any (i.e. we do not enforce that all eigenvalues are close to 1). observation are using soft con- straint, therefore not ensured error signal preserved. happens Jaco- bian matrices such explodes (as t k increases), then lead exploding gradi- ents problem need deal example described section 3.2. This seen dynamical systems perspective well: preventing implies are pushing such further away attrac- tor (such that it does not converge to it, case in which the gradients vanish) and closer boundaries basins attractions, making probable gradients explode. 4. Experiments Results 4.1. Pathological synthetic problems done Martens Sutskever (2011), address pathological problems Hochreiter Schmidhuber (1997) that require learning correlations. refer reader original pa- per detailed description tasks supplementary materials complete description experimental setup. 4.1.1. Temporal Order consider temporal pro- totypical pathological problem, extending results proposed afterwards. input stream discrete symbols. At points (in the beginning and middle of the sequence) a symbol within f A; B emitted. consists classifying (either AA; AB; BA; BB ) at end sequence. Fig. 7 shows success standard SGD, SGD-C (SGD enhanced with out clipping strategy) and SGD- CR (SGD with the clipping strategy and the regular- ization term). Note sequences longer 20, vanishing ensures neither SGD nor SGD-C algorithms solve task. -axis log scale. provides empirical evidence exploding linked tasks require mem- ory traces. know initially oper- ates one-attractor regime (i.e. 1 < 1), which amount memory controlled . More memory larger spectral radius, and, value crosses certain threshold enters rich regimes are likely explode. see Fig. 7 vanishing prob- Recurrent Figure 7. Rate success solving temporal versus log length. See text. lem not become issue, addressing explod- ing ensures better rate. When combining proposed section 3.3, call algorithm SGD-CR. SGD-CR solved success 100% up 200 steps (the maximal length used in Martens and Sutskever (2011)). Fur- thermore, train any 50 up 200 (by providing se- quences of dierent lengths for dierent SGD steps). Interestingly enough, trained gen- eralize new twice long ones seen during 4.1.2. Other pathological SGD-CR was able solve (100% success on the lengths listed below, for all but one task) other patho- logical problems Hochreiter Schmid- huber (1997), addition problem, mul- tiplication problem, 3-bit prob- lem random permutation problem noise- less memorization two variants (when the pattern needed to be memorized is 5 bits in length and when it contains over 20 bits of information; see Martens and Sutskever (2011)). rst 4 prob- lems single lengths up 200, while noiseless memorization used dif- ferent each (50, 100, 150 and 200). hardest problems only trail 8 succeeded was permutation problem. cases, observe successful generaliza- tion longer than sequences. most cases, results outperforms Martens Sutskever (2011) in terms rate, they longer sequences Hochreiter Schmid- huber (1997) and compared (Jaeger, 2012) they gen- eralize longer sequences. Table Results polyphonic music prediction nega- tive log likelihood per step. Lower better. Data Data fold SGD SGD+C SGD+CR Piano- 6.87 6.81 7.01 midi.de 7.56 7.53 7.46 Nottingham 3.67 3.21 3.24 3.80 3.48 3.46 MuseData 8.25 6.54 6.51 7.11 7.00 6.99 Table 2. Results next character prediction entropy (bits/character) Data Data fold SGD SGD+C SGD+CR train 1.46 1.34 1.36 1.50 1.42 1.41 5 steps N/A 3.76 3.70 N/A 3.89 3.74 4.2. Natural problems address task polyphonic music prediction, using datasets Piano-midi.de, Nottingham MuseData described Boulanger-Lewandowski (2012) and language modelling at character level Penn Treebank dataset (Mikolov et al. , 2012). explore modied version task, ask model predict 5th character future (instead of the next). Our assumption is solve modied task correlations important short ones, hence our should helpful. test scores reported Table average negative log likelihood per step. hyper-parameters across three runs, except regularization factor cuto threshold. SGD-CR provides statistically signicant im- provement state-of-the-art RNNs all polyphonic music prediction except MuseData on get exactly per- formance state-of-the-art (Bengio et al. , 2012), uses architecture. Table contains results language modelling (in bits per letter). These results suggest clipping solves optimization issue not act regular- izer, both test improve general. Results Penn Treebank reach state art achieved Mikolov (2012), who dierent algorithm similar ours, thus pro- viding evidence both behave similarly. reg- ularized performs well Hessian-Free trained model. By employing proposed able improve even not Recurrent dominated long contributions. 5. Summary Conclusions provided perspectives through one gain insight into issue. To deal problem, propose solution involves clipping exploded when too large. algorithm motivated assumption when explode, curvature higher or- der derivatives well, are faced specic pattern error surface, namely val- ley single steep wall. vanishing regulariza- tion forces signal not vanish travels back time. regularization forces Jacobian matrices i --- i preserve only relevant directions. practice show so- lutions improve performance both pathological synthetic datasets considered well polyphonic music prediction language modelling. Acknowledgements like thank Theano development team (particularly to Frederic Bastien, Pascal Lam- blin and James Bergstra) for their help. acknowledge NSERC, FQRNT, CIFAR, RQCHP and Compute Canada resources provided. References Bastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I., Bergeron, A., Bouchard, N., Bengio, Y. (2012). Theano: features speed improvements. Submited Deep Learning Un- supervised Feature Learning NIPS 2012 Workshop. Bengio, Y., Frasconi, P., Simard, P. (1993). learning long-term dependencies re- current networks. pages 1183{1195, San Francisco. IEEE Press. (invited paper). Bengio, Y., Simard, P., Frasconi, P. (1994). Learn- long-term dependencies descent dicult. IEEE Transactions Networks 5 (2), 157{166. Bengio, Y., Boulanger-Lewandowski, N., and Pascanu, R. (2012). Advances optimizing net- works. Technical Report arXiv:1212.0901, U. Mon- treal. Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde- Farley, D., Bengio, Y. (2010). Theano: CPU and GPU math expression compiler. Proceedings Python Scientic Computing Conference (SciPy) . Oral Presentation. Boulanger-Lewandowski, N., Bengio, Y., Vincent, P. (2012). Modeling dependencies high- dimensional sequences: Application polyphonic music generation transcription. Proceed- ings Twenty-nine International Conference Machine Learning (ICML’12) . ACM. Doya, K. (1993). Bifurcations neural net- works descent learning. IEEE Transac- tions Networks 75{80. Doya, K. and Yoshizawa, S. (1991). Adaptive synchro- nization neural physical oscillators. J. E. Moody, S. J. Hanson, R. Lippmann, editors, NIPS , pages 109{116. Morgan Kaufmann. Duchi, J. C., Hazan, E., and Singer, Y. (2011). Adap- tive subgradient methods online stochastic optimization. Journal Machine Learn- Research 12 2121{2159. Elman, J. (1990). Finding structure time. Cognitive Science 14 (2), 179{211. Graves, A., Liwicki, M., Fernandez, S., Bertolami, R., Bunke, H., and Schmidhuber, J. (2009). A Novel Connectionist System Unconstrained Handwrit- Recognition. IEEE Transactions Pattern Analysis Machine Intelligence 31 (5), 855{868. Hochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Computation 9 (8), 1735{1780. Jaeger, H. (2012). Long short-term memory echo networks: Details simulation study. Tech- nical report, Jacobs University Bremen. Jaeger, H., Lukosevicius, M., Popovici, D., Siew- ert, U. (2007). Optimization applications echo networks leaky- integrator neurons. Networks 20 (3), 335{352. Lukosevicius, M. and Jaeger, H. (2009). Reservoir computing approaches neural network training. Computer Science Review 3 (3), 127{149. Martens, J. and Sutskever, I. (2011). Learning recur- rent networks Hessian-free optimization. Proc. ICML’2011 ACM. Mikolov, T. (2012). Statistical Language Models based Networks Ph.D. thesis, Brno University Technology. Recurrent Mikolov, T., Deoras, A., Kombrink, S., Burget, L., Cernocky, J. (2011). Empirical evaluation combination advanced language modeling tech- niques. Proc. 12th annual conference in- ternational speech communication association (IN- TERSPEECH 2011) . Mikolov, T., Sutskever, I., Deoras, A., Le, H.-S., Kombrink, S., Cernocky, J. (2012). Subword language modeling networks. preprint (http://www.t.vutbr.cz/ imikolov/rnnlm/char.pdf). Moreira, M. Fiesler, E. (1995). Neural net- works adaptive learning momentum terms. Idiap-RR Idiap-RR-04-1995, IDIAP, Mar- tigny, Switzerland. Pascanu, R. Jaeger, H. (2011). A neurodynamical working memory. Netw. 24 199{ 207. Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986). Learning representations back- propagating errors. Nature 323 (6088), 533{536. Strogatz, S. (1994). Nonlinear Dynamics And Chaos: With Applications To Physics, Biology, Chemistry, And Engineering (Studies in Nonlinearity) . Studies nonlinearity. Perseus Books Group, edition. Sutskever, I., Martens, J., Hinton, G. (2011). Generating text neural networks. L. Getoor T. Scheer, editors, Proceedings 28th International Conference Machine Learning (ICML-11) , ICML ’11, pages 1017{1024, New York, NY, USA. ACM. Werbos, P. J. (1988). Generalization backpropa- gation application recurrent gas market model. Networks (4), 339{356. Analytical analysis of the exploding and vanishing gradients problem x t = W rec ( x t 1 ) + W in u (11) Let us consider g linear version parametrization (11) (i.e. set to the identity function) and assume goes innity l that: l (12) By employing generic power iteration method based proof show that, certain conditions, l grows exponentially. Proof Let W eigenvalues ::; n j > 2 > :: > n j corresponding eigen- vectors q ::; n form vector basis. now write row vector into basis: E P N =1 c If c 6 0 any < j; c 0, using fact c n X c (13) used fact i < i > which means lim !1 l 0. If > 1, follows @ grows exponentially fast l so along direction q proof assumes W diagonalizable simplic- ity, using Jordan normal form W extend proof considering not just eigen- vector largest eigenvalue whole subspace spanned eigenvectors sharing (largest) eigenvalue. result provides necessary condition gradients grow, namely spectral radius (the absolute value of the largest eigenvalue) of W rec must larger If q not null space + entire component grows exponentially l approach extends easily entire gradient. If re-write terms eigen-decomposition W get: E n X =1 X c ! (14) the diculty Recurrent Networks now pick c not norm, maximizing . If chosen holds j > c --- dom- inate sum because grows exponen- tially fast innity same happen sum. Experimental setup Note hyper-parameters selected based their performance validation using grid search. pathological synthetic tasks Similar criteria all tasks be- low (borrowed from Martens and Sutskever (2011)), namely model should make no 1% error batch 10000 test samples. cases, discrete symbols depicted one-hot encoding, case regression prediction given se- quence considered success if error less 0.04. Addition input consists sequence random numbers, random positions (one the beginning and one in the middle of the sequence) are marked. needs predict sum two ran- dom numbers after entire was seen. generated sample length [ T; 11 10 T ], though clarity refer length paper. rst position sam- pled [1 ; 10 ], while second position sampled [ 10 ; T 0 2 ]. These positions i; marked dif- ferent channel 0 everywhere except two sampled positions 1. needs predict sum random numbers found at sampled positions i; divided 2. To address problem 50 model, tanh activation function. learn- ing set .01 factor front regularization 0.5. clipping cut- o threshold 6 norm gradients. weights initialized normal distribution mean standard derivation : 1. trained sequences varying length 50 200. manage get success 100% at solving task, outperforms results presented Martens Sutskever (2011) (using Hessian Free), where see decline length gets closer 200 steps. Hochreiter Schmidhuber (1997) only con- siders up 100 steps. Jaeger (2012) also addresses task 100% success rate, solution not seem generalize well relies very output weights, ESNs usually sign instability. all lengths (50, 100, 150 200), and the trained generalizes new sequences up 400 steps (while the error is still under 1%). Multiplication task similar problem above, just predicted value product random num- bers instead sum. used hyper- parameters as previous case, obtained very similar results. Temporal temporal xed T xed set symbols f A; B 4 distractor symbols f c; d; e; f g en- tries uniformly sampled distractor sym- bols everywhere except at random positions, rst position sampled [ ; 2 T 10 ], while second [ 4 10 ; 5 T 10 ]. The task predict order non-distractor symbols were provided, i.e. either f AA; AB; BA; BB g . We 50 units model, learning .001 , coecient, 2. cut-o threshold clipping left 6. As other 100% success at sequences 50 200 steps. outperforms previous state art because success rate, single generalizes longer (up 400 steps). 3-bit temporal Similar previous one, except have 3 random positions, rst sampled [ ; 2 T 10 ], second [ 3 10 ; 4 T 10 ] and last [ 6 10 ; 7 T 10 ]. We use similar hyper-parameters above, but increase layer size 100 hidden units. As before outperform state art single able generalize new lengths. Random permutation problem case have dictionary 100 symbols. Ex- cept rst last position value sampled f 2 g entries ran- diculty Recurrent Networks domly picked [3 100]. The task do next symbol prediction, though only predictable sym- bol last one. use 100 hidden units learning .001 , coecient, 1. cuto threshold left 6. turns out more dicult learn, only out 8 experiments succeeded. As before single multiple values (from 50 to 200 units). Noiseless memorization For noiseless memorization presented binary pattern 5, followed steps constant value. After steps needs generate pattern seen initially. con- sider extension problem Martens Sutskever (2011), where pattern 10, symbol cardinality 5 instead 2. manage 100% success rate tasks, though train dierent 5 lengths considered (50, 100, 150, 200). Natural Tasks Polyphonic music prediction train our model, sigmoid units RNN, se- quences 200 steps. cut-o coecient thresh- old all cases, namely 8 (note that one has to take the mean over the sequence length when computing the gradients). In case Piano-midi.de dataset 300 hid- den units initial learning 1.0 (whir the learning rate halved every time the error over an epoch increased instead of decreasing). For the regularized used initial value coef- cient 0.5, follows =t schedule, i.e. 2 (where measures the number of epochs). For the Nottingham dataset used exact setup. MuseData we increased layer 400 hidden units. was decreased 0.5. regularized model, initial was 0.1, . make observation natural tasks seems useful use schedule decreases reg- ularization term. assume regularization forces model focus long correla- tions at expense short ones, so may useful have decaying factor order allow model make better use short infor- mation. Language modelling language modelling used 500 sig- moidal units no biases (Mikolov et al. , 2012). The model trained over 200 steps, hidden carried over step next one. use cut-o threshold 45 (though we take the sum of the cost over the sequence length) for all ex- periments. next character prediction learning 0.01 using no reg- ularization term, 0.05 add regularization 0.001 we do not clipping. When predicting 5th character future learning 0.05 regularization 0.1 without it. factor next character predic- tion was .01 kept constant, while modied used initial 0.05 1 schedule.