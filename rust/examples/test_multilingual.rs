//! Test multilingual compression with statistical filtering
//!
//! Demonstrates stop word filtering across 10 languages

use compression_prompt::statistical_filter::{StatisticalFilter, StatisticalFilterConfig};
use compression_prompt::tokenizer::{MockTokenizer, Tokenizer};

fn main() {
    let tokenizer = MockTokenizer;
    let config = StatisticalFilterConfig {
        compression_ratio: 0.5,
        ..Default::default()
    };
    let filter = StatisticalFilter::new(config);

    println!("ğŸŒ Multilingual Compression Test\n");
    println!("Testing stop word filtering across 10 languages:\n");

    // Test cases in different languages
    let test_cases = vec![
        (
            "ğŸ‡¬ğŸ‡§ English",
            "The quick brown fox jumps over the lazy dog in the garden",
        ),
        (
            "ğŸ‡ªğŸ‡¸ Spanish",
            "El rÃ¡pido zorro marrÃ³n salta sobre el perro perezoso en el jardÃ­n",
        ),
        (
            "ğŸ‡§ğŸ‡· Portuguese",
            "A rÃ¡pida raposa marrom pula sobre o cachorro preguiÃ§oso no jardim",
        ),
        (
            "ğŸ‡«ğŸ‡· French",
            "Le rapide renard brun saute par-dessus le chien paresseux dans le jardin",
        ),
        (
            "ğŸ‡©ğŸ‡ª German",
            "Der schnelle braune Fuchs springt Ã¼ber den faulen Hund im Garten",
        ),
        (
            "ğŸ‡®ğŸ‡¹ Italian",
            "La volpe marrone veloce salta sopra il cane pigro nel giardino",
        ),
        (
            "ğŸ‡·ğŸ‡º Russian (romanized)",
            "Bystraya korichnevaya lisa prygnula cherez lenivuyu sobaku v sadu",
        ),
        (
            "ğŸ‡¨ğŸ‡³ Chinese",
            "å¿«é€Ÿçš„æ£•è‰²ç‹ç‹¸åœ¨èŠ±å›­é‡Œè·³è¿‡æ‡’ç‹—",
        ),
        (
            "ğŸ‡¯ğŸ‡µ Japanese",
            "é€Ÿã„èŒ¶è‰²ã®ã‚­ãƒ„ãƒã¯åº­ã§æ€ ã‘è€…ã®çŠ¬ã‚’é£›ã³è¶Šãˆã‚‹",
        ),
        (
            "ğŸ‡¸ğŸ‡¦ Arabic (romanized)",
            "Al-thalab al-asmar al-saria qafaza fawqa al-kalb al-kasul fi al-hadiqah",
        ),
    ];

    for (lang, text) in test_cases {
        let original_tokens = tokenizer.count_tokens(text);
        let compressed = filter.compress(text, &tokenizer);
        let compressed_tokens = tokenizer.count_tokens(&compressed);
        
        let ratio = compressed_tokens as f64 / original_tokens as f64;
        let savings = (1.0 - ratio) * 100.0;

        println!("{}", lang);
        println!("  Original:    {} ({} tokens)", text, original_tokens);
        println!("  Compressed:  {} ({} tokens)", compressed, compressed_tokens);
        println!("  Savings:     {:.1}%", savings);
        println!();
    }

    println!("âœ… Multilingual compression complete!");
    println!("\nğŸ“Š Stop word coverage:");
    println!("  - English: ~50 words");
    println!("  - Spanish: ~60 words");
    println!("  - Portuguese: ~60 words");
    println!("  - French: ~50 words");
    println!("  - German: ~40 words");
    println!("  - Italian: ~60 words");
    println!("  - Russian: ~20 words (romanized)");
    println!("  - Chinese: ~20 particles");
    println!("  - Japanese: ~20 particles");
    println!("  - Arabic: ~20 words (romanized)");
    println!("  - Hindi: ~20 words (romanized)");
    println!("\nğŸŒ Total: ~450 stop words across 10 languages");
    println!("   Coverage: ~60% of world population (~4.5B speakers)");
}

