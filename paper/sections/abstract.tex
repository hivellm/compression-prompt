Large language models (LLMs) impose significant computational and financial costs through token-based pricing, with costs scaling linearly with input prompt length. We present a model-free statistical filtering approach that achieves 50\% token reduction while maintaining 89\% quality retention. Unlike existing methods that rely on external language models (e.g., LLMLingua), our approach uses pure statistical heuristics—inverse document frequency (IDF), position-based importance, part-of-speech filtering, and named entity detection—to identify and remove low-value tokens. The algorithm operates in $O(n \log n)$ time where $n$ is word count, achieving 10.58 MB/s throughput on modern hardware. We validate our approach on 1.66M tokens from 200 arXiv papers, demonstrating exactly 50\% compression with 100\% keyword retention and 91.8\% entity retention. Our implementation in Rust provides a production-ready solution with comprehensive quality metrics and configurable compression levels (30\%, 50\%, 70\%). For GPT-4 pricing (\$5/1M input tokens), this translates to \$2.50 saved per million tokens, or \$300K/year for enterprise applications processing 1B tokens monthly. The algorithm is transparent, deterministic, and requires no external dependencies, making it suitable for offline deployment and cost-sensitive production systems.
