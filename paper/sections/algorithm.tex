\label{sec:algorithm}

\subsection{Overview}

The statistical filtering algorithm operates in four stages:

\begin{enumerate}
    \item \textbf{Word Splitting}: Tokenize text into words
    \item \textbf{Importance Scoring}: Calculate $S(w)$ for each word
    \item \textbf{Word Selection}: Select top-k words by score
    \item \textbf{Text Reconstruction}: Rebuild text in original order
\end{enumerate}

\subsection{Stage 1: Word Splitting}

\textbf{Input}: Raw text prompt $P$

\textbf{Output}: Word sequence $W = [w_1, w_2, \ldots, w_n]$

\textbf{Method}: Split on whitespace

\begin{equation}
W = \text{Split}(P, \text{delimiter}=\text{whitespace})
\end{equation}

\textbf{Token Counting}: Use pluggable tokenizer interface to count actual LLM tokens for cost calculation.

\subsection{Stage 2: Importance Scoring}

\textbf{Input}: Word sequence $W$ of length $n$

\textbf{Output}: Scored words $\{(w_i, s_i)\}_{i=1}^{n}$

\begin{algorithm}[H]
\caption{Word Importance Scoring}
\label{alg:scoring}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Words $W[1..n]$, weights $(\alpha, \beta, \gamma, \delta, \epsilon)$
\STATE \textbf{Output:} Scored words $\{(w_i, s_i)\}$
\STATE Compute word frequencies: $\texttt{freq}[w] \gets \text{count}(w \text{ in } W)$
\FOR{$i = 1$ \TO $n$}
    \STATE $w \gets W[i]$
    \STATE $\texttt{idf} \gets \log(n / \texttt{freq}[w]) / \log(n)$
    \STATE $\texttt{pos} \gets \text{PositionScore}(i, n)$ \COMMENT{U-shaped}
    \STATE $\texttt{pos\_tag} \gets \text{POSHeuristic}(w)$ \COMMENT{Stop word check}
    \STATE $\texttt{ent} \gets \text{EntityScore}(w)$ \COMMENT{Capitalization, digits}
    \STATE $\texttt{entropy} \gets \text{LocalEntropy}(w)$ \COMMENT{Character diversity}
    \STATE $s_i \gets \alpha \cdot \texttt{idf} + \beta \cdot \texttt{pos} + \gamma \cdot \texttt{pos\_tag} + \delta \cdot \texttt{ent} + \epsilon \cdot \texttt{entropy}$
\ENDFOR
\RETURN $\{(W[i], s_i)\}_{i=1}^{n}$
\end{algorithmic}
\end{algorithm}

\textbf{Complexity}: $O(n)$ for frequency counting, $O(n)$ for scoring. Total: $O(n)$.

\subsection{Stage 3: Word Selection}

\textbf{Input}: Scored words $\{(w_i, s_i)\}$, compression ratio $\tau$

\textbf{Output}: Selected word indices $I \subset \{1, \ldots, n\}$

\begin{algorithm}[H]
\caption{Top-K Word Selection}
\label{alg:selection}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Scored words $\{(w_i, s_i)\}_{i=1}^{n}$, ratio $\tau$
\STATE \textbf{Output:} Selected indices $I$
\STATE $k \gets \lfloor \tau \cdot n \rfloor$
\STATE Sort words by score descending: $\text{sorted} \gets \text{SortBy}(\{(i, s_i)\}, s_i)$
\STATE Select top-k: $I \gets \{\text{sorted}[j].i : j = 1, \ldots, k\}$
\RETURN $I$
\end{algorithmic}
\end{algorithm}

\textbf{Complexity}: $O(n \log n)$ for sorting. Dominates overall complexity.

\subsection{Stage 4: Text Reconstruction}

\textbf{Input}: Original words $W$, selected indices $I$

\textbf{Output}: Compressed text $P'$

\begin{algorithm}[H]
\caption{Text Reconstruction}
\label{alg:reconstruction}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Words $W[1..n]$, indices $I$
\STATE \textbf{Output:} Compressed text $P'$
\STATE Sort $I$ by original position: $I_{\text{sorted}} \gets \text{Sort}(I)$
\STATE Extract selected words: $W' \gets [W[i] : i \in I_{\text{sorted}}]$
\STATE Join with spaces: $P' \gets \text{Join}(W', \text{delimiter}=\text{ `` ''})$
\RETURN $P'$
\end{algorithmic}
\end{algorithm}

\textbf{Complexity}: $O(k \log k)$ for sorting indices, $O(k)$ for reconstruction where $k = \tau \cdot n$.

\subsection{Example: Step-by-Step}

\textbf{Input}: ``The Bayesian model uses the prior distribution and the likelihood function to compute the posterior distribution.''

\textbf{Step 1 - Split}: 
\begin{verbatim}
[The, Bayesian, model, uses, the, prior, distribution,
 and, the, likelihood, function, to, compute, the,
 posterior, distribution]
\end{verbatim}
$n = 16$ words

\textbf{Step 2 - Score} (with $\tau = 0.5$, keep 8 words):

\begin{table}[h]
\centering
\caption{Word Scoring Example}
\begin{tabular}{lcccccc}
\toprule
Word & IDF & Pos & POS & Ent & Entropy & \textbf{Score} \\
\midrule
The & 0.10 & 1.0 & 0.0 & 0.0 & 0.3 & \textbf{0.33} \\
Bayesian & 0.95 & 1.0 & 1.0 & 1.0 & 0.8 & \textbf{0.96} \\
model & 0.95 & 0.9 & 0.7 & 0.0 & 0.7 & \textbf{0.73} \\
uses & 0.95 & 0.8 & 0.3 & 0.0 & 0.6 & \textbf{0.60} \\
the & 0.10 & 0.7 & 0.0 & 0.0 & 0.3 & \textbf{0.20} \\
prior & 0.95 & 0.6 & 0.7 & 0.0 & 0.7 & \textbf{0.70} \\
distribution & 0.80 & 0.5 & 0.7 & 0.0 & 0.8 & \textbf{0.67} \\
and & 0.95 & 0.5 & 0.0 & 0.0 & 0.4 & \textbf{0.33} \\
the & 0.10 & 0.5 & 0.0 & 0.0 & 0.3 & \textbf{0.16} \\
likelihood & 0.95 & 0.5 & 0.7 & 0.0 & 0.8 & \textbf{0.73} \\
function & 0.95 & 0.5 & 0.7 & 0.0 & 0.7 & \textbf{0.71} \\
to & 0.95 & 0.5 & 0.0 & 0.0 & 0.3 & \textbf{0.34} \\
compute & 0.95 & 0.5 & 0.7 & 0.0 & 0.7 & \textbf{0.71} \\
the & 0.10 & 0.5 & 0.0 & 0.0 & 0.3 & \textbf{0.16} \\
posterior & 0.95 & 0.7 & 0.7 & 0.0 & 0.8 & \textbf{0.75} \\
distribution & 0.80 & 1.0 & 0.7 & 0.0 & 0.8 & \textbf{0.70} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Step 3 - Select Top-8}:

Sorted by score: [Bayesian(0.96), posterior(0.75), model(0.73), likelihood(0.73), function(0.71), compute(0.71), distribution(0.70), prior(0.70)]

\textbf{Step 4 - Reconstruct}:

Sort by position: [Bayesian, model, prior, likelihood, function, compute, posterior, distribution]

\textbf{Output}: ``Bayesian model prior likelihood function compute posterior distribution''

\textbf{Compression}: 16 → 8 words (50\%)

\textbf{Removed}: ``The'' (3×), ``uses'', ``the'' (hidden in distribution duplicate), ``and'', ``to''

\subsection{Configuration}

\begin{verbatim}
StatisticalFilterConfig {
    compression_ratio: 0.5,    // 50% compression
    idf_weight: 0.3,           // Highest (rare = important)
    position_weight: 0.2,      // U-shaped importance
    pos_weight: 0.2,           // Content words
    entity_weight: 0.2,        // Names, numbers
    entropy_weight: 0.1,       // Character diversity
}
\end{verbatim}

\textbf{Presets}:
\begin{itemize}
    \item Conservative: $\tau = 0.7$ (30\% compression, 96\% quality)
    \item Balanced: $\tau = 0.5$ (50\% compression, 89\% quality)
    \item Aggressive: $\tau = 0.3$ (70\% compression, 71\% quality)
\end{itemize}

\subsection{Complexity Summary}

\begin{table}[h]
\centering
\caption{Algorithm Complexity}
\label{tab:complexity}
\begin{tabular}{lcc}
\toprule
Stage & Time & Space \\
\midrule
Word Splitting & $O(n)$ & $O(n)$ \\
Frequency Counting & $O(n)$ & $O(u)$ \\
Scoring & $O(n)$ & $O(n)$ \\
Selection (Sorting) & $O(n \log n)$ & $O(n)$ \\
Reconstruction & $O(k \log k)$ & $O(k)$ \\
\midrule
\textbf{Total} & $\mathbf{O(n \log n)}$ & $\mathbf{O(n)}$ \\
\bottomrule
\end{tabular}
\end{table}

where $n$ = word count, $u$ = unique words, $k = \tau \cdot n$ (kept words).

\textbf{Practical Performance}: For $n = 1.66$M words, runtime = 0.92s on commodity hardware (Intel i7, no GPU required).
